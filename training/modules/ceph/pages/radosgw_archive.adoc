= RadosGW Archive Zone 

:numbered:


== Introduction(TP in 5.3)

This sync module leverages the versioning feature of the S3 objects in RGW to have an archive zone that captures the different versions of the S3 objects as they occur over time in the other zones.

An archive zone allows to have a history of versions of S3 objects that can only be eliminated through the gateways associated with the archive zone.

This functionality is useful to have a configuration where several non-versioned zones replicate their data and metadata through their zone gateways (mirror configuration) providing high availability to the end users, while the archive zone captures all the data updates and metadata for consolidate them as versions of S3 objects.

Including an archive zone in a multizone configuration allows you to have the flexibility of an S3 object history in one only zone while saving the space that the replicas of the versioned S3 objects would consume in the rest of the zones.

RGW archive zone and has the following main characteristics:

* Versioning is enabled in all buckets in the RGW archive zone.
* Every time a user uploads a new object, this object is asynchronous replicated to the archive zone.
* If an object is modified, a new version is generated in the archive zone.
* If an object is deleted, the object is kept in the archive zone.

== Archive Zone Lab.

In this lab we are going to deploy an advanced configuration that consists of a
single realm with one zone group and two zones, one zone will be the production
zone, and the second zone will be the archive zone, each zone with two Ceph RGW
instances. Each zone is backed by its own Ceph Storage Cluster. 


=== Configure the Master Realm, Zonegroup and Zone.

[IMPORTANT]
====
If you have already completed the RadosGW introduction module, you will have
the realm,zonegroup and zone(zone1) created, but with no endpoints configured,
you will need to add valid rgw endpoints to the zonegroup and zone.

For example:

----
# radosgw-admin zone modify --endpoints=http://proxy01:8000 --rgw-zone=zone1
# radosgw-admin zonegroup modify --endpoints=http://proxy01:8000  --rgw-zonegroup=multizg
# radosgw-admin zone modify --access-key=sync --secret=sync --rgw-zone=zone1
# radosgw-admin period update --rgw-realm=multisite --commit
----

And then skip to section 3.2
====

[IMPORTANT]
====
If you have done the virtual host configuration from the RadosGW introduction
module it needs to be removed, before you continue with this lab
====

[WARNING]
====
Always Run commands on the master/primary cluster.
====

. Create a realm:

+
[source,sh]
----
# radosgw-admin realm create --rgw-realm=multisite --default
{
    "id": "ce31cd75-37c4-4b10-91db-1cda1ca12d95",
    "name": "multisite",
    "current_period": "0ad144e7-a880-43ab-8a64-c9deaf581280",
    "epoch": 1
}
----

. Create a zone group:
+
[source,sh]
----
# radosgw-admin zonegroup create --rgw-zonegroup=multizg --endpoints=http://proxy01:8000 --master --default
{
    "id": "2e41dde9-80f4-4ec8-a099-ec0e8a60938d",
    "name": "multizg",
    "api_name": "multizg",
    "is_master": "true",
    "endpoints": [],
    "hostnames": [],
    "hostnames_s3website": [],
    "master_zone": "",
    "zones": [],
    "placement_targets": [],
    "default_placement": "",
    "realm_id": "ce31cd75-37c4-4b10-91db-1cda1ca12d95",
    "sync_policy": {
        "groups": []
    }
}
----

[NOTE]
====
If you have more than one RGW service running per zone, as you would do for
production, you can add all the rgw address to the endpoints list
--endpoints=http://proxy01:8000,http://ceph-node02:8000 for example, if we want
the sync to survive DNS outages we can use the IP for the endpoints instead
of the Hostnames.
====

. Create a zone:
+
[source,sh]
----
# radosgw-admin zone create --rgw-zonegroup=multizg --rgw-zone=zone1 --access-key=sync --secret=sync --master --default --endpoints=http://proxy01:8000
{
    "id": "0e06b95f-3b6e-4a1c-95e8-b857f699e9e3",
    "name": "zone1",
    "domain_root": "zone1.rgw.meta:root",
    "control_pool": "zone1.rgw.control",
    "gc_pool": "zone1.rgw.log:gc",
    "lc_pool": "zone1.rgw.log:lc",
    "log_pool": "zone1.rgw.log",
    "intent_log_pool": "zone1.rgw.log:intent",
    "usage_log_pool": "zone1.rgw.log:usage",
    "roles_pool": "zone1.rgw.meta:roles",
    "reshard_pool": "zone1.rgw.log:reshard",
    "user_keys_pool": "zone1.rgw.meta:users.keys",
    "user_email_pool": "zone1.rgw.meta:users.email",
    "user_swift_pool": "zone1.rgw.meta:users.swift",
    "user_uid_pool": "zone1.rgw.meta:users.uid",
    "otp_pool": "zone1.rgw.otp",
    "system_key": {
        "access_key": "sync",
        "secret_key": "sync"
    },
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "zone1.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "zone1.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "zone1.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    "realm_id": "b3f73708-67c5-4b19-b378-6af9cc66c0b0",
    "notif_pool": "zone1.rgw.log:notif"
}
----

[TIP]
====
We can have one or mode REALMS,ZONEGROUPS or ZONES, if we don't specify
them on the radosgw-admin command with --rgw-realm , --rgw-zonegroup= ,
--rgw-zone= , the radosgw-admin command will use the ones set as the default
using the --default flag like we did in the previous commands.
====

. Commit the changes:
+
[source,sh]
----
[ceph: root@ceph-mon01 /]# radosgw-admin period update --rgw-realm=multisite --commit
----

. Deploy the RGW daemons with the name `multi.zone1`:
+
[source,sh]
----
[ceph: root@ceph-mon01 /]# ceph orch apply rgw multi.zone1 --realm=multisite --zone=zone1 --placement="2 proxy01 ceph-node02" --port=8000
----
+
[source,texinfo]
----
Scheduled multi.zone1 update...
# ceph orch ps | grep rgw
rgw.multi.zone1.ceph-node02.lviwfb  ceph-node02  *:8000       running (3m)      3m ago   3m    45.7M        -  16.2.8-85.el8cp  b2c997ff1898  0e3521f3a162
rgw.multi.zone1.proxy01.mhawfj      proxy01      *:8000       running (30m)     4m ago  30m    61.9M        -  16.2.8-85.el8cp  b2c997ff1898  4de70934f04e
----

=== Create Sync User

Create a system user that we will use to configure the sync between sites.

----
# radosgw-admin user create --uid=syncuser --display-name="syncuser" --access-key=sync --secret=sync --system
----

=== Configure Seconday Zone

Steps to configure the RADOS Gateway instance on the Archive zone.

[IMPORTANT]
====
Run commands on the secondary/Archive Ceph cluster, in our example ceph-mon01
====

----
# radosgw-admin realm pull --rgw-realm=multisite  --url=http://proxy01:8000 --access-key=sync --secret=sync --default
2022-12-23T09:26:56.377-0500 7fccf8715500  1 error read_lastest_epoch .rgw.root:periods.e7ccb8e8-4a93-4a87-9a6d-8a650696e839.latest_epoch
2022-12-23T09:26:56.415-0500 7fccf8715500  1 Set the period's master zonegroup 6b9fbc87-3202-4a35-85d0-e3e16fc91b32 as the default
{
    "id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
    "name": "multisite",
    "current_period": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
    "epoch": 2
}
----


Pull the period.
----
# radosgw-admin period pull --url=http://proxy01:8000 --access-key=sync --secret=sync
{
    "id": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
    "epoch": 5,
    "predecessor_uuid": "68a74587-6404-4798-83e0-6cd3bf417288",
    "sync_status": [],
    "period_map": {
        "id": "e7ccb8e8-4a93-4a87-9a6d-8a650696e839",
        "zonegroups": [
            {
                "id": "6b9fbc87-3202-4a35-85d0-e3e16fc91b32",
                "name": "multizg",
                "api_name": "multizg",
                "is_master": "true",
                "endpoints": [],
                "hostnames": [],
                "hostnames_s3website": [],
                "master_zone": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                "zones": [
                    {
                        "id": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                        "name": "zone1",
                        "endpoints": [],
                        "log_meta": "false",
                        "log_data": "false",
                        "bucket_index_max_shards": 11,
                        "read_only": "false",
                        "tier_type": "",
                        "sync_from_all": "true",
                        "sync_from": [],
                        "redirect_zone": ""
                    }
                ],
                "placement_targets": [
                    {
                        "name": "default-placement",
                        "tags": [],
                        "storage_classes": [
                            "SSD",
                            "STANDARD"
                        ]
                    },
                    {
                        "name": "ssd",
                        "tags": [
                            "allowed-ssd"
                        ],
                        "storage_classes": [
                            "STANDARD"
                        ]
                    }
                ],
                "default_placement": "default-placement",
                "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
                "sync_policy": {
                    "groups": []
                }
            }
        ],
        "short_zone_ids": [
            {
                "key": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
                "val": 2695141038
            }
        ]
    },
    "master_zonegroup": "6b9fbc87-3202-4a35-85d0-e3e16fc91b32",
    "master_zone": "c5dc9503-6c11-4851-91bd-f1d5ca61473c",
    "period_config": {
        "bucket_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        },
        "user_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        }
    },
    "realm_id": "e72107cb-4b3f-49b9-abb0-83c68a9967f9",
    "realm_name": "multisite",
    "realm_epoch": 2
}
----

Create the archive zone.

----
# radosgw-admin zone create --rgw-zone=archive --rgw-zonegroup=multizg --endpoints=http://proxy02:8000 --access-key=sync --secret=sync --default --tier-type=archive
2023-03-15T07:23:42.554-0400 7ffb657fd500  0 failed reading obj info from .rgw.root:zone_info.1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f: (2) No such file or directory
2023-03-15T07:23:42.554-0400 7ffb657fd500  0 WARNING: could not read zone params for zone id=1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f name=zone1
{
    "id": "d7c2da60-e269-4681-a231-e744d1f0dc8a",
    "name": "archive",
    "domain_root": "archive.rgw.meta:root",
    "control_pool": "archive.rgw.control",
    "gc_pool": "archive.rgw.log:gc",
    "lc_pool": "archive.rgw.log:lc",
    "log_pool": "archive.rgw.log",
    "intent_log_pool": "archive.rgw.log:intent",
    "usage_log_pool": "archive.rgw.log:usage",
    "roles_pool": "archive.rgw.meta:roles",
    "reshard_pool": "archive.rgw.log:reshard",
    "user_keys_pool": "archive.rgw.meta:users.keys",
    "user_email_pool": "archive.rgw.meta:users.email",
    "user_swift_pool": "archive.rgw.meta:users.swift",
    "user_uid_pool": "archive.rgw.meta:users.uid",
    "otp_pool": "archive.rgw.otp",
    "system_key": {
        "access_key": "sync",
        "secret_key": "sync"
    },
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "archive.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "archive.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "archive.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    "realm_id": "da2a45d7-764b-4c92-a28a-049d02c8c22f",
    "notif_pool": "archive.rgw.log:notif"
----

Commit the changes.

----
# radosgw-admin period update --commit
2023-03-15T07:24:31.295-0400 7f62337c9500  1 Cannot find zone id=d7c2da60-e269-4681-a231-e744d1f0dc8a (name=archive), switching to local zonegroup configuration
Sending period to new master zone 1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f
{
    "id": "9c4eba19-7849-48cf-8024-7fe44067a9a9",
    "epoch": 2,
    "predecessor_uuid": "40025556-2e6b-4a26-af36-89158d79816c",
    "sync_status": [],
    "period_map": {
        "id": "9c4eba19-7849-48cf-8024-7fe44067a9a9",
        "zonegroups": [
            {
                "id": "fc3debfe-2a29-448b-baa8-1e54c7eee9b1",
                "name": "multizg",
                "api_name": "multizg",
                "is_master": "true",
                "endpoints": [
                    "http://proxy01:8000"
                ],
                "hostnames": [],
                "hostnames_s3website": [],
                "master_zone": "1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f",
                "zones": [
                    {
                        "id": "1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f",
                        "name": "zone1",
                        "endpoints": [
                            "http://proxy01:8000"
                        ],
                        "log_meta": "false",
                        "log_data": "true",
                        "bucket_index_max_shards": 11,
                        "read_only": "false",
                        "tier_type": "",
                        "sync_from_all": "true",
                        "sync_from": [],
                        "redirect_zone": "",
                        "supported_features": [
                            "resharding"
                        ]
                    },
                    {
                        "id": "d7c2da60-e269-4681-a231-e744d1f0dc8a",
                        "name": "archive",
                        "endpoints": [
                            "http://proxy02:8000"
                        ],
                        "log_meta": "false",
                        "log_data": "true",
                        "bucket_index_max_shards": 11,
                        "read_only": "false",
                        "tier_type": "archive",
                        "sync_from_all": "true",
                        "sync_from": [],
                        "redirect_zone": "",
                        "supported_features": [
                            "resharding"
                        ]
                    }
                ],
                "placement_targets": [
                    {
                        "name": "default-placement",
                        "tags": [],
                        "storage_classes": [
                            "STANDARD"
                        ]
                    }
                ],
                "default_placement": "default-placement",
                "realm_id": "da2a45d7-764b-4c92-a28a-049d02c8c22f",
                "sync_policy": {
                    "groups": []
                },
                "enabled_features": [
                    "resharding"
                ]
            }
        ],
        "short_zone_ids": [
            {
                "key": "1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f",
                "val": 3465814334
            },
            {
                "key": "d7c2da60-e269-4681-a231-e744d1f0dc8a",
                "val": 702125852
            }
        ]
    },
    "master_zonegroup": "fc3debfe-2a29-448b-baa8-1e54c7eee9b1",
    "master_zone": "1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f",
    "period_config": {
        "bucket_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        },
        "user_quota": {
            "enabled": false,
            "check_on_raw": false,
            "max_size": -1,
            "max_size_kb": 0,
            "max_objects": -1
        }
    },
    "realm_id": "da2a45d7-764b-4c92-a28a-049d02c8c22f",
    "realm_name": "multisite",
    "realm_epoch": 2
}
----

Create the RADOS Gateway service for the secondary zone.

----
# ceph orch apply rgw multi.archive --realm=multisite --zone=archive --placement="2 proxy02 ceph-mon02" --port=8000
Scheduled rgw.multi.archive update...
----

Use the radosgw-admin sync status command, we can see the sync is started and a
full copy of the master zone is being synced to the Archive zone

----
# radosgw-admin sync status
          realm e72107cb-4b3f-49b9-abb0-83c68a9967f9 (multisite)
      zonegroup 6b9fbc87-3202-4a35-85d0-e3e16fc91b32 (multizg)
           zone ec5a7187-95e1-4bf2-8519-208175c81487 (Archive)
   current time 2022-12-23T14:41:08Z
  metadata sync syncing
                full sync: 1/64 shards
                full sync: 21 entries to sync
                incremental sync: 63/64 shards
                metadata is behind on 1 shards
                behind shards: [0]
      data sync source: c5dc9503-6c11-4851-91bd-f1d5ca61473c (zone1)
                        syncing
                        full sync: 63/128 shards
                        full sync: 77 buckets to sync
                        incremental sync: 65/128 shards
                        data is behind on 63 shards
                        behind shards: [4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,36,37,38,39,40,41,42,43,44,45,46,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,105,106,107,108,109,110,111,112,113,114,115,116]
----

[TIP]
====
The output can differ depending on the sync status. The shards are described as two different types during sync:
- Behind shards are shards that need a full data sync and shards needing an incremental data sync because they are not up-to-date.
- Recovery shards are shards that encountered an error during sync and marked for retry. The error mostly occurs on minor issues like acquiring a lock on a bucket. This will typically resolve itself.
====

[NOTE]
====
If you encounter sync errors in your configuration, with shards falling behind
, you can run the commandi `# radosgw-admin  sync error list`.
Also increasing the verbosity of
the RGW logs is a good place to start looking for errors, to increase the
verbosity you can follow the steps of this
https://access.redhat.com/solutions/2085183[KCS]
====

After a while if we run the same command we will probably see metadata and data in sync:

----
# radosgw-admin sync status
          realm da2a45d7-764b-4c92-a28a-049d02c8c22f (multisite)
      zonegroup fc3debfe-2a29-448b-baa8-1e54c7eee9b1 (multizg)
           zone d7c2da60-e269-4681-a231-e744d1f0dc8a (archive)
   current time 2023-03-15T11:27:00Z
zonegroup features enabled: resharding
  metadata sync syncing
                full sync: 0/64 shards
                incremental sync: 64/64 shards
                metadata is caught up with master
      data sync source: 1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f (zone1)
                        syncing
                        full sync: 0/128 shards
                        incremental sync: 128/128 shards
                        data is caught up with source
----

The replication with the Archive zone is uni-directional, so only objects from
zone1 will be replicated to the archive zone, and not the other way around.

That is why if we check the same radosgw-admin sync status command on the
cluster from zone1, we can see there us no data sync information:

----
# radosgw-admin sync status
          realm da2a45d7-764b-4c92-a28a-049d02c8c22f (multisite)
      zonegroup fc3debfe-2a29-448b-baa8-1e54c7eee9b1 (multizg)
           zone 1ed728d7-f0bf-4401-8ea8-657c9f1f4b0f (zone1)
   current time 2023-03-15T11:31:00Z
zonegroup features enabled: resharding
  metadata sync no sync (zone is master)
      data sync source: d7c2da60-e269-4681-a231-e744d1f0dc8a (archive)
                        not syncing from zone
----



=== Prepare the client Environment.

We are going to use 2 clients, the aws cli and the rclone cli tool.


First we are going to create a specific user for our tests, sone in our zone1
cluster we run:

----
# radosgw-admin user create --uid=archuser --display-name="S3 user to test the archive zone" --access-key=archuser
--secret-key=archuser
{
    "user_id": "archuser",
    "display_name": "S3 user to test the archive zone",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "subusers": [],
    "keys": [
        {
            "user": "archuser",
            "access_key": "archuser",
            "secret_key": "archuser"
        }
    ],
    "swift_keys": [],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "temp_url_keys": [],
    "type": "rgw",
    "mfa_ids": []
}
----

We are going to configure the AWS client with this user:

----
# aws configure
AWS Access Key ID [None]: archuser
AWS Secret Access Key [None]: archuser
Default region name [None]: multizg
Default output format [None]: text
----

I'm also going to create a couple of alias to make our life easier.

Alias for zone1:

----
# alias s3apiarchive='aws --endpoint=http://proxy02:8000 s3api'
# alias s3apizone1='aws --endpoint=http://proxy01:8000 s3api'
----

We will also use rclone, so lets download and install the rclone rpm:

----
# yum install https://downloads.rclone.org/v1.62.0/rclone-v1.62.0-linux-amd64.rpm -y
----

Configure the rclone client:

----
# mkdir -p /root/.config/rclone/
# cat <<EOF > /root/.config/rclone/rclone.conf
[zone1]
type = s3
provider = Other
access_key_id = archuser
secret_access_key = archuser
endpoint = http://proxy01:8000
location_constraint = multizg
acl = bucket-owner-full-control

[archive]
type = s3
provider = Ceph
access_key_id = archuser
secret_access_key = archuser
endpoint = http://proxy02:8000
location_constraint = multizg
acl = bucket-owner-full-control
EOF
----

Create some test files and get the output of their MD5 sum, so we can
review further down the line:

----
# echo "This is file 1" > /tmp/test-file-1
# echo "This is file 2" > /tmp/test-file-2
# echo "This is file 3" > /tmp/test-file-3
# md5sum /tmp/test-file-1
88c16a56754e0f17a93d269ae74dde9b  /tmp/test-file-1
# md5sum /tmp/test-file-2
db06069ef1c9f40986ffa06db4fe8fd7  /tmp/test-file-2
# md5sum /tmp/test-file-3
95227e10e2c33771e1c1379b17330c86  /tmp/test-file-3
----


=== Archive Zone testing

We have our client's ready, let's check out the archive zone.

Create a new bucket and verify the bucket has been created in all RGW zones:

----
# s3apizone1 create-bucket --bucket my-bucket
# s3apizone1 list-buckets
BUCKETS 2023-03-15T12:03:54.315000+00:00        my-bucket
OWNER   S3 user to test the archive zone        archuser
# s3apiarchive list-buckets
BUCKETS 2023-03-15T12:03:54.315000+00:00        my-bucket
OWNER   S3 user to test the archive zone        archuser
----

Verify that the object versioning is not yet configured as this is implemented lazily

----
# s3apizone1 get-bucket-versioning --bucket my-bucket
# s3apiarchive get-bucket-versioning --bucket my-bucket
----


Upload a new object to our bucket my-bucket and verify the object has been created in all RGW zones

----
# rclone copy /tmp/test-file-1 zone1:my-bucket
----


Verify how S3 versioning has been enabled in the archive zone but not in zone1:

----
# s3apiarchive get-bucket-versioning --bucket my-bucket 
{
    "Status": "Enabled",
    "MFADelete": "Disabled"
}
# s3apizone1 get-bucket-versioning --bucket my-bucket 
----


Verify object version ID is null in master and secondary zone but not in the archive zone:


----
# s3apizone1 list-object-versions --bucket my-bucket
{
    "Versions": [
        {
            "ETag": "\"88c16a56754e0f17a93d269ae74dde9b\"",
            "Size": 15,
            "StorageClass": "STANDARD",
            "Key": "test-file-1",
            "VersionId": "null",
            "IsLatest": true,
            "LastModified": "2023-03-15T12:07:12.914000+00:00",
            "Owner": {
                "DisplayName": "S3 user to test the archive zone",
                "ID": "archuser"
            }
        }
    ]
}

# s3apiarchive list-object-versions --bucket my-bucket
{
    "Versions": [
        {
            "ETag": "\"88c16a56754e0f17a93d269ae74dde9b\"",
            "Size": 15,
            "StorageClass": "STANDARD",
            "Key": "test-file-1",
            "VersionId": "6DRlC7fKtpmkvHA9zknhFA87RjyilTV",
            "IsLatest": true,
            "LastModified": "2023-03-15T12:07:12.914000+00:00",
            "Owner": {
                "DisplayName": "S3 user to test the archive zone",
                "ID": "archuser"
            }
        }
    ]
}
----


Modify the object in the master zone and verify a new version is created in the RGW archive zone.

----
# rclone copyto /tmp/test-file-2 zone1:my-bucket/test-file-1
# rclone ls zone1:my-bucket
       15 test-file-1
----

Verify a new version has been created in the RGW archive zone:

----
# s3apiarchive list-object-versions --bucket my-bucket
{
    "Versions": [
        {
            "ETag": "\"db06069ef1c9f40986ffa06db4fe8fd7\"",
            "Size": 15,
            "StorageClass": "STANDARD",
            "Key": "test-file-1",
            "VersionId": "mXoINEnZsSCDNaWwCDELVysUbnMqNqx",
            "IsLatest": true,
            "LastModified": "2023-03-15T12:13:27.057000+00:00",
            "Owner": {
                "DisplayName": "S3 user to test the archive zone",
                "ID": "archuser"
            }
        },
        {
            "ETag": "\"88c16a56754e0f17a93d269ae74dde9b\"",
            "Size": 15,
            "StorageClass": "STANDARD",
            "Key": "test-file-1",
            "VersionId": "6DRlC7fKtpmkvHA9zknhFA87RjyilTV",
            "IsLatest": false,
            "LastModified": "2023-03-15T12:07:12.914000+00:00",
            "Owner": {
                "DisplayName": "S3 user to test the archive zone",
                "ID": "archuser"
            }
        }
    ]
}
----

You can check the ETag it will match the MD5sum for the file, this is only the
case if multipart upload or object encryption is configured.

----
# md5sum /tmp/test-file-2
db06069ef1c9f40986ffa06db4fe8fd7  /tmp/test-file-2
# md5sum /tmp/test-file-1
88c16a56754e0f17a93d269ae74dde9b  /tmp/test-file-1
----


=== Recovering S3 objects from the RGW archive zone

Let's update one more version of the object

----
# rclone copyto /tmp/test-file-3 zone1:my-bucket/test-file-1
----

On the primary zone we only have one version, the current version of the object

----
# rclone --s3-versions lsl zone1:my-bucket
       15 2023-03-15 07:59:10.779573336 test-file-1
----

But in the Archive zone we have all 3 versions available:

----
# rclone --s3-versions lsl archive:my-bucket
       15 2023-03-15 07:59:10.779573336 test-file-1
       15 2023-03-15 07:59:03.782438991 test-file-1-v2023-03-15-121327-057
       15 2023-03-15 07:58:58.135330567 test-file-1-v2023-03-15-120712-914
----

So let's delete test-file1 from my-bucket in zone1, and recover the object from
the archive zone:

----
# rclone delete zone1:my-bucket/test-file-1
# rclone --s3-versions lsl zone1:my-bucket
# rclone --s3-versions lsl archive:my-bucket
       15 2023-03-15 07:59:10.779573336 test-file-1
       15 2023-03-15 07:59:03.782438991 test-file-1-v2023-03-15-121327-057
       15 2023-03-15 07:58:58.135330567 test-file-1-v2023-03-15-120712-914
----

The object has been delete from zone1, but still available in the archive zone
with all it's versions, if we recover the latest version test-file-1 it should
match with the md5 for out test-file-3.

----
# rclone copyto archive:my-bucket/test-file-1 zone1:my-bucket/test-file-1
# rclone copyto zone1:my-bucket/test-file-1 /tmp/recovered-file1
# md5sum /tmp/recovered-file1
95227e10e2c33771e1c1379b17330c86  /tmp/recovered-file1
# md5sum /tmp/test-file-3
95227e10e2c33771e1c1379b17330c86  /tmp/test-file-3
----

But let's say that we want to recover the object with the version that has date 2023-03-15-121327-057.

----
# rclone --s3-versions copyto archive:my-bucket/test-file-1-v2023-03-15-121327-057 zone1:my-bucket/test-file-1
# rclone copyto zone1:my-bucket/test-file-1 /tmp/recovered-file1
# md5sum /tmp/recovered-file1
db06069ef1c9f40986ffa06db4fe8fd7  /tmp/recovered-file1
# md5sum /tmp/test-file-2
db06069ef1c9f40986ffa06db4fe8fd7  /tmp/test-file-2
----



