<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Ceph Deployment with Cephadm :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/ceph_deploy_basic.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_ui.html">Deploy Ceph from the UI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_dashboard_metrics.html">Ceph Dashboard Management &amp; Metrics</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_configuration.html">Ceph Configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pgs.html">Ceph Health and PGs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephx.html">Rados CephX Auth/AuthZ</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_version.html">What version of Ceph am I running?</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_export.html">RBD Import/Export</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_mirroring.html">RBD Mirroring</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge.html">Challenge RBD</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_archive.html">RGW Archive Zone</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_logging.html">Troubleshooting Logs Debug Mode</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_break_and_fix.html">Troubleshooting Break &amp; Fix Hands-on</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_fio.html">Benchmarking Ceph block and File</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Challenge Solutions</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge_solution.html">Ceph Deployment Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge_solution.html">Ceph RBD Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge_solution.html">Ceph CephFS Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge_solution.html">Ceph RGW Solution</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>Core Ceph</li>
    <li><a href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/ceph_deploy_basic.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Ceph Deployment with Cephadm</h1>
<div id="preamble">
<div class="sectionbody">
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Before performing this hands-on lab you need to go through the LAB env
preparation steps described in then <a href="opentlc_lab_env.html" class="xref page">OpenTLC LAB</a> section</p>
</div>
<div class="paragraph">
<p>The automation we run in the previous section takes care of
registring/subscribing the
Ceph nodes to RH CDN and enabling the following repos:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">subscription-manager repos --disable=*
subscription-manager repos --enable=rhel-8-for-x86_64-baseos-rpms
subscription-manager repos --enable=rhel-8-for-x86_64-appstream-rpms
subscription-manager repos --enable=rhceph-5-tools-for-rhel-8-x86_64-rpms</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_run_pre_flight_playbook"><a class="anchor" href="#_run_pre_flight_playbook"></a>1. Run Pre-Flight Playbook</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>This Ansible Playbook configures the Ceph repository and prepares the storage cluster for bootstrapping. It also installs some prerequisites, such as <code>podman</code>, <code>lvm2</code>, <code>chronyd</code>, and <code>cephadm</code>.</p>
</li>
<li>
<p>The <code>cephadm-preflight</code> playbook uses the Ansible inventory file to identify the nodes in the storage cluster. Your Ansible inventory file must contain all of the nodes that are going to be part of the cluster.</p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>As part of the OpenTLC LAB env preparations you have the cephadm-ansible rpm
installed an inventory available in <code>/usr/share/cephadm-ansible/inventory</code> on the admin node for cluster1 <code>ceph-node01</code></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Prerequisites
* Initial admin host is up and running
* Ansible Automation Platform is installed on the host
* Root-level access to all nodes in the storage cluster</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Run the <code>cephadm-preflight</code> playbook before you bootstrap the initial host.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If needed edit the inventory file at <code>/usr/share/cephadm-ansible/inventory</code>
leaving:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-texinfo hljs" data-lang="texinfo"># cat /usr/share/cephadm-ansible/inventory
ceph-node01.example.com
ceph-node02.example.com
ceph-node03.example.com
proxy01.example.com

[admin]
ceph-node01.example.com</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In this inventory file we are using additional groups that are not needed
by the preflight cephadm playbook like <code>[mgmt]</code> and <code>[client]</code> so you can
remove them.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01]# ansible-playbook -i /usr/share/cephadm-ansible/inventory /usr/share/cephadm-ansible/cephadm-preflight.yml  --extra-vars "ceph_origin=rhcs"
PLAY RECAP ***********************************************************************************************************************************************************************************
ceph-node01.example.com    : ok=5    changed=0    unreachable=0    failed=0    skipped=9    rescued=0    ignored=0
ceph-node02.example.com    : ok=5    changed=0    unreachable=0    failed=0    skipped=9    rescued=0    ignored=0
ceph-node03.example.com    : ok=5    changed=0    unreachable=0    failed=0    skipped=9    rescued=0    ignored=0
proxy01.example.com        : ok=5    changed=0    unreachable=0    failed=0    skipped=9    rescued=0    ignored=0</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>We are using the <code>"ceph_origin=rhcs"</code> ansible variable to specify that we want
to use the RHCS downstream repos.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prepare_the_registry_credentials_file"><a class="anchor" href="#_prepare_the_registry_credentials_file"></a>2. Prepare the registry credentials file</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Because we are using the RHCS downstream container images, we need to
authenticate against registry.redhat.io to get access to the ceph container
images.</p>
</div>
<div class="paragraph">
<p>Then OpenTLC LAB env preparations created a file called <code>registry.json</code> on the admin node for cluster1 <code>ceph-node01</code> with
the needed auth details, if any modifications are needed edit the file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat /root/registry.json
{
 "url": "registry.redhat.io",
 "username": "user@redhat.com",
 "password": "PAssWord"
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_storage_cluster"><a class="anchor" href="#_create_storage_cluster"></a>3. Create Storage Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <code>cephadm</code> utility performs the following tasks during the bootstrap process:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a monitor and manager daemon for the new cluster on the local host.</p>
</li>
<li>
<p>Generate a new SSH key for the Ceph cluster and add it to the root user’s /root/.ssh/authorized_keys file.</p>
</li>
<li>
<p>Write a copy of the public key to /etc/ceph/ceph.pub.</p>
</li>
<li>
<p>Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster.</p>
</li>
<li>
<p>Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring.</p>
</li>
<li>
<p>Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring.</p>
</li>
<li>
<p>Deploys a basic monitoring stack with Prometheus, Grafana, and other tools such as <code>node-exporter</code> and <code>alertmanager</code>.</p>
</li>
<li>
<p>Running the bootstrapping process establishes the default user name and password for the initial login to the dashboard. Be sure to change the password after you log in.</p>
</li>
<li>
<p><strong>Prerequisites</strong></p>
<div class="ulist">
<ul>
<li>
<p>An IP address for the first Ceph Monitor container, usually the IP address for the first node in the storage
cluster</p>
</li>
<li>
<p>Root access to all nodes</p>
</li>
<li>
<p>Login access to <code>registry.redhat.io</code></p>
</li>
<li>
<p>A minimum of 10 GB of free space for <code>/var/lib/containers/</code></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In this lab environment, you do not have 10 GB of free space, but you can safely ignore warnings related to this.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_bootstrap_new_storage_cluster"><a class="anchor" href="#_bootstrap_new_storage_cluster"></a>4. Bootstrap New Storage Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Bootstrap a storage cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01]# cephadm \
	bootstrap \
	--registry-json /root/registry.json \
	--mon-ip 192.168.56.61

Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit chronyd.service is enabled and running
firewalld ready
Ceph Dashboard is now available at:
         URL: https://ceph-node01.example.com:8443/
        User: admin
    Password: PASSWORD

You can access the Ceph CLI with:
    sudo /usr/sbin/cephadm shell --fsid 99e4add2-d971-11eb-ad7e-2cc260754989 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:
    ceph telemetry on
:
Bootstrap complete.</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can also deploy the full ceph cluster deployment with the help of a
specification yaml file, if you would like to use this option you have an
example in file: <code>cat /root/cluster-spec.yaml</code> on node: <code>ceph-node01</code>, to
reference the spec file you can use the <code>--apply-spec /root/cluster-spec.yaml</code></p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If the storage cluster includes multiple networks and interfaces, be sure to choose a network that is
accessible by any node that uses the storage cluster.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The Ceph dashboard must be accessed by using the Public IP of the <code>ceph-node01</code> host by running <code>curl ifconfig.co</code> or by checking the email message you received for the URL to the lab console.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Currently the OpenTLC ENV only has the 8443 port open to the external world for
the ceph-mon0X clusters, if we want to access the dashboard for the ceph-node0X
cluster we have to use SSH tunneling.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Currently the OpenTLC ENV only has the 8443 port open to the external world, if
we want to access ceph metrics provided by grafana standalone or in the
dashboard we will need to use ssh tunneling and also forward DNS resolution to
the remote/opentlc dns server, in linux we can use <code>sshuttle</code> for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># sshuttle --dns -r USER@workstation.dynamic.opentlc.com 192.168.56.0/24</code></pre>
</div>
</div>
<div class="paragraph">
<p>Install sshutle for Mac:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
$ brew install sshuttle //</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Get the Public IP address of <code>ceph-node01</code> to access the dashboard from your browser:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01 cephadm-ansible]# curl ifconfig.co
52.117.178.51</code></pre>
</div>
</div>
<div class="paragraph">
<p>Go to a browser and enter a URL matching the pattern <code><a href="https://$IP_ADDRESS:8443" class="bare">https://$IP_ADDRESS:8443</a></code>, using the IP address returned in the previous step and accepting the certificate and key in warnings:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use the admin username and password provided earlier.</p>
</li>
<li>
<p>The web interface asks you to change the password for upon first login as the</p>
</li>
<li>
<p>admin user to the dashboard[you can avoid this by using the option].</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you see <code>ceph-node02.example.com</code> as the browser link while trying to access the dashboard, change the IP address to the one provided by the <code>ceph-node02</code> server on your browser.</p>
</div>
<div class="paragraph">
<p>If the admin user does not work, create a new user called <code>admin1</code> with a password stored in a file called <code>password.txt</code> using the <code>ceph dashboard ac-user-create admin1 -i password.txt administrator</code> command.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_invoke_cephadm_shell_command"><a class="anchor" href="#_invoke_cephadm_shell_command"></a>5. Invoke <code>cephadm shell</code> Command</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <code>cephadm shell</code> command launches a <code>bash</code> shell in a container with all of the Ceph packages installed. This enables you to perform “Day One” cluster setup tasks, such as adding hosts, and to invoke <code>ceph</code> commands.</p>
</div>
<div class="paragraph">
<p>There are two ways to invoke the <code>cephadm</code> shell:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To launch the shell, enter <code>cephadm shell</code> at the system prompt, which enables you to run Ceph commands in interactive shell mode:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01 cephadm-ansible]# cephadm shell
Inferring fsid 99e4add2-d971-11eb-ad7e-2cc260754989
Inferring config /var/lib/ceph/99e4add2-d971-11eb-ad7e-2cc260754989/mon.ceph-node01.example.com/config
Using recent ceph image docker.io/ceph/ceph@sha256:54e95ae1e11404157d7b329d0bef866ebbb214b195a009e87aae4eba9d282949
[ceph: root@ceph-node01 /]# ceph -s
  cluster:
    id:     99e4add2-d971-11eb-ad7e-2cc260754989
    health: HEALTH_WARN
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum ceph-node01.example.com (age 57m)
    mgr: ceph-node01.example.com.lwycwe(active, since 56m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>To execute a single command, at the system prompt type <code>cephadm shell</code> and the command you want to execute:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01 cephadm-ansible]# cephadm shell ceph -s
Inferring fsid 99e4add2-d971-11eb-ad7e-2cc260754989
Inferring config /var/lib/ceph/99e4add2-d971-11eb-ad7e-2cc260754989/mon.ceph-node01.example.com/config
Using recent ceph image docker.io/ceph/ceph@sha256:54e95ae1e11404157d7b329d0bef866ebbb214b195a009e87aae4eba9d282949
  cluster:
    id:     99e4add2-d971-11eb-ad7e-2cc260754989
    health: HEALTH_WARN
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum ceph-node01.example.com (age 57m)
    mgr: ceph-node01.example.com.lwycwe(active, since 56m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure that the host from which you are invoking the <code>cephadm shell</code> command has copies of the keyring and <code>ceph.conf</code> files. If you are using the bootstrap node to invoke the shell, the files are already installed in <code>/etc/ceph</code>. If you are using a different node to invoke the shell, the Ceph CLI is not accessible from within the <code>cephadm</code> shell. In that case, exit the shell and copy the keyring and <code>ceph.conf</code> files to <code>/etc/ceph</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_verify_cephadm_bootstrap_process"><a class="anchor" href="#_verify_cephadm_bootstrap_process"></a>6. Verify <code>cephadm</code> Bootstrap Process</h2>
<div class="sectionbody">
<div class="paragraph">
<p>After the <code>cephadm</code> bootstrap process is complete, you can verify that your new installation is running properly. <code>cephadm</code> installs and configures <code>mon</code>, <code>mgr</code> , <code>crash</code>, <code>prometheus</code>, <code>grafana</code>, <code>alertmanager</code>, and <code>node-exporter</code>.</p>
</div>
<div class="paragraph">
<p>Launch the <code>cephadm</code> shell:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01 cephadm-ansible]# cephadm shell</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">Sample Output</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-texinfo hljs" data-lang="texinfo">Inferring fsid 99e4add2-d971-11eb-ad7e-2cc260754989
Inferring config /var/lib/ceph/99e4add2-d971-11eb-ad7e-2cc260754989/mon.ceph-node01.example.com/config
Using recent ceph image docker.io/ceph/ceph@sha256:54e95ae1e11404157d7b329d0bef866ebbb214b195a009e87aae4eba9d282949</code></pre>
</div>
</div>
<div class="paragraph">
<p>Verify that the installation is up and running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph -s
  cluster:
    id:     99e4add2-d971-11eb-ad7e-2cc260754989
    health: HEALTH_WARN
            OSD count 0 &lt; osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum ceph-node01.example.com (age 67m)
    mgr: ceph-node01.example.com.lwycwe(active, since 66m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:</code></pre>
</div>
</div>
<div class="paragraph">
<p>List the services that are running on the new installation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch ls
NAME           PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
alertmanager              1/1  21s ago    67m  count:1
crash                     1/1  21s ago    67m  *
grafana                   1/1  21s ago    67m  count:1
mgr                       1/2  21s ago    67m  count:2
mon                       1/5  21s ago    67m  count:5
node-exporter             1/1  21s ago    67m  *
prometheus                1/1  21s ago    67m  count:1</code></pre>
</div>
</div>
<div class="paragraph">
<p>View the daemon processes that are running on the new installation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch ps
NAME                               HOST                    PORTS        STATUS         REFRESHED  AGE  VERSION  IMAGE ID      CONTAINER ID
alertmanager.ceph-node01            ceph-node01.example.com  *:9093,9094  running (66m)  26s ago    67m  0.20.0   0881eb8f169f  4a707803a4d7
crash.ceph-node01                   ceph-node01.example.com               running (67m)  26s ago    67m  16.2.4   8d91d370c2b8  c4d5af688177
grafana.ceph-node01                 ceph-node01.example.com  *:3000       running (66m)  26s ago    66m  6.7.4    ae5c36c3d3cd  5dbcb83564d0
mgr.ceph-node01.example.com.lwycwe  ceph-node01.example.com  *:9283       running (68m)  26s ago    68m  16.2.4   8d91d370c2b8  da76c87f27de
mon.ceph-node01.example.com         ceph-node01.example.com               running (68m)  26s ago    68m  16.2.4   8d91d370c2b8  2d0b697a1e41
node-exporter.ceph-node01           ceph-node01.example.com  *:9100       running (66m)  26s ago    66m  0.18.1   e5a616e4b9cf  f76fc8ba8c6c
prometheus.ceph-node01              ceph-node01.example.com  *:9095       running (66m)  26s ago    66m  2.18.1   de242295e225  2f2e63ecb350</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_add_new_hosts_to_the_ceph_cluster"><a class="anchor" href="#_add_new_hosts_to_the_ceph_cluster"></a>7. Add New Hosts to the Ceph Cluster</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Bootstrapping the installation of Red Hat Ceph Storage creates a basic single-node storage cluster, consisting of one Monitor daemon and one Manager daemon. You can use <code>cephadm</code> to add more hosts to the newly created storage cluster.</p>
</div>
<div class="paragraph">
<p>Install the storage cluster’s public SSH key in the root user’s <code>authorized_keys</code> file on the new host:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node01.example.com
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node02.example.com
ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-node03.example.com
ssh-copy-id -f -i /etc/ceph/ceph.pub root@proxy01.example.com</code></pre>
</div>
</div>
<div class="paragraph">
<p>On the bootstrap node, launch the <code>cephadm</code> shell to access the <code>cephadm</code> orchestrator:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01 cephadm-ansible]# cephadm shell
Inferring fsid 99e4add2-d971-11eb-ad7e-2cc260754989
Inferring config /var/lib/ceph/99e4add2-d971-11eb-ad7e-2cc260754989/mon.ceph-node01.example.com/config
Using recent ceph image docker.io/ceph/ceph@sha256:54e95ae1e11404157d7b329d0bef866ebbb214b195a009e87aae4eba9d282949</code></pre>
</div>
</div>
<div class="paragraph">
<p>Add the new host to the storage cluster using the <code>addr</code> option to identify hosts with IP address in addition to the host name:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch host add ceph-node02 192.168.56.62
[ceph: root@ceph-node01 /]# ceph orch host add ceph-node03 192.168.56.63
[ceph: root@ceph-node01 /]# ceph orch host add proxy01 192.168.56.24
Added host 'ceph-node02'
Added host 'ceph-node03'
Added host 'proxy01'</code></pre>
</div>
</div>
<div class="paragraph">
<p>View the status of the storage cluster and verify that the new
host was added:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch host ls
HOST         ADDR           LABELS  STATUS
ceph-node01  192.168.56.61  _admin
ceph-node02  192.168.56.62
ceph-node03  192.168.56.63
proxy01      192.168.56.24
4 hosts in cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>List the services that are running on the new installation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch ls
NAME           PORTS        RUNNING  REFRESHED  AGE  PLACEMENT
alertmanager   ?:9093,9094      1/1  0s ago     5m   count:1
crash                           4/4  1s ago     5m   *
grafana        ?:3000           1/1  0s ago     5m   count:1
mgr                             2/2  1s ago     5m   count:2
mon                             4/5  1s ago     5m   count:5
node-exporter  ?:9100           4/4  1s ago     5m   *
prometheus     ?:9095           1/1  0s ago     5m   count:1</code></pre>
</div>
</div>
<div class="paragraph">
<p>View the daemon processes that are running on the new installation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch ps
NAME                       HOST         PORTS        STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION          IMAGE ID      CONTAINER ID
alertmanager.ceph-node01   ceph-node01  *:9093,9094  running (3m)     2m ago   7m    16.9M        -                   2de2e7d63e1b  448ef753db57
crash.ceph-node01          ceph-node01               running (7m)     2m ago   7m    7998k        -  16.2.8-85.el8cp  b2c997ff1898  f2c0eac730b8
crash.ceph-node02          ceph-node02               running (5m)     2m ago   5m    11.8M        -  16.2.8-85.el8cp  b2c997ff1898  52a389476b8f
crash.ceph-node03          ceph-node03               running (4m)     2m ago   4m    13.7M        -  16.2.8-85.el8cp  b2c997ff1898  1e75632d09dd
crash.proxy01              proxy01                   running (4m)     2m ago   4m    15.2M        -  16.2.8-85.el8cp  b2c997ff1898  611c2b6186c8
grafana.ceph-node01        ceph-node01  *:3000       running (6m)     2m ago   7m    57.7M        -  8.3.5            a283f9df3197  9fac3b14d304
mgr.ceph-node01.cjknxe     ceph-node01  *:9283       running (9m)     2m ago   9m     481M        -  16.2.8-85.el8cp  b2c997ff1898  2f240abefa18
mgr.ceph-node02.himyza     ceph-node02  *:8443,9283  running (5m)     2m ago   5m     413M        -  16.2.8-85.el8cp  b2c997ff1898  92ec80963e86
mon.ceph-node01            ceph-node01               running (9m)     2m ago   9m    76.0M    2048M  16.2.8-85.el8cp  b2c997ff1898  8d6b3d441a4d
mon.ceph-node02            ceph-node02               running (5m)     2m ago   5m    69.5M    2048M  16.2.8-85.el8cp  b2c997ff1898  d1500bb807c7
mon.ceph-node03            ceph-node03               running (4m)     2m ago   4m    63.4M    2048M  16.2.8-85.el8cp  b2c997ff1898  91e27c0564e0
mon.proxy01                proxy01                   running (4m)     2m ago   4m    64.7M    2048M  16.2.8-85.el8cp  b2c997ff1898  9ae1e3ae75f8
node-exporter.ceph-node01  ceph-node01  *:9100       running (7m)     2m ago   7m    20.3M        -                   6c8570b1928b  8f19aa43c639
node-exporter.ceph-node02  ceph-node02  *:9100       running (5m)     2m ago   5m    17.9M        -                   6c8570b1928b  bf578a47f724
node-exporter.ceph-node03  ceph-node03  *:9100       running (3m)     2m ago   3m    15.9M        -                   6c8570b1928b  71d901f560b0
node-exporter.proxy01      proxy01      *:9100       running (3m)     2m ago   3m    17.2M        -                   6c8570b1928b  d0b22564d863
prometheus.ceph-node01     ceph-node01  *:9095       running (3m)     2m ago   7m    39.9M        -                   39847ff1cddf  8e34b09df769</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we only have 4 nodes, we need to reduce the number of mons to three:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[ceph: root@ceph-node01 /]# ceph orch apply mon --placement='ceph-node01,ceph-node02,ceph-node03'
Scheduled mon update...
[root@ceph-node01 ~]# ceph -s | grep mon
    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03 (age 15s)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_add_osds"><a class="anchor" href="#_add_osds"></a>8. Add OSDs</h2>
<div class="sectionbody">
<div class="paragraph">
<p><code>cephadm</code> does not provision an OSD on a device that is not available. A storage device is considered available if meets all of the following conditions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Must not have any partitions</p>
</li>
<li>
<p>Must not have any LVM state</p>
</li>
<li>
<p>Must not be mounted</p>
</li>
<li>
<p>Must not contain a file system</p>
</li>
<li>
<p>Must not contain a Ceph BlueStore OSD</p>
</li>
<li>
<p>Must be larger than 5 GB</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>ceph-mon</code> servers are also used as OSD servers. Each server has at least <code>/dev/vdb</code> disk that is used as an OSD disk.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[root@ceph-node01 ~]# ceph orch device ls
HOST         PATH      TYPE  DEVICE ID              SIZE  AVAILABLE  REFRESHED  REJECT REASONS
ceph-node01  /dev/vdb  hdd   2d61773d-7328-4f6f-9  10.7G  Yes        29s ago
ceph-node01  /dev/vdc  hdd   774b2784-8220-4578-a  10.7G  Yes        29s ago
ceph-node01  /dev/vdd  hdd   51d89573-192e-4145-8  10.7G  Yes        29s ago
ceph-node01  /dev/vde  hdd   30f26450-0d1e-4fd2-b  10.7G  Yes        29s ago
ceph-node02  /dev/vdb  hdd   eaf7a900-905f-46a9-9  10.7G  Yes        6s ago
ceph-node02  /dev/vdc  hdd   61963761-a821-4206-9  10.7G  Yes        6s ago
ceph-node02  /dev/vdd  hdd   ab4ae2e2-9c09-446f-b  10.7G  Yes        6s ago
ceph-node02  /dev/vde  hdd   ce49a03d-c56c-49f5-9  10.7G  Yes        6s ago
ceph-node03  /dev/vdb  hdd   a445ceda-53cb-4da0-a  10.7G  Yes        6m ago
ceph-node03  /dev/vdc  hdd   41090fad-7ec1-4c7b-9  10.7G  Yes        6m ago
ceph-node03  /dev/vdd  hdd   4e55c310-fa55-42a8-8  10.7G  Yes        6m ago
ceph-node03  /dev/vde  hdd   d74446e5-3b7a-4bec-a  10.7G  Yes        6m ago</code></pre>
</div>
</div>
<div class="paragraph">
<p>Make sure that the disk is clean from any partitions on all three hosts:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch device zap ceph-node01 /dev/vdb --force
[ceph: root@ceph-node01 /]# ceph orch device zap ceph-node02 /dev/vdb --force
[ceph: root@ceph-node01 /]# ceph orch device zap ceph-node03 /dev/vdb --force</code></pre>
</div>
</div>
<div class="paragraph">
<p>Invoke the <code>cephadm shell</code> command to give yourself the ability to add the disks as OSDs to the cluster.</p>
</div>
<div class="paragraph">
<p>Create the OSD daemons with the Ceph orchestrator, we can use different
parameter depending on the OSDs per node that we want to configure we could
use <code>--all-available-devices</code></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can use the --dry-run option to preview what disks will actually be used</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph orch apply osd --all-available-devices --dry-run
Scheduled osd.all-available-devices update...
[root@ceph-node01 ~]# ceph orch apply osd --all-available-devices --dry-run
WARNING! Dry-Runs are snapshots of a certain point in time and are bound
to the current inventory setup. If any of these conditions change, the
preview will be invalid. Please make sure to have a minimal
timeframe between planning and applying the specs.
################
OSDSPEC PREVIEWS
################
----------------------------------------------------------------
|SERVICE  |NAME                   |HOST         |DATA      |DB  |WAL  |
----------------------------------------------------------------
|osd      |all-available-devices  |ceph-node01  |/dev/vdb  |-   |-    |
|osd      |all-available-devices  |ceph-node01  |/dev/vdc  |-   |-    |
|osd      |all-available-devices  |ceph-node01  |/dev/vdd  |-   |-    |
|osd      |all-available-devices  |ceph-node01  |/dev/vde  |-   |-    |
|osd      |all-available-devices  |ceph-node02  |/dev/vdb  |-   |-    |
|osd      |all-available-devices  |ceph-node02  |/dev/vdc  |-   |-    |
|osd      |all-available-devices  |ceph-node02  |/dev/vdd  |-   |-    |
|osd      |all-available-devices  |ceph-node02  |/dev/vde  |-   |-    |
|osd      |all-available-devices  |ceph-node03  |/dev/vdb  |-   |-    |
|osd      |all-available-devices  |ceph-node03  |/dev/vdc  |-   |-    |
|osd      |all-available-devices  |ceph-node03  |/dev/vdd  |-   |-    |
|osd      |all-available-devices  |ceph-node03  |/dev/vde  |-   |-    |
----------------------------------------------------------------</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or select specific drives from certain hosts, for example we are going to add 3
OSDs using drive <code>/dev/vdb</code> from nodes <code>ceph-node01,02,03</code>:</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The ceph orch daemon add osd does not have the --dry-run option available</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For Advanced OSD configuration we recomended to use a OSD specification file.
You have examples on configuration options you can use this <a href="https://docs.ceph.com/en/quincy/cephadm/services/osd/#advanced-osd-service-specifications">link</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[root@ceph-node01 ~]#  ceph orch daemon add osd ceph-node0[1-3]:/dev/vdb
Created osd(s) 2 on host 'ceph-node01', Created osd(s) 1 on host 'ceph-node02', Created osd(s) 0 on host 'ceph-node03'</code></pre>
</div>
</div>
<div class="paragraph">
<p>List the services that are running on the new installation to verify that the OSDs are created:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[root@ceph-node01 ~]# ceph orch ps | grep osd
osd.0                      ceph-node03               running (95s)    90s ago  95s    24.7M    4096M  16.2.8-85.el8cp  b2c997ff1898  f2b304ea66f4
osd.1                      ceph-node02               running (94s)    88s ago  94s    35.4M    4096M  16.2.8-85.el8cp  b2c997ff1898  f4a910345f9f
osd.2                      ceph-node01               running (94s)    85s ago  93s    30.7M    4096M  16.2.8-85.el8cp  b2c997ff1898  428c6c0289cf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Because this is a lab environment and there are insufficient resources to handle the scrubbing process, stop the deep scrub in the cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph osd set nodeep-scrub
nodeep-scrub is set</code></pre>
</div>
</div>
<div class="paragraph">
<p>Determine the Ceph storage cluster status:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph -s
cluster:
    id:     bdef9acc-d99c-11eb-9652-2cc260754989
    health: HEALTH_OK
                nodeep-scrub flag(s) set
 ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>List the OSDs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh">[ceph: root@ceph-node01 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.02939  root default
-5         0.00980      host ceph-node01
 2    hdd  0.00980          osd.2            up   1.00000  1.00000
-7         0.00980      host ceph-node02
 1    hdd  0.00980          osd.1            up   1.00000  1.00000
-3         0.00980      host ceph-node03
 0    hdd  0.00980          osd.0            up   1.00000  1.00000</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>You can safely ignore any <code>slow ops</code>-related warnings. These are likely due to lack of time synchronization on the cluster nodes.</p>
</li>
</ul>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
