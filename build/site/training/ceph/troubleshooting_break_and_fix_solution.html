<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Troubleshooting with Break and Fix Exercises :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/troubleshooting_break_and_fix_solution.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_ui.html">Deploy Ceph from the UI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_dashboard_metrics.html">Ceph Dashboard Management &amp; Metrics</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_configuration.html">Ceph Configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pgs.html">Ceph Health and PGs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephx.html">Rados CephX Auth/AuthZ</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_version.html">What version of Ceph am I running?</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_export.html">RBD Import/Export</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_mirroring.html">RBD Mirroring</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge.html">Challenge RBD</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_logging.html">Troubleshooting Logs Debug Mode</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_break_and_fix.html">Troubleshooting Break &amp; Fix Hands-on</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_fio.html">Benchmarking Ceph block and File</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Challenge Solutions</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge_solution.html">Ceph Deployment Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge_solution.html">Ceph RBD Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge_solution.html">Ceph CephFS Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge_solution.html">Ceph RGW Solution</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li><a href="troubleshooting_break_and_fix_solution.html">Troubleshooting with Break and Fix Exercises</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/troubleshooting_break_and_fix_solution.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Troubleshooting with Break and Fix Exercises</h1>
<div class="sect1">
<h2 id="_prepare_the_environment"><a class="anchor" href="#_prepare_the_environment"></a>Prepare the Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To be able to complete the Break &amp; Fix exercises we need to fulfil certain pre-reqs:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Deploy the ceph cluster <code>ceph-mon0X</code>.</p>
</li>
<li>
<p>Use the script available in the admin node <code>ceph-mon01</code> called <code>/root/one_step_cluster_deploy.sh</code></p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Once the cluster is deployed and in a healthy state, you will have ceph,rgw and
mds services running</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph -s
  cluster:
    id:     3c6182ba-9b1d-11ed-87b3-2cc260754989
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum ceph-mon01,ceph-mon02,ceph-mon03 (age 22m)
    mgr: ceph-mon01.ndicbs(active, since 22m)
    mds: 1/1 daemons up
    osd: 3 osds: 3 up (since 22m), 3 in (since 13h)
    rgw: 1 daemon active (1 hosts, 1 zones)
...</pre>
</div>
</div>
<div class="paragraph">
<p>At this point from the <code>workstation</code> node, run the <code>break_and_fix.yaml</code> playbook</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ansible-playbook -i /root/cluster2 /root/cephadmdeploy/ansible-cephadm-deploy/break-fix.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>After the playbook finishes its run, you will have three different issues on your ceph cluster that need fixing</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Take a look at the cluster health in detail</p>
</li>
<li>
<p>Increase debug log verbosity if needed</p>
</li>
<li>
<p>Take a look at the container logs during the startup</p>
</li>
<li>
<p>ceph pg PG.NUM query can help identify the problem</p>
</li>
<li>
<p>Once problems have been fixed restart the affected daemons if needed</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A first look at the cluster health we can see several problems:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph -s
  cluster:
    id:     cd172aa4-b8fe-11ed-b71d-2cc260754989
    health: HEALTH_WARN
            2 failed cephadm daemon(s)
            1 filesystem is degraded
            1 MDSs report slow metadata IOs
            1 osds down
            1 host (1 osds) down
            Reduced data availability: 32 pgs inactive
            Degraded data redundancy: 257/683 objects degraded (37.628%), 58 pgs degraded, 225 pgs undersized

  services:
    mon: 3 daemons, quorum ceph-mon01,ceph-mon02,ceph-mon03 (age 19m)
    mgr: ceph-mon01.anygbp(active, since 19m)
    mds: 1/1 daemons up
    osd: 3 osds: 2 up (since 8m), 3 in (since 18m)

  data:
    volumes: 0/1 healthy, 1 recovering
    pools:   8 pools, 225 pgs
    objects: 213 objects, 8.6 KiB
    usage:   25 MiB used, 30 GiB / 30 GiB avail
    pgs:     14.222% pgs not active
             257/683 objects degraded (37.628%)
             148 active+undersized
             45  active+undersized+degraded
             19  undersized+peered
             13  undersized+degraded+peered</pre>
</div>
</div>
<div class="paragraph">
<p>My first priority would be to see what is happening with the <code>14.222% pgs not
active</code> as these PGs are not serving any IO:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph pg dump_stuck inactive
PG_STAT  STATE                       UP     UP_PRIMARY  ACTING  ACTING_PRIMARY
2.1c     undersized+degraded+peered  [0,2]           0   [0,2]               0
2.1d     undersized+degraded+peered  [2,0]           2   [2,0]               2
2.1e              undersized+peered  [2,0]           2   [2,0]               2
2.1f     undersized+degraded+peered  [2,0]           2   [2,0]               2
2.18              undersized+peered  [2,0]           2   [2,0]               2
2.19              undersized+peered  [2,0]           2   [2,0]               2
2.1a              undersized+peered  [0,2]           0   [0,2]               0
2.1b              undersized+peered  [2,0]           2   [2,0]               2
2.14     undersized+degraded+peered  [2,0]           2   [2,0]               2
2.15     undersized+degraded+peered  [0,2]           0   [0,2]               0
2.16              undersized+peered  [0,2]           0   [0,2]               0
2.17              undersized+peered  [2,0]           2   [2,0]               2
2.10     undersized+degraded+peered  [2,0]           2   [2,0]               2
2.11              undersized+peered  [0,2]           0   [0,2]               0
2.12              undersized+peered  [0,2]           0   [0,2]               0
2.13     undersized+degraded+peered  [2,0]           2   [2,0]               2
2.9               undersized+peered  [0,2]           0   [0,2]               0
2.8               undersized+peered  [0,2]           0   [0,2]               0
2.f      undersized+degraded+peered  [0,2]           0   [0,2]               0
2.e      undersized+degraded+peered  [0,2]           0   [0,2]               0
2.c               undersized+peered  [2,0]           2   [2,0]               2
2.d      undersized+degraded+peered  [2,0]           2   [2,0]               2
2.a               undersized+peered  [2,0]           2   [2,0]               2
2.b      undersized+degraded+peered  [2,0]           2   [2,0]               2
2.4               undersized+peered  [0,2]           0   [0,2]               0
2.5               undersized+peered  [2,0]           2   [2,0]               2
2.2               undersized+peered  [2,0]           2   [2,0]               2
2.3               undersized+peered  [2,0]           2   [2,0]               2
2.1               undersized+peered  [0,2]           0   [0,2]               0
2.0               undersized+peered  [0,2]           0   [0,2]               0
2.7      undersized+degraded+peered  [0,2]           0   [0,2]               0
2.6      undersized+degraded+peered  [2,0]           2   [2,0]               2</pre>
</div>
</div>
<div class="paragraph">
<p>I can see that all inactive PGs are from pool 2, and check who is pool 2.</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph osd lspools
1 rbdpool
2 cephfs.cephfs.meta
3 cephfs.cephfs.data
4 device_health_metrics
5 .rgw.root
6 default.rgw.log
7 default.rgw.control
8 default.rgw.meta</pre>
</div>
</div>
<div class="paragraph">
<p>So all affected PGs are from the cephfs.cephfs.meta pool, that explains why I
also have my CephFS filesystem down:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph -s
  cluster:
    id:     cd172aa4-b8fe-11ed-b71d-2cc260754989
    health: HEALTH_WARN
            1 filesystem is degraded</pre>
</div>
</div>
<div class="paragraph">
<p>It seems strange that with 2 OSDs available in the Acting set, the PGs are
stuck in Peered</p>
</div>
<div class="paragraph">
<p>I can check the status of a pg for information, there is something odd with the
<code>num_object_copies</code> saying <code>5</code>, and <code>num_objects_degraded `3</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph pg 2.7 query
...
            "stat_sum": {
                "num_bytes": 0,
                "num_objects": 1,
                "num_object_clones": 0,
                "num_object_copies": 5,   &lt;-------------
                "num_objects_missing_on_primary": 0,
                "num_objects_missing": 0,
                "num_objects_degraded": 3, &lt;------------
...</pre>
</div>
</div>
<div class="paragraph">
<p>The description of peered gives us a huge clue <a href="https://docs.ceph.com/en/quincy/rados/operations/pg-states/">Doc</a>:</p>
</div>
<div class="paragraph">
<p><strong>peered:</strong>
The placement group has peered, but cannot serve client IO due to not having enough copies to reach the pool’s configured min_size parameter. Recovery may occur in this state, so the pg may heal up to min_size eventually.</p>
</div>
<div class="paragraph">
<p>So it seems that the min_size parameter is higher than the amount of hosts that
we have, and by the pg query info it seems set to 5, lets check:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph osd pool ls detail | grep 'pool 2'
pool 2 'cephfs.cephfs.meta' replicated size 5 min_size 3 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 41 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs</pre>
</div>
</div>
<div class="paragraph">
<p>Aha, so the size is set to 5 and min_size set to 3, the failure domain for the crush rule is set to host, and we only have 3 hosts available</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[root@ceph-mon01 ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                STATUS  REWEIGHT  PRI-AFF
-1         0.02939  root default
-3         0.02939      datacenter DC1
-2         0.00980          host ceph-mon01
 0    hdd  0.00980              osd.0            up   1.00000  1.00000
-4         0.00980          host ceph-mon02
 2    hdd  0.00980              osd.2            up   1.00000  1.00000
-5         0.00980          host ceph-mon03
 1    hdd  0.00980              osd.1          down         0  1.00000</pre>
</div>
</div>
<div class="paragraph">
<p>We are not able to comply with min_size 3 because we have 1 OSD in down state, so lets fix the size and min_size first</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph osd pool set cephfs.cephfs.meta size 3
set pool 2 size to 3
# ceph osd pool set cephfs.cephfs.meta min_size 2
set pool 2 min_size to 2

# ceph pg dump_stuck inactive
ok</pre>
</div>
</div>
<div class="paragraph">
<p>Great!, no inactive PGs!, all PGs are serving IO, let&#8217;s check the cephfs pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph fs status
cephfs - 0 clients
======
RANK  STATE             MDS                ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  cephfs.ceph-mon03.ceyxck  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL</pre>
</div>
</div>
<div class="paragraph">
<p>Nice, just by fixing the inactive PGs of the cephfs metadata pool we are back ONLINE!</p>
</div>
<div class="paragraph">
<p>Let&#8217;s try and see what is going on with osd.1 that is in down state, first
thing I would try is a quick restart:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch ps  | grep osd.1
osd.1                           ceph-mon03               error             9m ago  60m        -    4096M  &lt;unknown&gt;          &lt;unknown&gt;     &lt;unknown&gt;
# ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'ceph-mon03'
# ceph orch ps --refresh | grep osd.1
osd.1                           ceph-mon03               error             4s ago  60m        -    4096M  &lt;unknown&gt;          &lt;unknown&gt;     &lt;unknown&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>No luck.., the daemon is still in error state, let&#8217;s check the output log on
the host where OSD.1 is running ceph-node03</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ssh ceph-node03
# cephadm ls | grep osd.1
        "name": "osd.1",
        "systemd_unit": "ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989@osd.1",</pre>
</div>
</div>
<div class="paragraph">
<p>Just for basic output and checking quickly what can be wrong with OSD.1
start-up I do a follow to the journalctl(I have logging to journald configured)
and just do a grep by osd.1</p>
</div>
<div class="listingblock">
<div class="content">
<pre># journalctl -f | grep osd.1
Mar 02 09:49:28 ceph-mon03 systemd[1]: Started Ceph osd.1 for cd172aa4-b8fe-11ed-b71d-2cc260754989.
Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-mon-ceph-mon03[10586]: debug 2023-03-02T14:49:28.299+0000 7f7d1ea65700  0 cephx server osd.1: couldn't find entity name: osd.1
Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-osd-1[30984]: debug 2023-03-02T14:49:28.299+0000 7f3a52ca1700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-osd-1[30984]: debug 2023-03-02T14:49:28.301+0000 7f3a524a0700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-osd-1[30984]: debug 2023-03-02T14:49:28.303+0000 7f3a51c9f700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-osd-1[30984]: failed to fetch mon config (--no-mon-config to skip)
Mar 02 09:49:28 ceph-mon03 systemd[1]: ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989@osd.1.service: Main process exited, code=exited, status=1/FAILURE
Mar 02 09:49:29 ceph-mon03 systemd[1]: ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989@osd.1.service: Failed with result 'exi</pre>
</div>
</div>
<div class="paragraph">
<p>Strate away we can see 2 lines with important information:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-mon-ceph-mon03[10586]: debug 2023-03-02T14:49:28.299+0000 7f7d1ea65700  0 cephx server osd.1: couldn't find entity name: osd.1
Mar 02 09:49:28 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-osd-1[30984]: debug 2023-03-02T14:49:28.299+0000 7f3a52ca1700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]</pre>
</div>
</div>
<div class="paragraph">
<p>cephx authentication is complaning that osd.1 doesn&#8217;t have a key: <code>cephx server
osd.1: couldn&#8217;t find entity name: osd.1</code> and auth is failing
<code>handle_auth_bad_method server allowed_methods [2] but i only support [2]</code>,
next stop check if the key in <code>ceph auth ls</code> matches with the key the osd is
using to start up.</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph auth ls | grep osd.1
installed auth entries:

# ceph auth ls | grep osd.2
installed auth entries:
osd.2</pre>
</div>
</div>
<div class="paragraph">
<p>What no key for OSD.1 ??, no wonder it doesn&#8217;t start&#8230;&#8203;, I&#8217;m going to re-create
the cephx key for OSD.1</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph auth get-or-create osd.1 mon 'allow profile osd' mgr 'allow profile osd' osd 'allow *'
 [osd.1]
	key = AQBaugBkdGa7CRAAG5CltpQSVYZ68aq81lYxyg==
# ceph auth ls | grep osd.1
installed auth entries:

osd.1</pre>
</div>
</div>
<div class="paragraph">
<p>Ready!, let&#8217;s restart the OSD.1 daemon</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'ceph-mon03'
(failed reverse-i-search)`grpe': ceph auth ls | ^Cep osd.1
[root@ceph-mon01 ~]# ceph orch ps | grep osd.1
osd.1                           ceph-mon03               error             9s ago  85m        -    4096M  &lt;unknown&gt;          &lt;unknown&gt;     &lt;unknown&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Still not working??, lets check the log for the OSD</p>
</div>
<div class="listingblock">
<div class="content">
<pre># journalctl -f | grep osd.1
Mar 02 10:04:09 ceph-mon03 systemd[1]: Started Ceph osd.1 for cd172aa4-b8fe-11ed-b71d-2cc260754989.
Mar 02 10:04:09 ceph-mon03 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-mon-ceph-mon03[10586]: debug 2023-03-02T15:04:09.951+0000 7f7d1ea65700  0 cephx server osd.1:  unexpected key: req.key=a3e8dfcbdf87a19a expected_key=1f716d63dfcfe808</pre>
</div>
</div>
<div class="paragraph">
<p>So we have <code>cephx server osd.1:  unexpected key: req.key=a3e8dfcbdf87a19a
expected_key=1f716d63dfcfe808</code> , there is a mismatch of keys with what I&#8217;m
using in the OSD and what is created in the mon database, let&#8217;s compare both:</p>
</div>
<div class="paragraph">
<p>Key from OSD.1 in ceph auth ls: <code>key = AQBaugBkdGa7CRAAG5CltpQSVYZ68aq81lYxyg==</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre># cat /var/lib/ceph/cd172aa4-b8fe-11ed-b71d-2cc260754989/osd.1/keyring
[osd.1]
key = AQCjpgBk/aBWBhAA4aKa3tlejoGLmUHI1SiVtw==</pre>
</div>
</div>
<div class="paragraph">
<p>Ok&#8230;&#8203;, the keys clearly don&#8217;t match, I have to re-config my OSD daemon it uses
the new key in the cephx auth</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch daemon reconfig osd.1
Scheduled to reconfig osd.1 on host 'ceph-mon03'

# cat /var/lib/ceph/cd172aa4-b8fe-11ed-b71d-2cc260754989/osd.1/keyring
[osd.1]
	key = AQBaugBkdGa7CRAAG5CltpQSVYZ68aq81lYxyg==</pre>
</div>
</div>
<div class="paragraph">
<p>Looking better!, one more restart</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'ceph-mon03'
# ceph orch ps | grep osd.1
osd.1                           ceph-mon03               error             9s ago  85m        -    4096M  &lt;unknown&gt;          &lt;unknown&gt;     &lt;unknown&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Noooo!, they key has changed again to the old key, add this point I decide to
just re-create the OSD..</p>
</div>
<div class="listingblock">
<div class="content">
<pre># cat /var/lib/ceph/cd172aa4-b8fe-11ed-b71d-2cc260754989/osd.1/keyring
[osd.1]
key = AQCjpgBk/aBWBhAA4aKa3tlejoGLmUHI1SiVtw==</pre>
</div>
</div>
<div class="paragraph">
<p>Remove the OSD with the force flag because we are in health_warn and OSD in error state</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch osd rm 1 --force --zap
Scheduled OSD(s) for removal
[root@ceph-mon01 ~]# ceph orch osd rm status
OSD  HOST        STATE                    PGS  REPLACE  FORCE  ZAP   DRAIN STARTED AT
1    ceph-mon03  done, waiting for purge    0  False    True   True</pre>
</div>
</div>
<div class="paragraph">
<p>We now run the purge command</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph osd purge 1 --yes-i-really-mean-it
purged osd.1
# ceph orch osd rm status
No OSD remove/replace operations reported
# ceph orch device zap ceph-mon03 /dev/vdb --force
zap successful for /dev/vdb on ceph-mon03
# ceph orch device ls --refresh
HOST        PATH      TYPE  DEVICE ID              SIZE  AVAILABLE  REFRESHED  REJECT REASONS
ceph-mon01  /dev/vdb  hdd   2c4f7f08-fb26-428d-9  10.7G             111s ago   Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-mon02  /dev/vdb  hdd   8164c2a5-fc30-4abd-8  10.7G             111s ago   Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-mon03  /dev/vdb  hdd   c9a07526-4b59-43d5-a  10.7G  Yes        111s ago</pre>
</div>
</div>
<div class="paragraph">
<p>We can wait for the cephadm cache to expire or restart the manager to makes
things quicker:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch daemon restart  mgr.ceph-mon01.anygbp
# ceph orch device ls
HOST        PATH      TYPE  DEVICE ID              SIZE  AVAILABLE  REFRESHED  REJECT REASONS
ceph-mon01  /dev/vdb  hdd   2c4f7f08-fb26-428d-9  10.7G             9m ago     Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-mon02  /dev/vdb  hdd   8164c2a5-fc30-4abd-8  10.7G             9m ago     Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-mon03  /dev/vdb  hdd   c9a07526-4b59-43d5-a  10.7G             9m ago     Insufficient space (&lt;10 extents) on vgs, LVM detected, locked</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph pg stat
225 pgs: 225 active+clean; 9.7 KiB data, 28 MiB used, 30 GiB / 30 GiB avail</pre>
</div>
</div>
<div class="paragraph">
<p>Great all PGs are in active+clean state!, one final hurdle:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph health detail
HEALTH_WARN 1 failed cephadm daemon(s)
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon rgw.objectgw.ceph-mon02.fewdjv on ceph-mon02 is in error state</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph orch daemon restart rgw.objectgw.ceph-mon02.fewdjv
Scheduled to restart rgw.objectgw.ceph-mon02.fewdjv on host 'ceph-mon02'
# ceph orch ps | grep rgw
rgw.objectgw.ceph-mon02.fewdjv  ceph-mon02  *:8080       error             3s ago   2h        -        -  &lt;unknown&gt;          &lt;unknown&gt;     &lt;unknown&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Arg!, that would have been to easy&#8230;&#8203; , let&#8217;s take a look at the logs.., I do a
follow while I restart the RGW</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ssh ceph-mon02
# journalctl -f | grep rgw | grep -C 10 ERROR
Mar 02 11:25:53 ceph-mon02 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-rgw-objectgw-ceph-mon02-fewdjv[33479]: debug 2023-03-02T16:25:53.085+0000 7f3c5b6085c0  1 radosgw_Main not setting numa affinity
Mar 02 11:25:53 ceph-mon02 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-rgw-objectgw-ceph-mon02-fewdjv[33479]: debug 2023-03-02T16:25:53.115+0000 7f3c5b6085c0 -1 rgw main: Cannot find zone id= (name=nozone)
Mar 02 11:25:53 ceph-mon02 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-rgw-objectgw-ceph-mon02-fewdjv[33479]: debug 2023-03-02T16:25:53.115+0000 7f3c5b6085c0  0 rgw main: ERROR: failed to start notify service ((22) Invalid argument
Mar 02 11:25:53 ceph-mon02 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-rgw-objectgw-ceph-mon02-fewdjv[33479]: debug 2023-03-02T16:25:53.115+0000 7f3c5b6085c0  0 rgw main: ERROR: failed to init services (ret=(22) Invalid argument)
Mar 02 11:25:53 ceph-mon02 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-rgw-objectgw-ceph-mon02-fewdjv[33479]: debug 2023-03-02T16:25:53.118+0000 7f3c5b6085c0 -1 Couldn't init storage provider (RADOS)
Mar 02 11:25:53 ceph-mon02 systemd[1]: ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989@rgw.objectgw.ceph-mon02.fewdjv.service: Main process exited, code=exited, status=5/NOTINSTALLED</pre>
</div>
</div>
<div class="paragraph">
<p>A clear Error pops out here:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Mar 02 11:25:53 ceph-mon02 ceph-cd172aa4-b8fe-11ed-b71d-2cc260754989-rgw-objectgw-ceph-mon02-fewdjv[33479]: debug 2023-03-02T16:25:53.115+0000 7f3c5b6085c0 -1 rgw main: Cannot find zone id= (name=nozone)</pre>
</div>
</div>
<div class="paragraph">
<p>So the RGW is triying to start using a zone name called nozone, but it seems
that the RGW nozone pools dont exist, let&#8217;s check:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph config dump | grep zone
    client.rgw                                   advanced  rgw_zone                               nozone                                                                                                            *
# ceph osd lspools | grep rgw
5 .rgw.root
6 default.rgw.log
7 default.rgw.control
8 default.rgw.meta</pre>
</div>
</div>
<div class="paragraph">
<p>So there is a mismatch, I will change the rgw_zone config parameter to nozone</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph config set client.rgw rgw_zone default
# ceph orch daemon restart rgw.objectgw.ceph-mon02.fewdjv
Scheduled to restart rgw.objectgw.ceph-mon02.fewdjv on host 'ceph-mon02'
# ceph orch ps | grep rgw
rgw.objectgw.ceph-mon02.fewdjv  ceph-mon02  *:8080       running (10s)     8s ago   2h    17.6M        -  16.2.10-138.el8cp  8400da5f0ec0  7106a6dfd654</pre>
</div>
</div>
<div class="paragraph">
<p>Working!!, let&#8217;s check the health:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># ceph health
HEALTH_OK</pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There is a way to change the keyring for the OSD, without removing the OSD like
we did in the exercise:</p>
</div>
<div class="paragraph">
<p>You need to modify the OSD unit run and betweein the ceph-volume activate pod
and the osd start pod, add the following container run, change the key for they
new OSD key you want to use, you can find it with -v at the very end of the
podman run command</p>
</div>
<div class="listingblock">
<div class="content">
<pre># vi /var/lib/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef/osd.0/unit.run
...
! /bin/podman rm -f ceph-4d197f56-b77c-11ed-80f2-2cc26078e4ef-osd.0-activate 2&gt; /dev/null
! /bin/podman rm -f ceph-4d197f56-b77c-11ed-80f2-2cc26078e4ef-osd-0-activate 2&gt; /dev/null
! /bin/podman rm -f --storage ceph-4d197f56-b77c-11ed-80f2-2cc26078e4ef-osd-0-activate 2&gt; /dev/null
! /bin/podman rm -f --storage ceph-4d197f56-b77c-11ed-80f2-2cc26078e4ef-osd.0-activate 2&gt; /dev/null
/bin/podman run --rm --ipc=host --stop-signal=SIGTERM --authfile=/etc/ceph/podman-auth.json --net=host --entrypoint /usr/bin/ceph-bluestore-tool --privileged --group-add=disk --init --name ceph-4d197f56-b77c-11ed-80f2-2cc26078e4ef-osd-0-activate -e CONTAINER_IMAGE=registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:8aed15890a6b27a02856e66bf13611a15e6dba71c781a0ae09b3ecc8616ab8fa -e NODE_NAME=ceph-node01 -e CEPH_USE_RANDOM_NONCE=1 -e CEPH_VOLUME_SKIP_RESTORECON=yes -e CEPH_VOLUME_DEBUG=1 -v /var/run/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef:/var/run/ceph:z -v /var/log/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef:/var/log/ceph:z -v /var/lib/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef/crash:/var/lib/ceph/crash:z -v /var/lib/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef/osd.0:/var/lib/ceph/osd/ceph-0:z -v /var/lib/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef/osd.0/config:/etc/ceph/ceph.conf:z -v /dev:/dev -v /run/udev:/run/udev -v /sys:/sys -v /run/lvm:/run/lvm -v /run/lock/lvm:/run/lock/lvm -v /:/rootfs -v /etc/hosts:/etc/hosts:ro registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:8aed15890a6b27a02856e66bf13611a15e6dba71c781a0ae09b3ecc8616ab8fa set-label-key --path /var/lib/ceph/osd/ceph-0 --dev /dev/mapper/ceph--9edcda9a--f7a8--4496--a5ca--d19c4e8a4bdf-osd--block--3c190bab--0c06--4950--9942--d41dc8f93afb  -k osd_key -v 'AQCWHf5jBupNKRAAALhkcGeBglz1SCT3/zHNJw=='
...</pre>
</div>
</div>
<div class="paragraph">
<p>After you restart( or use bash
/var/lib/ceph/4d197f56-b77c-11ed-80f2-2cc26078e4ef/osd.0/unit.run) once with
add  this line the new key will be in place and the OSD will work fine, after
the rist reboot you need to remove the extra podman run we added to the
unit.run file,</p>
</div>
<div class="paragraph">
<p>The explanation being:</p>
</div>
<div class="paragraph">
<p>the command that is creating the new keyring and overwriting the key deployed by cephadm is <code>/usr/bin/ceph-bluestore-tool   prime-osd-dir</code> that is part of <code>ceph-volume activate</code> command that runs when the OSD container is started, it reads the information from a label in the main bluestore block device that contains OSD metadata, one of the keys it contains is the OSD keyring</p>
</div>
<div class="listingblock">
<div class="content">
<pre># /usr/bin/ceph-bluestore-tool show-label --path /var/lib/ceph/osd/ceph-0
inferring bluefs devices from bluestore path
{
    "/var/lib/ceph/osd/ceph-0/block": {
        "osd_uuid": "3c190bab-0c06-4950-9942-d41dc8f93afb",
        "size": 10733223936,
        "btime": "2023-02-28T15:28:23.907605+0000",
        "description": "main",
        "bfm_blocks": "2620416",
        "bfm_blocks_per_key": "128",
        "bfm_bytes_per_block": "4096",
        "bfm_size": "10733223936",
        "bluefs": "1",
        "ceph_fsid": "4d197f56-b77c-11ed-80f2-2cc26078e4ef",
        "kv_backend": "rocksdb",
        "magic": "ceph osd volume v026",
        "mkfs_done": "yes",
        "osd_key": "AQCWHf5jBupNKRAAALhkcGeBglz1SCT3/zHNJw=v",
        "osdspec_affinity": "all-available-devices",
        "ready": "ready",
        "require_osd_release": "16",
        "whoami": "0"
    }
}</pre>
</div>
</div>
<div class="paragraph">
<p>Changing the old key for the new key with the set-label-key makes a permanent
change to the bluestore lable for the device, and fixed the issue</p>
</div>
<div class="listingblock">
<div class="content">
<pre># /usr/bin/ceph-bluestore-tool set-label-key --path /var/lib/ceph/osd/ceph-0 --dev /dev/mapper/ceph--9edcda9a--f7a8--4496--a5ca--d19c4e8a4bdf-osd--block--3c190bab--0c06--4950--9942--d41dc8f93afb  -k osd_key -v 'AQC3OOZj5a5MARAAW4+XD/sESGy7AW</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
