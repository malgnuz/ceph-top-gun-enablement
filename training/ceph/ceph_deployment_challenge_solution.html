<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Ceph Deployment Challenge Solutions :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/ceph_deployment_challenge_solution.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_ui.html">Deploy Ceph from the UI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_dashboard_metrics.html">Ceph Dashboard Management &amp; Metrics</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_configuration.html">Ceph Configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pgs.html">Ceph Health and PGs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephx.html">Rados CephX Auth/AuthZ</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_version.html">What version of Ceph am I running?</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_export.html">RBD Import/Export</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_mirroring.html">RBD Mirroring</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge.html">Challenge RBD</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_archive.html">RGW Archive Zone</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_logging.html">Troubleshooting Logs Debug Mode</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_break_and_fix.html">Troubleshooting Break &amp; Fix Hands-on</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_fio.html">Benchmarking Ceph block and File</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Challenge Solutions</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge_solution.html">Ceph Deployment Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge_solution.html">Ceph RBD Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge_solution.html">Ceph CephFS Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge_solution.html">Ceph RGW Solution</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>Ceph Challenge Solutions</li>
    <li><a href="ceph_deployment_challenge_solution.html">Ceph Deployment Solution</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/ceph_deployment_challenge_solution.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Ceph Deployment Challenge Solutions</h1>
<div class="sect1">
<h2 id="_goals"><a class="anchor" href="#_goals"></a>1. Goals</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Review the the Ceph deployment with Cephadm</p>
</li>
<li>
<p>Setting specific CPU/memory targets for the ceph containers</p>
</li>
<li>
<p>Review the Pool and replica concepts.</p>
</li>
<li>
<p>Review scaling out the cluster with more nodes</p>
</li>
<li>
<p>Review PG autoscaler configuration</p>
</li>
<li>
<p>Review the configuration of segregated physical pools</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Please try and solve the challenge by yourself only come to this page as a last
resource after fighting with ceph for a while :D</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploy_a_ceph_cluster"><a class="anchor" href="#_deploy_a_ceph_cluster"></a>2. Deploy a Ceph Cluster</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise"><a class="anchor" href="#_exercise"></a>2.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>Deploy a Ceph cluster with 4 Nodes: <code>ceph-node01, ceph-node02, ceph-node03, proxy01</code></p>
<div class="ulist">
<ul>
<li>
<p>Bootstrap node will be ceph-node01</p>
</li>
<li>
<p>The cluster needs to have segregated networks, public and cluster network</p>
<div class="ulist">
<ul>
<li>
<p>Public Network will be: <code>192.168.56.0/24</code></p>
</li>
<li>
<p>Cluster Network will be: <code>172.16.7.0/24</code></p>
</li>
</ul>
</div>
</li>
<li>
<p>The cluster will be deployed using a specification file, to include the four nodes during the initial bootstrap</p>
</li>
<li>
<p>Only one OSD per Node is required</p>
</li>
<li>
<p>Limit the amount of memory an osd can consume to 2GB</p>
<div class="ulist">
<ul>
<li>
<p>The nodes will belong to a crushmap location of <code>RACK</code> called <code>RACK1</code></p>
</li>
</ul>
</div>
</li>
<li>
<p>Place the Observavility stack Services in node proxy01</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution"><a class="anchor" href="#_solution"></a>2.2. Solution</h3>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you are using your already running cluster don&#8217;t use the following spec
file, because the location only works during the initial bootstrap of the node
ti can be modifed afterwards.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create a Spec file as the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat cluster-spec.yaml
service_type: host
hostname: ceph-node01
addr: 192.168.56.61
location:
  root: default
  rack: RACK1
labels:
- osd
- mon
- mgr
---
service_type: host
hostname: ceph-node02
addr: 192.168.56.62
location:
  rack: RACK1
labels:
- osd
- mon
- rgw
---
service_type: host
hostname: ceph-node03
addr: 192.168.56.63
location:
  rack: RACK1
labels:
- osd
- mds
- mon

---
service_type: host
hostname: proxy01
addr: 192.168.56.24
location:
  rack: RACK1
labels:
- mgmt
---
service_type: mon
placement:
  label: "mon"
---
service_type: mgr
service_name: mgr
placement:
  label: "mgr"
---
service_type: osd
service_id: all-available-devices
service_name: osd.all-available-devices
spec:
  data_devices:
    all: true
    limit: 1
placement:
  label: "osd"
---
service_id: grafana.srv
service_type: grafana
placement:
  count: 1
  label: mgmt
---
service_id: alertmanager.srv
service_type: alertmanager
placement:
  count: 1
  label: mgmt
---
service_id: prometheus.srv
service_type: prometheus
placement:
  count: 1
  label: mgmt</code></pre>
</div>
</div>
<div class="paragraph">
<p>And then run the bootstrap command, adding the spec file and the cluster-network:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cephadm bootstrap --registry-json /root/registry.json --dashboard-password-noupdate  --ssh-user=root --ssh-private-key /root/.ssh/ceph --ssh-public-key /root/.ssh/ceph.pub --mon-ip 192.168.56.61 --apply-spec /root/cluster-spec.yaml  --cluster-network 172.16.7.0/24</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can change the cluster_network on a running cluster</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph config set global cluster_network 172.16.7.0/24</code></pre>
</div>
</div>
<div class="paragraph">
<p>A restart of the daemons is needed. Ceph daemons bind dynamically, so you do not have to restart the entire cluster at once if you change the network configuration for a specific daemon.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Once the cluster is deployed, configure the OSD memory target to 2GB</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph config set osd osd_memory_target_autotune false
# ceph config set osd osd_memory_target 2147483648</code></pre>
</div>
</div>
<div class="paragraph">
<p>With the Spec file we used during bootstrap we achieved all of the other
requirements, only one osd per host:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">service_type: osd
service_id: all-available-devices
service_name: osd.all-available-devices
spec:
  data_devices:
    all: true
    limit: 1   &lt;---- Here
placement:
  label: "osd"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The observability stack is deployed in proxy01 with the use of labels, example
of one service:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">---
service_type: host
hostname: proxy01
addr: 192.168.56.24
location:
  rack: RACK1
labels:
- mgmt   &lt;---- Label set in proxy01
---
service_id: alertmanager.srv
service_type: alertmanager
placement:
  count: 1
  label: mgmt &lt;--- Same label specified in the services placement
---


# ceph orch ps | grep -E '(grafana|prom|alert)'
alertmanager.proxy01       proxy01      *:9093,9094  running (6m)     5m ago   8m    13.7M        -                    0496af347f36  c9322ad0b959
grafana.proxy01            proxy01      *:3000       running (6m)     5m ago   7m    43.2M        -  8.3.5             bf676a29bcc5  5568a4470d96
prometheus.proxy01         proxy01      *:9095       running (6m)     5m ago   6m    50.7M        -                    dd9d8964582c  e1bd0f538a4c</code></pre>
</div>
</div>
<div class="paragraph">
<p>Also the rack location of RACK1 is achieved during bootstrap:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">---
service_type: host
hostname: proxy01
addr: 192.168.56.24
location:           &lt;--- Using the location config option
  rack: RACK1
labels:
- mgmt
---

# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
-1         0.02939  root default
-3         0.02939      rack RACK1
-2         0.00980          host ceph-node01
 0    hdd  0.00980              osd.0             up   1.00000  1.00000
-4         0.00980          host ceph-node02
 1    hdd  0.00980              osd.1             up   1.00000  1.00000
-5         0.00980          host ceph-node03
 2    hdd  0.00980              osd.2             up   1.00000  1.00000</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Using <code>location</code> inside service_type: host is only used during bootstrap, if you
change the <code>location</code> to a different value and use <code>ceph orch apply -i</code> the
changed won&#8217;t get applied, you would need to use <code>ceph osd crush move</code>
commands. for example</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd crush add-bucket RACK1 rack
# ceph osd crush move RACK1 root=default
# ceph osd crush move ceph-node01 rack=RACK1
# ceph osd crush move ceph-node02 rack=RACK1
# ceph osd crush move ceph-node03 rack=RACK1</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_rbd_pools"><a class="anchor" href="#_create_rbd_pools"></a>3. Create RBD Pools</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise_2"><a class="anchor" href="#_exercise_2"></a>3.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>Once the Ceph cluster is deployed we have to create two pools</p>
<div class="ulist">
<ul>
<li>
<p>pool1 called <code>rbdreplica2</code> with replica 2</p>
</li>
<li>
<p>pool2 called <code>rbdreplica3</code> with replica 3</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution_2"><a class="anchor" href="#_solution_2"></a>3.2. Solution</h3>
<div class="paragraph">
<p>Create pools and specify the app type</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool create rbdreplica2 replicated replicated_rule
# ceph osd pool set rbdreplica2 size 2
# ceph osd pool set rbdreplica2 min_size 1
# ceph osd pool application enable rbdreplica2 rbd
# ceph osd pool ls detail  | grep rbdreplica2
pool 5 'rbdreplica2' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 54 flags hashpspool stripe_width 0 application rbd

# ceph osd pool create rbdreplica3 replicated replicated_rule
# ceph osd pool application enable rbdreplica3 rbd</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_scale_out_the_cluster"><a class="anchor" href="#_scale_out_the_cluster"></a>4. Scale out the Cluster</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise_3"><a class="anchor" href="#_exercise_3"></a>4.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>We have to add nodes <code>ceph-mon01,ceph-mon02,ceph-mon03,proxy02</code> to the cluster</p>
<div class="ulist">
<ul>
<li>
<p>The new nodes will belong to a crushmap location of <code>RACK</code> called <code>RACK2</code></p>
</li>
<li>
<p>Nodes <code>ceph-mon01,ceph-mon02,ceph-mon03</code> will configure one OSD per node</p>
</li>
<li>
<p>Move the MGR services so we have one MGR on each RACK: RACK1 and RACK2.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution_3"><a class="anchor" href="#_solution_3"></a>4.2. Solution</h3>
<div class="paragraph">
<p>We need ssh access to nodes <code>ceph-mon01,ceph-mon02,ceph-mon03,proxy02</code> from the
bootstrap node <code>ceph-node01</code></p>
</div>
<div class="paragraph">
<p>The easiest way is to copy the public key being used in ceph-node01 to the
.ssh/authorized_keys file in hosts <code>ceph-mon01,ceph-mon02,ceph-mon03,proxy02</code></p>
</div>
<div class="paragraph">
<p>Take into account that the ssh key being used for passwordless ssh in the hosts
is ~/.ssh/ceph , as you can see from the config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[root@ceph-node01 ~]# cat .ssh/config
Host *
User root
IdentityFile ~/.ssh/ceph
StrictHostKeyChecking no</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once passwordless ssh is in place, we can add the nodes to our running cluster</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch host add ceph-mon01 192.168.56.64
# ceph orch host add ceph-mon02 192.168.56.65
# ceph orch host add ceph-mon03 192.168.56.66
# ceph orch host add proxy02 192.168.56.25

# ceph orch host ls
HOST         ADDR           LABELS              STATUS
ceph-mon01   192.168.56.64
ceph-mon02   192.168.56.65
ceph-mon03   192.168.56.66
ceph-node01  192.168.56.61  _admin osd mon mgr
ceph-node02  192.168.56.62  osd mon rgw
ceph-node03  192.168.56.63  osd mds mon
proxy01      192.168.56.24  mgmt
proxy02      192.168.56.25</code></pre>
</div>
</div>
<div class="paragraph">
<p>We need add the osd labels to the new hosts so the OSDs get configured on those nodes</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch host label add ceph-mon01 osd
# ceph orch host label add ceph-mon02 osd
# ceph orch host label add ceph-mon03 osd</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>ceph-mon0X nodes need their drives zapped/deleted before they can be use you
can use script /root/zap-disks.sh available in ceph-mon01, to zap all
ceph-mon0X disks</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After a while the ceph-mon OSDs should show up in the device list, and in turn
be consumed as OSDs by the cluster</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch device ls --refresh
ceph orch device ls
HOST         PATH      TYPE  DEVICE ID              SIZE  AVAILABLE  REFRESHED  REJECT REASONS
ceph-mon01   /dev/vdb  hdd   19ec5c29-2ac6-4851-8  10.7G  Yes        6s ago
ceph-mon02   /dev/vdb  hdd   5044b50b-6a2b-4103-9  10.7G  Yes        6s ago
ceph-mon03   /dev/vdb  hdd   9453fae9-2f4b-4802-9  10.7G  Yes        6s ago</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once OSDs are created:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[root@ceph-node01 ~]# ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
 -1         0.05878  root default
 -3         0.02939      rack RACK1
 -2         0.00980          host ceph-node01
  0    hdd  0.00980              osd.0             up   1.00000  1.00000
 -4         0.00980          host ceph-node02
  1    hdd  0.00980              osd.1             up   1.00000  1.00000
 -5         0.00980          host ceph-node03
  2    hdd  0.00980              osd.2             up   1.00000  1.00000
 -6               0          host proxy01
-13         0.00980      host ceph-mon01
  5    hdd  0.00980          osd.5                 up   1.00000  1.00000
-15         0.00980      host ceph-mon02
  3    hdd  0.00980          osd.3                 up   1.00000  1.00000
-17         0.00980      host ceph-mon03
  4    hdd  0.00980          osd.4                 up   1.00000  1.00000</code></pre>
</div>
</div>
<div class="paragraph">
<p>We are still missing the RACK2 crush label, let&#8217;s configure it.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd crush add-bucket RACK2 rack
# ceph osd crush move RACK2 root=default
# ceph osd crush move ceph-mon01 rack=RACK2
# ceph osd crush move ceph-mon02 rack=RACK2
# ceph osd crush move ceph-mon03 rack=RACK2
# ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
 -1         0.05878  root default
 -3         0.02939      rack RACK1
 -2         0.00980          host ceph-node01
  0    hdd  0.00980              osd.0             up   1.00000  1.00000
 -4         0.00980          host ceph-node02
  1    hdd  0.00980              osd.1             up   1.00000  1.00000
 -5         0.00980          host ceph-node03
  2    hdd  0.00980              osd.2             up   1.00000  1.00000
 -6               0          host proxy01
-19         0.02939      rack RACK2
-13         0.00980          host ceph-mon01
  5    hdd  0.00980              osd.5             up   1.00000  1.00000
-15         0.00980          host ceph-mon02
  3    hdd  0.00980              osd.3             up   1.00000  1.00000
-17         0.00980          host ceph-mon03
  4    hdd  0.00980              osd.4             up   1.00000  1.00000</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_ec_cephfs_pool"><a class="anchor" href="#_create_ec_cephfs_pool"></a>5. Create EC cephfs Pool</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise_4"><a class="anchor" href="#_exercise_4"></a>5.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>Create a new pool for cephfs data called <code>cephfsec</code> with EC replication profile 4+2, set the failure domain to host</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution_4"><a class="anchor" href="#_solution_4"></a>5.2. Solution</h3>
<div class="paragraph">
<p>First we need to create a new EC 4+2 profile:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd erasure-code-profile set profile42 k=4 m=2</code></pre>
</div>
</div>
<div class="paragraph">
<p>We cam see that by default the failure domain for the profile is host:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd erasure-code-profile get profile42
crush-device-class=
crush-failure-domain=host     &lt;--------- Host
crush-root=default
jerasure-per-chunk-alignment=false
k=4
m=2
plugin=jerasure
technique=reed_sol_van
w=8</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create the pool using the new profile we created:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool create cephfsec 16 16 erasure profile42
# ceph osd pool application enable cephfsec cephfs
# ceph osd pool ls detail | grep cephfsec
pool 6 'cephfsec' erasure profile profile42 size 6 min_size 5 crush_rule 1 object_hash rjenkins pg_num 16 pgp_num 16 autoscale_mode on last_change 78 flags hashpspool stripe_width 16384</code></pre>
</div>
</div>
<div class="paragraph">
<p>Do a quick test and upload an object at the rados layer</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># rados -p cephfsec put mytestvi /usr/bin/vi
# rados -p cephfsec ls
mytestvi</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_create_a_cephfs_pool"><a class="anchor" href="#_create_a_cephfs_pool"></a>6. Create a cephfs Pool</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise_5"><a class="anchor" href="#_exercise_5"></a>6.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>Create a new pool for cephfs called <code>cephfsreplica2</code></p>
<div class="ulist">
<ul>
<li>
<p>with PG count 16</p>
</li>
<li>
<p>replica 2, and the failure domain set to Rack</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution_5"><a class="anchor" href="#_solution_5"></a>6.2. Solution</h3>
<div class="paragraph">
<p>If we check our current crush rules, we only have 1 created, and the failure domain it uses it&#8217;s set to host</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">#  ceph osd crush rule dump replicated_rule
{
    "rule_id": 0,
    "rule_name": "replicated_rule",
    "ruleset": 0,
    "type": 1,
    "min_size": 1,
    "max_size": 10,
    "steps": [
        {
            "op": "take",
            "item": -1,
            "item_name": "default"
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "host"     &lt;--------------- Failure domain set at the crush host level
        },
        {
            "op": "emit"
        }
    ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we need to create a new crush rule</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd crush rule create-replicated rackrule default rack
# ceph osd crush rule dump  rackrule
{
    "rule_id": 2,
    "rule_name": "rackrule",
    "ruleset": 2,
    "type": 1,
    "min_size": 1,
    "max_size": 10,
    "steps": [
        {
            "op": "take",
            "item": -12,
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "rack"  &lt;----- Rack Failure domain
        },
        {
            "op": "emit"
        }
    ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>And use it when creating our new replica 2 pool</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool create cephfsreplica2 16 16 replicated rackrule
pool 'cephfsreplica2' created
# ceph osd pool set cephfsreplica2 size 2
set pool 7 size to 2
# ceph osd pool set cephfsreplica2 min_size 2
set pool 7 min_size to 2
# ceph osd pool application enable cephfsreplica2 cephfs
enabled application 'cephfs' on pool 'cephfsreplica2'</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_autoscale_pools"><a class="anchor" href="#_autoscale_pools"></a>7. Autoscale Pools</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise_6"><a class="anchor" href="#_exercise_6"></a>7.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>Enable autoscale mode on all pools, and configure the target size ratio with the following ratios:</p>
<div class="ulist">
<ul>
<li>
<p>rbdreplica2. 10%</p>
</li>
<li>
<p>rbdreplica3. 20%</p>
</li>
<li>
<p>cephfsreplica2 20%</p>
</li>
<li>
<p>cephfsec 50%</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution_6"><a class="anchor" href="#_solution_6"></a>7.2. Solution</h3>
<div class="paragraph">
<p>Auto scale is on by default</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool get cephfsreplica2 pg_autoscale_mode
pg_autoscale_mode: on
# ceph osd pool get noautoscale
noautoscale is off</code></pre>
</div>
</div>
<div class="paragraph">
<p>But we need to set the ratios for each pool</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool autoscale-status
POOL                     SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE  BULK
device_health_metrics      0                 3.0        61416M  0.0000                                  1.0       1              on         False
rbdreplica3                0                 3.0        61416M  0.0000                                  1.0      32              on         False
rbdreplica2                0                 2.0        61416M  0.0000                                  1.0      32              on         False
cephfsec                1168k                1.5        61416M  0.0000                                  1.0      16              on         False
cephfsreplica2             0                 2.0        61416M  0.0000                                  1.0      16              on         False

# ceph osd pool set rbdreplica3 target_size_ratio 0.3
# ceph osd pool set rbdreplica3 target_size_ratio 0.2
# ceph osd pool set rbdreplica2 target_size_ratio 0.1
# ceph osd pool set cephfsreplica2 target_size_ratio 0.2
# ceph osd pool set cephfsec target_size_ratio 0.5

# ceph osd pool autoscale-status
POOL                     SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE  BULK
device_health_metrics      0                 3.0        61416M  0.0000                                  1.0       1              on         False
rbdreplica3                0                 3.0        61416M  0.2000        0.2000           0.2000   1.0     128          32  on         False
rbdreplica2                0                 2.0        61416M  0.1000        0.1000           0.1000   1.0      32              on         False
cephfsec                1168k                1.5        61416M  0.5000        0.5000           0.5000   1.0      16              on         False
cephfsreplica2             0                 2.0        61416M  0.2000        0.2000           0.2000   1.0      16              on         False</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_add_osds_to_the_cluster"><a class="anchor" href="#_add_osds_to_the_cluster"></a>8. Add OSDs to the cluster</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_exercise_7"><a class="anchor" href="#_exercise_7"></a>8.1. Exercise</h3>
<div class="ulist">
<ul>
<li>
<p>We need to add a dedicated data pool for RGW, that has to be physically segregated from the rest of the cluster data</p>
<div class="ulist">
<ul>
<li>
<p>We need to add 2 new drives from nodes <code>ceph-node01,ceph-node02 and ceph-node03</code></p>
</li>
<li>
<p>We will configure 2 OSD&#8217;s per drive, With Encryption enabled at the OSD level.</p>
</li>
<li>
<p>We need to use a specific device class for the new osds that we want to
segregate (take a look at <code>ceph osd crush class</code>, cephadm in 5.3 doesn&#8217;t support specifying a class during bootstrap of the osd service)</p>
</li>
<li>
<p>Create a pool called <code>rgw-security</code> with <code>replica 3</code>, it will use a rule that uses the new device classes we created</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_solution_7"><a class="anchor" href="#_solution_7"></a>8.2. Solution</h3>
<div class="paragraph">
<p>We need to create a new OSD drivegroup service that only has 3 nodes listed
<code>ceph-node01,ceph-node02 and ceph-node03</code>, we are going to use labels for
placement, I will create a new label called osd-secure</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch host label add ceph-node01 osd-secure
# ceph orch host label add ceph-node02 osd-secure
# ceph orch host label add ceph-node03 osd-secure</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we create a OSD service spec with Encryption and 2 OSD&#8217;s per drive added to
the config</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat osds-crypt.yaml
---
service_type: osd
service_id: osds-encrypt
service_name: osds-encrypt.cephnodes
placement:
  label: osd-secure
spec:
  data_devices:
    all: true
    limit: 2
  encrypted: true
  osds_per_device: 2</code></pre>
</div>
</div>
<div class="paragraph">
<p>And finally apply the config</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch apply -i osds-crypt.yaml
Scheduled osd.osds-encrypt update...</code></pre>
</div>
</div>
<div class="paragraph">
<p>After a while the OSDs get created and we can see 2 new devices being used in
the device list</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch device ls ceph-node01
HOST         PATH      TYPE  DEVICE ID              SIZE  AVAILABLE  REFRESHED  REJECT REASONS
ceph-node01  /dev/vdb  hdd   78cc2058-d24c-473a-b  10.7G             39s ago    Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-node01  /dev/vdc  hdd   1c10de98-ff4d-4fbe-9  10.7G             39s ago    Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-node01  /dev/vdd  hdd   aa586e5a-52a4-4557-8  10.7G             39s ago    Insufficient space (&lt;10 extents) on vgs, LVM detected, locked
ceph-node01  /dev/vde  hdd   f5fd6339-69b3-4c05-8  10.7G  Yes        39s ago</code></pre>
</div>
</div>
<div class="paragraph">
<p>And also the OSDs are now part of the crush tree, because we are using
<code>osds_per_device: 2</code> for each device we have 2 OSDs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
 -1         0.11755  root default
 -3         0.08817      rack RACK1
 -2         0.02939          host ceph-node01
  0    hdd  0.00980              osd.0             up   1.00000  1.00000
 13    hdd  0.00490              osd.13            up   1.00000  1.00000
 15    hdd  0.00490              osd.15            up   1.00000  1.00000
 16    hdd  0.00490              osd.16            up   1.00000  1.00000
 17    hdd  0.00490              osd.17            up   1.00000  1.00000
 -4         0.02939          host ceph-node02
  1    hdd  0.00980              osd.1             up   1.00000  1.00000
  6    hdd  0.00490              osd.6             up   1.00000  1.00000
  7    hdd  0.00490              osd.7             up   1.00000  1.00000
  9    hdd  0.00490              osd.9             up   1.00000  1.00000
 12    hdd  0.00490              osd.12            up   1.00000  1.00000
 -5         0.02939          host ceph-node03
  2    hdd  0.00980              osd.2             up   1.00000  1.00000
  8    hdd  0.00490              osd.8             up   1.00000  1.00000
 10    hdd  0.00490              osd.10            up   1.00000  1.00000
 11    hdd  0.00490              osd.11            up   1.00000  1.00000
 14    hdd  0.00490              osd.14            up   1.00000  1.00000</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can also check the OSDs are encrypted with luks at the node level</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># lsblk | grep -A 5 vdd
vdd                                                                                                   252:48   0   10G  0 disk
├─ceph--9d343a25--72a9--42d2--ac40--b73924eda2ee-osd--block--6828dece--055f--4673--8b24--baaf9f3fd2e2 253:5    0    5G  0 lvm
│ └─NJaa3E-Rfr1-T8d2-C4Cv-gBuL-Q7gX-sr4hXY                                                            253:6    0    5G  0 crypt
└─ceph--9d343a25--72a9--42d2--ac40--b73924eda2ee-osd--block--1dc3a1b0--b9a3--4b90--a40d--1cc2d0299999 253:7    0    5G  0 lvm
  └─iqYzvh-2W3w-gHAV-yykP-C79U-v7Zz-2OgeNd                                                            253:8    0    5G  0 crypt</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or at the OSD level:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"></code></pre>
</div>
</div>
<div class="paragraph">
<p>We create the new device class called secret</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd crush class ls
[
    "hdd"
]
# ceph osd crush class create secret
# ceph osd crush class ls
[
    "hdd",
    "secret"
]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we assing the new device class to our encrypted OSDs, to get a list of the
we can use <code>ceph device ls</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph device ls | grep ceph-node | grep -v vdb
1c10de98-ff4d-4fbe-9  ceph-node01:vdc  osd.13 osd.15
3e305801-0444-4489-b  ceph-node02:vdc  osd.7 osd.9
9b722a39-8135-42f1-a  ceph-node02:vdd  osd.12 osd.6
aa586e5a-52a4-4557-8  ceph-node01:vdd  osd.16 osd.17
b583f9cc-cbc7-48a7-a  ceph-node03:vdc  osd.10 osd.8
f5e09ef3-f755-4a85-a  ceph-node03:vdd  osd.11 osd.14</code></pre>
</div>
</div>
<div class="paragraph">
<p>We now need to delete the hdd device class and add our secrete device class for
the list of OSDs</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># for i in {6..17} ; do ceph osd crush rm-device-class $i ; ceph osd crush set-device-class secret $i ; done
done removing class of osd(s): 6
set osd(s) 6 to class 'secret'
done removing class of osd(s): 7
set osd(s) 7 to class 'secret'
done removing class of osd(s): 8
set osd(s) 8 to class 'secret'
done removing class of osd(s): 9
set osd(s) 9 to class 'secret'
done removing class of osd(s): 10
set osd(s) 10 to class 'secret'
done removing class of osd(s): 11
set osd(s) 11 to class 'secret'
done removing class of osd(s): 12
set osd(s) 12 to class 'secret'
done removing class of osd(s): 13
set osd(s) 13 to class 'secret'
done removing class of osd(s): 14
set osd(s) 14 to class 'secret'
done removing class of osd(s): 15
set osd(s) 15 to class 'secret'
done removing class of osd(s): 16
set osd(s) 16 to class 'secret'
done removing class of osd(s): 17
set osd(s) 17 to class 'secret'

# ceph osd tree
ID   CLASS   WEIGHT   TYPE NAME                 STATUS  REWEIGHT  PRI-AFF
 -1          0.11755  root default
 -3          0.08817      rack RACK1
 -2          0.02939          host ceph-node01
  0     hdd  0.00980              osd.0             up   1.00000  1.00000
 13  secret  0.00490              osd.13            up   1.00000  1.00000
 15  secret  0.00490              osd.15            up   1.00000  1.00000
 16  secret  0.00490              osd.16            up   1.00000  1.00000
 17  secret  0.00490              osd.17            up   1.00000  1.00000
 -4          0.02939          host ceph-node02
  1     hdd  0.00980              osd.1             up   1.00000  1.00000
  6  secret  0.00490              osd.6             up   1.00000  1.00000
  7  secret  0.00490              osd.7             up   1.00000  1.00000
  9  secret  0.00490              osd.9             up   1.00000  1.00000
 12  secret  0.00490              osd.12            up   1.00000  1.00000
 -5          0.02939          host ceph-node03
  2     hdd  0.00980              osd.2             up   1.00000  1.00000
  8  secret  0.00490              osd.8             up   1.00000  1.00000
 10  secret  0.00490              osd.10            up   1.00000  1.00000
 11  secret  0.00490              osd.11            up   1.00000  1.00000
 14  secret  0.00490              osd.14            up   1.00000  1.00000</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we need to create a new crush rule that uses the <code>secret</code> device class, we
specify the device class at the end of the <code>crush rule create-replicated</code>
command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd crush rule create-replicated secretrule default host secret</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now that we have the rule in place we can create the pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool create rgw-security 16 16 replicated secretrule
# ceph osd pool application enable rgw-security rgw</code></pre>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
