<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Ceph CephFS Deep Dive and Advanced Topics :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/cephfs_advanced.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_ui.html">Deploy Ceph from the UI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_dashboard_metrics.html">Ceph Dashboard Management &amp; Metrics</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_configuration.html">Ceph Configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pgs.html">Ceph Health and PGs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephx.html">Rados CephX Auth/AuthZ</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_version.html">What version of Ceph am I running?</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_export.html">RBD Import/Export</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_mirroring.html">RBD Mirroring</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge.html">Challenge RBD</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_logging.html">Troubleshooting Logs Debug Mode</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_break_and_fix.html">Troubleshooting Break &amp; Fix Hands-on</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_fio.html">Benchmarking Ceph block and File</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Challenge Solutions</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge_solution.html">Ceph Deployment Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge_solution.html">Ceph RBD Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge_solution.html">Ceph CephFS Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge_solution.html">Ceph RGW Solution</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>CephFS Shared FileSystem</li>
    <li><a href="cephfs_advanced.html">CephFS Deep Dive</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/cephfs_advanced.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Ceph CephFS Deep Dive and Advanced Topics</h1>
<div class="sect1">
<h2 id="_ceph_file_system_io_path"><a class="anchor" href="#_ceph_file_system_io_path"></a>1. Ceph File System IO-PATH</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All file data in CephFS is stored as RADOS objects. CephFS clients can directly access RADOS to operate on file data. MDS only handles metadata operations.</p>
</div>
<div class="paragraph">
<p>To read/write a CephFS file, the client needs to have ‘file read/write’ capabilities for the corresponding inode. If the client does not have the required capabilities, it sends a ‘cap message’ to MDS, telling MDS what it wants. MDS will issue capabilities to clients when it is possible. Once the client has ‘file read/write’ capabilities, it can directly access RADOS to read/write file data. File data are stored as RADOS objects in the form of &lt;inode number&gt;.&lt;object index&gt;. See the Upstream doc about ‘Data Striping’ in the Architecture section for more information. If the file is only opened by one client, MDS also issues ‘file cache/buffer’ capabilities to the only client. The ‘file cache’ capability means that file read can be satisfied by the client cache. The ‘file buffer’ capability means that file write can be buffered in the client cache.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/cephfs_io_path.png" alt="cephfs layout" width="840" height="680">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cephfs_file_layouts"><a class="anchor" href="#_cephfs_file_layouts"></a>2. Cephfs File Layouts</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The layout of a file controls how its contents are mapped to Ceph RADOS objects. You can read and write a file’s layout using virtual extended attributes or xattrs.</p>
</div>
<div class="paragraph">
<p>The name of the layout xattrs depends on whether a file is a regular file or a directory. Regular files’ layout xattrs are called ceph.file.layout, whereas directories’ layout xattrs are called ceph.dir.layout. Where subsequent examples refer to ceph.file.layout, substitute dir as appropriate when dealing with directories.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_layout_fields"><a class="anchor" href="#_layout_fields"></a>3. Layout fields</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>pool</strong></p>
</div>
<div class="paragraph">
<p>String, giving ID or name. The string can only have characters in the set [a-zA-Z0-9\_-.]. Which RADOS pool a file&#8217;s data objects will be stored in.</p>
</div>
<div class="paragraph">
<p><strong>pool_id</strong></p>
</div>
<div class="paragraph">
<p>String of digits. This is the system-assigned pool id for the RADOS pool whenever it is created.</p>
</div>
<div class="paragraph">
<p><strong>pool_name</strong></p>
</div>
<div class="paragraph">
<p>String, given name. This is the user-defined name for the RADOS pool whenever a user creates it.</p>
</div>
<div class="paragraph">
<p><strong>pool_namespace</strong></p>
</div>
<div class="paragraph">
<p>String with only characters in the set [a-zA-Z0-9\_-.].  Within the data pool, which RADOS namespace the objects will
    be written to.  Empty by default (i.e. default namespace).</p>
</div>
<div class="paragraph">
<p><strong>stripe_unit</strong></p>
</div>
<div class="paragraph">
<p>Integer in bytes.  The size (in bytes) of a block of data used in the RAID 0 distribution of a file. All stripe units for a file have an equal size. The last stripe unit is typically incomplete–i.e. it represents the data at the end of the file as well as unused “space” beyond it up to the end of the fixed stripe unit size.</p>
</div>
<div class="paragraph">
<p><strong>stripe_count</strong></p>
</div>
<div class="paragraph">
<p>Integer.  The number of consecutive stripe units that constitute a RAID 0 “stripe” of file data.</p>
</div>
<div class="paragraph">
<p><strong>object_size</strong></p>
</div>
<div class="paragraph">
<p>Integer in bytes.  File data is chunked into RADOS objects of this size.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>RADOS enforces a configurable limit on object sizes: if you increase CephFS object sizes beyond that limit then writes may not succeed. The OSD setting is osd_max_object_size, which is 128MB by default. Very large RADOS objects may prevent the smooth operation of the cluster, so increasing the object size limit past the default is not recommended.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cephfs_layouts_hands_on"><a class="anchor" href="#_cephfs_layouts_hands_on"></a>4. Cephfs Layouts Hands-On</h2>
<div class="sectionbody">
<div class="paragraph">
<p>From the <code>workstation</code> server, we first need to install the <code>attr</code> RPM. Hence, we have access to the <code>setfattr</code> and
<code>getfattr</code> commands</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># dnf install -y attr</code></pre>
</div>
</div>
<div class="paragraph">
<p>We mount the cephfs filesystem at the root level with user id 0. This user has
Read/Write client caps at the / level</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=0,secret="AQA+KrxjWUovChAACWGj0YUbUEZHSKmNtYxriw=="</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can check the layout attributes of the root filesystem</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># getfattr -n ceph.dir.layout /mnt
# file: mnt
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs.fs_name.data"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Because user <code>0</code> was created with the following client capabilities.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph auth get client.0
[client.0]
	key = AQA+KrxjWUovChAACWGj0YUbUEZHSKmNtYxriw==
	caps mds = "allow rw fsname=fs_name"
	caps mon = "allow r fsname=fs_name"
	caps osd = "allow rw tag cephfs data=fs_name"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we try to modify the attribute for the / cephfs filesystem we will get
permission denied</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># setfattr -n ceph.dir.layout.stripe_count -v 2 /mnt
setfattr: /mnt: Permission denied</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Users need the 'P' client capability to be able to modify FS attributes,
and quotas
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Let&#8217;s create a new cephx user that has the <code>P</code> capability flag to modify
attributes in the <code>/dir4</code> folder</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># mkdir /mnt/dir4
#  ceph fs authorize fs_name client.4 / rw /dir4 rwp
[client.4]
	key = AQBmK71j0FcKERAAJqwhXOHoucR+iY0nzGV9BQ==</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now let&#8217;s re-mount the FS using a user with id <code>4</code> that we just created</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
[root@workstation-lb1719 ~]# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=4,secret="AQBmK71j0FcKERAAJqwhXOHoucR+iY0nzGV9BQ=="</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s modify the attributes of a file in dir4</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># touch /mnt/dir4/file1
# setfattr -n ceph.file.layout.stripe_count -v 2 /mnt/dir4/file1
# getfattr -n ceph.file.layout /mnt/dir4/file1
# file: mnt/dir4/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 pool=cephfs.fs_name.data"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Files inherit the layout of their parent directory at creation time. However, subsequent changes to the parent directory’s layout do not affect children.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Files created as descendants of the directory also inherit the layout if the intermediate directories do not have layouts set
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_creating_files_in_different_rados_namespaces"><a class="anchor" href="#_creating_files_in_different_rados_namespaces"></a>4.1. Creating files in different Rados Namespaces</h3>
<div class="paragraph">
<p>Using the dir layout, we can select a rados namespace for a directory</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># mkdir /mnt/dir4/dir-namespace
# setfattr -n ceph.dir.layout.pool_namespace -v client4 /mnt/dir4/dir-namespace
# getfattr -n ceph.dir.layout /mnt/dir4/dir-namespace
# file: mnt/dir4/dir-namespace
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs.fs_name.data pool_namespace=client4"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s mount at the new directory level and create a file</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/dir4/dir-namespace /mnt -o name=4,secret="AQBmK71j0FcKERAAJqwhXOHoucR+iY0nzGV9BQ=="
# echo "Here we go" &gt; /mnt/file-in-namespace
# getfattr -n ceph.file.layout /mnt/file-in-namespace
# file: mnt/file-in-namespace
ceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 pool=cephfs.fs_name.data pool_namespace=client4"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we check at the rados level, we can see that the new file is created in the
client4 rados namespace. There is no data on the default namespace for the data
cephfs pool</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># rados ls -p cephfs.fs_name.data
#
# rados ls -p cephfs.fs_name.data -N client4
10000000003.00000000
# echo "Here we go 2" &gt; /mnt/file-in-namespace2
# rados ls -p cephfs.fs_name.data --all
client4	10000000003.00000000
client4	10000000004.00000000</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>How to map a cephfs file to a rados object?</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># pwd
/mnt/dir4/dir-namespace
# ls -l
total 1
-rw-r--r--. 1 root root 11 Jan 10 05:50 file-in-namespace
-rw-r--r--. 1 root root 13 Jan 10 05:53 file-in-namespace2
# printf '%x\n' $(stat -c %i file-in-namespace)
10000000003
# rados ls -p cephfs.fs_name.data -N client4
10000000003.00000000
10000000004.00000000</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_adding_a_different_pool_to_a_path_in_a_filesystem"><a class="anchor" href="#_adding_a_different_pool_to_a_path_in_a_filesystem"></a>4.2. Adding a different pool to a path in a filesystem</h3>
<div class="paragraph">
<p>We are going to create a new EC cephfs data pool and use it in our cephfs
<code>fs_name</code> filesystem</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph osd pool create cephfs-data-ec 16 erasure
pool 'cephfs-data-ec' created
# ceph osd pool set cephfs-data-ec allow_ec_overwrites true
set pool 5 allow_ec_overwrites to true
# ceph osd pool application enable cephfs-data-ec cephfs
enabled application 'cephfs' on pool 'cephfs-data-ec'</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To get more information on why EC pools for Cephfs need to enable
the config parameter cephfs-data-ec allow_ec_overwrites check out this <a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code/#erasure-coding-with-overwrites">Doc</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Let&#8217;s add the new pool to our filesystem</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs add_data_pool fs_name cephfs-data-ec
added data pool 5 to fsmap</code></pre>
</div>
</div>
<div class="paragraph">
<p>We create a new user that is going to have permissions to access the new
filesystem directory that will map to the new EC pool we created</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph auth get-or-create client.ec mon 'allow r' mds 'allow r, allow rw path=/clientec' osd 'allow rw pool=cephfs-data-ec'
[client.ec]
	key = AQDpSr1jQm/yGBAAst+6elZFUY3BIZqeySZZ+w==</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s mount the Cephfs root filesystem with the admin key, so we can modify the
layout properties for the fs directory <code>/clientec</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
# cat /etc/ceph/ceph.client.admin.keyring | grep key
	key = AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg==
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s create the directory <code>/clientec</code> and change the attributes, so we use a
rados namespace called <code>clientec</code> inside the new pool <code>cephfs-data-ec</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># mkdir /mnt/clientec
# setfattr -n ceph.dir.layout.pool_namespace -v clientec /mnt/clientec
# setfattr -n ceph.dir.layout.pool -v cephfs-data-ec /mnt/clientec</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s mount the directory with the client.ec we created before, and by
creating a file, we can check that the file went to the rados namespace called <code>clientec</code> inside the new pool <code>cephfs-data-ec</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/clientec /mnt -o name=ec,secret="AQDpSr1jQm/yGBAAst+6elZFUY3BIZqeySZZ+w=="
# echo "Here we go 2" &gt; /mnt/file-in-ecpool
# rados ls -p cephfs-data-ec --all
clientec	10000000006.00000000</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cephfs_quotas"><a class="anchor" href="#_cephfs_quotas"></a>5. CephFS Quotas</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_quota_restrictions"><a class="anchor" href="#_quota_restrictions"></a>5.1. <strong>Quota Restrictions</strong></h3>
<div class="paragraph">
<p>Quotas are cooperative and non-adversarial. CephFS quotas rely on the client&#8217;s cooperation, who is mounting the file system to stop writers when a limit is reached. A modified or adversarial client cannot be prevented from writing as much data as it needs. Quotas should not be relied on to prevent filling the system in environments where the clients are fully untrusted.</p>
</div>
<div class="paragraph">
<p>Quotas are imprecise. Processes written to the file system will be stopped shortly after the quota limit is reached. They will inevitably be allowed to write some data over the configured limit. How far over the quota they can go depends primarily on the amount of time, not the amount of data. Generally speaking, writers will be stopped within 10s of seconds of crossing the configured limit.</p>
</div>
<div class="paragraph">
<p>Quotas are implemented in the kernel client 4.17 and higher. Quotas are supported by the userspace client (libcephfs, ceph-fuse). Linux kernel clients &gt;= 4.17 support CephFS quotas but only on mimic+ clusters. Kernel clients (even recent versions) will fail to handle quotas on older clusters, even if they may be able to set the quota&#8217;s extended attributes.</p>
</div>
<div class="paragraph">
<p>Quotas must be configured carefully when used with path-based mount restrictions. The client needs access to the directory inode on which quotas are configured to enforce them. If the client has restricted access to a specific path (e.g., /home/user) based on the MDS capability, and a quota is configured on an ancestor directory they do not have access to (e.g., /home), the client will not enforce it. When using path-based access restrictions, be sure to configure the quota on the directory the client is restricted to (e.g., /home/user) or something nested beneath it.</p>
</div>
</div>
<div class="sect2">
<h3 id="_example"><a class="anchor" href="#_example"></a>5.2. <strong>Example</strong></h3>
<div class="paragraph">
<p>As the admin user, let&#8217;s mount the Filesystem</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="</code></pre>
</div>
</div>
<div class="paragraph">
<p>And we set a quota of 10MB max in size and a max of 10 files on the root of
the filesystem</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># setfattr -n ceph.quota.max_bytes -v 10000000 /mnt
# setfattr -n ceph.quota.max_files -v 10 /mnt
# getfattr -n ceph.quota.max_bytes /mnt
ceph.quota.max_bytes="10000000"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we check with the <code>df</code> command we can see the available space is the one we have specified with the quota</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># df -h /mnt/clientec
Filesystem                     Size  Used Avail Use% Mounted on
192.168.56.61,192.168.56.62:/  8.0M     0  8.0M   0% /mnt</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we now try to exeed the file count quota or max space, we can see it takes some time to
sync and block the writes as it specifies in the second point of the quota
restrictions</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># for i in {1..1000};do touch /mnt/file-${i}.txt;done
touch: cannot touch '/mnt/file-503.txt': Disk quota exceeded
touch: cannot touch '/mnt/file-504.txt': Disk quota exceeded

# dd if=/dev/zero of=/mnt/test-quota bs=1M count=100
dd: error writing '/mnt/test-quota': Disk quota exceeded
14+0 records in
13+0 records out
13631488 bytes (14 MB, 13 MiB) copied, 0.0367228 s, 371 MB/s

# df -h /mnt/
Filesystem                     Size  Used Avail Use% Mounted on
192.168.56.61,192.168.56.62:/  8.0M  8.0M     0 100% /mnt</code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove the Quotas</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># setfattr -n ceph.quota.max_files -v 0 /mnt/
# setfattr -n ceph.quota.max_bytes -v 0 /mnt/</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mds_high_availability"><a class="anchor" href="#_mds_high_availability"></a>6. MDS High Availability</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If an MDS daemon stops communicating with the cluster’s monitors, the monitors will wait mds_beacon_grace seconds (default 15) before marking the daemon as laggy. The monitor will immediately replace the laggy daemon if a standby MDS is available.</p>
</div>
<div class="paragraph">
<p>Each file system may specify a minimum number of standby daemons in order to be considered healthy. This number includes daemons in the standby-replay state waiting for a rank to fail. Note that a standby-replay daemon will not be assigned to take over a failure for another rank or a failure in a different CephFS file system). The pool of standby daemons not in replay counts towards any file system count.</p>
</div>
<div class="paragraph">
<p>Each file system may set the desired number of standby daemons by setting the
<code>ceph fs set &lt;fs name&gt; standby_count_wanted &lt;count&gt;</code> command.</p>
</div>
<div class="paragraph">
<p>By default, it&#8217;s set to 1. If we, for example, increase it to two, we will get a
new MDS daemon deployed as standby</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs get fs_name | grep standby_count_wanted
standby_count_wanted	1
# ceph fs status
fs_name - 1 clients
=======
RANK  STATE            MDS               ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  fs_name.proxy01.bbnkcu  Reqs:    0 /s   520     21     15      1
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata  10.5M  9695M
cephfs.fs_name.data    data    39.0M  9695M
   cephfs-data-ec      data    12.0k  18.9G
       STANDBY MDS
fs_name.ceph-node01.uygvno  &lt;------ We have one MDS daemon as standby
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Because we configured the MDS service placement to a count of two. With
dedicated hosts <code>ceph-node01,proxy01</code>, ceph can&#8217;t add a new MDS daemon to
fulfil the requirement of two <code>standby_count_wanted</code> per Filesystem.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph -s
  cluster:
    id:     e6c62efc-9016-11ed-b206-2cc26078e4ef
    health: HEALTH_WARN
            insufficient standby MDS daemons available</code></pre>
</div>
</div>
<div class="paragraph">
<p>We are going to modify our MDS service configuration, to add one more host into
the placement, let&#8217;s use a spec file for a change.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch ls mds mds.fs_name --export | tee mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 2
  hosts:
  - ceph-node01
  - proxy01</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once we have exported the config to a file open with an editor and increment
the <code>count: 3</code> and the <code>hosts:</code> with ceph-node02</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 3
  hosts:
  - ceph-node01
  - ceph-node02
  - proxy01</code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply the config(we can use dry-run option if we want)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph orch apply -i mds.yaml
Scheduled mds.fs_name update...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Lets check if we now have 2 standby MDS demons</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph -s | grep mds
    mds: 1/1 daemons up, 2 standby
# ceph orch ps | grep mds
mds.fs_name.ceph-node01.uygvno  ceph-node01               running (10h)     8m ago   7h    37.3M        -  16.2.8-85.el8cp  b2c997ff1898  d6169aee0209
mds.fs_name.ceph-node02.kxxoad  ceph-node02               running (68s)    63s ago  68s    32.7M        -  16.2.8-85.el8cp  b2c997ff1898  6ba364178785
mds.fs_name.proxy01.bbnkcu      proxy01                   running (14h)     9m ago   6h    70.5M        -  16.2.8-85.el8cp  b2c997ff1898  df91160e7fa6
# ceph fs status
fs_name - 1 clients
=======
RANK  STATE            MDS               ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  fs_name.proxy01.bbnkcu  Reqs:    0 /s   520     21     15      1
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata  10.5M  9694M
cephfs.fs_name.data    data    39.0M  9694M
   cephfs-data-ec      data    12.0k  18.9G
       STANDBY MDS
fs_name.ceph-node01.uygvno    &lt;----------- 2 standby
fs_name.ceph-node02.kxxoad    &lt;-----------
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_mds_configure_standby_replay"><a class="anchor" href="#_mds_configure_standby_replay"></a>6.1. MDS configure Standby-replay</h3>
<div class="paragraph">
<p>Each CephFS file system may be configured to add standby-replay daemons. These standby daemons follow the active MDS’s metadata journal in order to reduce failover time in the event that the active MDS becomes unavailable. Each active MDS may have only one standby-replay daemon following it.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Once an MDS has entered the standby-replay state, it will only be used as a standby for the rank that it is following. If another rank fails, this standby-replay daemon will not be used as a replacement, even if no other standbys are available. For this reason, it is advised that if standby-replay is used, then every active MDS should have a standby-replay daemon.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Configuration of standby-replay on a file system is done using the below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs set fs_name allow_standby_replay true
# ceph -s | grep mds
    mds: 1/1 daemons up, 1 standby, 1 hot standby
# ceph fs status
fs_name - 0 clients
=======
RANK      STATE                  MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0        active      fs_name.ceph-node01.uygvno  Reqs:    0 /s  1022     21     15      0
0-s   standby-replay    fs_name.proxy01.bbnkcu    Evts:    0 /s  1013     12      6      0  &lt;-------- New Standby reply for Rank 0
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata  10.5M  9693M
cephfs.fs_name.data    data    39.0M  9693M
   cephfs-data-ec      data    12.0k  18.9G
       STANDBY MDS
fs_name.ceph-node02.kxxoad</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_mds_stand_by_affinity"><a class="anchor" href="#_mds_stand_by_affinity"></a>6.2. MDS Stand-by Affinity</h3>
<div class="paragraph">
<p>When failing over MDS daemons, a cluster’s monitors will prefer standby daemons with mds_join_fs equal to the file system name with the failed rank. If no standby exists with mds_join_fs equal to the file system name, it will choose an unqualified standby (no setting for mds_join_fs) for the replacement, or any other available standby, as a last resort. Note, this does not change the behaviour that standby-replay daemons are always selected before other standbys.</p>
</div>
<div class="paragraph">
<p>We can check our current status with the <code>ceph fs dump</code> command, our standby
daemon <code>ceph-node02.kxxoad</code> currently doesn&#8217;t have any affinity defined</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs dump | tail -7
[mds.fs_name.ceph-node01.uygvno{0:144102} state up:active seq 22 join_fscid=1 addr [v2:192.168.56.61:6800/1486188007,v1:192.168.56.61:6801/1486188007] compat {c=[1],r=[1],i=[7ff]}]
[mds.fs_name.proxy01.bbnkcu{0:144104} state up:standby-replay seq 1 join_fscid=1 addr [v2:192.168.56.24:6800/2208661102,v1:192.168.56.24:6801/2208661102] compat {c=[1],r=[1],i=[7ff]}]

Standby daemons:
[mds.fs_name.ceph-node02.kxxoad{-1:144140} state up:standby seq 1 join_fscid=1 addr [v2:192.168.56.62:6800/331662506,v1:192.168.56.62:6801/331662506] compat {c=[1],r=[1],i=[7ff]}]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s set the affinity for <code>ceph-node02.kxxoad</code> to our FS <code>fs_name</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph config set mds.fs_name.ceph-node02.kxxoad  mds_join_fs fs_name</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_running_more_than_one_active_mds_per_filesystem"><a class="anchor" href="#_running_more_than_one_active_mds_per_filesystem"></a>7. Running more than one Active MDS per Filesystem</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You should configure multiple active MDS daemons when your metadata performance is bottlenecked on the single MDS that runs by default.</p>
</div>
<div class="paragraph">
<p>Adding more daemons may not increase performance on all workloads. Typically, a single application running on a single client will not benefit from an increased number of MDS daemons unless the application is doing a lot of metadata operations in parallel.</p>
</div>
<div class="paragraph">
<p>Workloads that typically benefit from a larger number of active MDS daemons are those with many clients, perhaps working on many separate directories.</p>
</div>
<div class="paragraph">
<p><strong>Example</strong></p>
</div>
<div class="paragraph">
<p>First let&#8217;s reduce the count of wanted stand-by daemons, currently we have it
set to <code>2</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs set fs_name  standby_count_wanted 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now let&#8217;s modify the count and placement in the mds service spec file, I&#8217;m
taking out the host <code>ceph-node02</code> from the list:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 2
  hosts:
  - ceph-node01
  - proxy01
# ceph orch apply -i mds.yaml
Scheduled mds.fs_name update...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we can check that we only have 2 daemons, one of them as a hot-standby</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph orch ps | grep mds
mds.fs_name.ceph-node01.uygvno  ceph-node01               running (56m)     5m ago  17h    39.4M        -  16.2.8-85.el8cp  b2c997ff1898  4711409b661d
mds.fs_name.proxy01.bbnkcu      proxy01                   running (56m)     5m ago  17h    53.3M        -  16.2.8-85.el8cp  b2c997ff1898  5030c8246c68
# ceph -s | grep mds
    mds: 1/1 daemons up, 1 hot standby</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we can start with the configuration of a new Active MDS for our FS.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs get fs_name | grep max_mds
max_mds	1
# ceph fs set fs_name max_mds 2
# ceph -s
  cluster:
    id:     e6c62efc-9016-11ed-b206-2cc26078e4ef
    health: HEALTH_WARN
            1 filesystem is online with fewer MDS than max_mds</code></pre>
</div>
</div>
<div class="paragraph">
<p>We are missing one MDS daemon, to have 2 active MDS daemons, as we requested. Let&#8217;s modify the spec again</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 3
  hosts:
  - ceph-node01
  - ceph-node02
  - proxy01
# ceph orch apply -i mds.yaml
Scheduled mds.fs_name update...
# ceph -s | grep mds
    mds: 2/2 daemons up, 1 hot standby
# ceph fs status
fs_name - 0 clients
=======
RANK      STATE                  MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0        active      fs_name.ceph-node01.uygvno  Reqs:    0 /s  1022     21     15      0
 1        active      fs_name.ceph-node02.jsvbzq  Reqs:    0 /s    10     13     11      0
0-s   standby-replay    fs_name.proxy01.bbnkcu    Evts:    0 /s  1013     12      6      0</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p><strong>dns</strong>: dentries. Dentries is a data structure which represents a directory or a folder.</p>
</li>
<li>
<p><strong>inos</strong>: inodes. Inode is a data structure which provides a representation of a file</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Once an MDS rank is assigned to a daemon, it proceeds through a series of states to start:
replay – replaying journal
resolve – disambiguating perhaps-incomplete MDCache operations such as import/export
reconnect – waiting for clients to send reconnect messages
rejoin – bring cache into a consistent state with peers (load)
clientreplay – execute part-done client requests
active – normal operation</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Currently Rank0 with MDS <code>fs_name.ceph-node01.uygvno</code> has a standby daemon, but
out new MDS <code>fs_name.ceph-node02.jsvbzq</code> with rank1 has not standy daemon,
let&#8217;s add one more MDS for standby.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Even with multiple active MDS daemons, a highly available system still requires standby daemons to take over if any of the servers running an active daemon fail.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># vi mds.yaml
# cat mds.yaml
service_type: mds
service_id: fs_name
service_name: mds.fs_name
placement:
  count: 4
  hosts:
  - ceph-node01
  - ceph-node02
  - ceph-node03
  - proxy01
# ceph orch apply -i mds.yaml
# ceph orch ps | grep mds
mds.fs_name.ceph-node01.uygvno  ceph-node01               running (67m)     5m ago  17h    41.3M        -  16.2.8-85.el8cp  b2c997ff1898  4711409b661d
mds.fs_name.ceph-node02.jsvbzq  ceph-node02               running (6m)      6m ago   6m    15.8M        -  16.2.8-85.el8cp  b2c997ff1898  83f83ab42547
mds.fs_name.ceph-node03.zamikx  ceph-node03               running (27s)    24s ago  27s    22.1M        -  16.2.8-85.el8cp  b2c997ff1898  a62184868686
mds.fs_name.proxy01.bbnkcu      proxy01                   running (66m)     5m ago  17h    53.5M        -  16.2.8-85.el8cp  b2c997ff1898  5030c8246c68
# ceph fs status
fs_name - 0 clients
=======
RANK      STATE                  MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0        active      fs_name.ceph-node01.uygvno  Reqs:    0 /s  1022     21     15      0
 1        active      fs_name.ceph-node02.jsvbzq  Reqs:    0 /s    10     13     11      0
0-s   standby-replay    fs_name.proxy01.bbnkcu    Evts:    0 /s  1013     12      6      0
1-s   standby-replay  fs_name.ceph-node03.zamikx  Evts:    0 /s     0      3      1      0</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>In multiple active metadata server configurations</strong> a balancer runs which works to spread metadata load evenly across the cluster.</p>
</div>
<div class="paragraph">
<p>This usually works well enough for most users, but sometimes it is desirable to override the dynamic balancer with explicit metadata mappings to particular ranks.</p>
</div>
<div class="paragraph">
<p>This can allow the administrator or users to evenly spread the application load or limit the impact of users’ metadata requests on the entire cluster.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/subtree-partitioning.svg" alt="Subtree Partitioning" width="840" height="680">
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s mount our FS with the admin user at the root of the FS</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="
# ls -l /mnt
total 13312
drwxr-xr-x. 2 root root        1 Jan 10 06:41 clientec
drwxr-xr-x. 3 root root        2 Jan 10 05:45 dir4
-rw-r--r--. 1 root root 13631488 Jan 10 16:17 test-quota</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s configure static pinning, <code>dir4</code> will go to <code>rank0</code> and <code>clientec</code> to <code>rank1</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># setfattr -n ceph.dir.pin -v 1 /mnt/dir4/
# setfattr -n ceph.dir.pin -v 0 /mnt/clientec/
# getfattr -n ceph.dir.pin /mnt/dir4
# file: mnt/dir4
ceph.dir.pin="1"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The problem with traditional subtree partitioning is that the workload growth by depth (across a single MDS) leads to a hotspot of activity. This results in a lack of vertical scaling and wastage of non-busy resources/MDSs.</p>
</div>
<div class="paragraph">
<p>This led to adopting a more dynamic way of handling metadata: Dynamic
Subtree Partitioning, where load-intensive portions of the directory hierarchy
from busy MDSs are migrated to non-busy MDSs. , check out more information on
this <a href="https://docs.ceph.com/en/quincy/cephfs/multimds/#setting-subtree-partitioning-policies">upstream doc</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mds_cache"><a class="anchor" href="#_mds_cache"></a>8. MDS Cache</h2>
<div class="sectionbody">
<div class="paragraph">
<p>While the data for inodes in a Ceph file system is stored in RADOS and accessed by the clients directly, inode metadata and directory information is managed by the Ceph metadata server (MDS). The MDS’s act as mediator for all metadata related activity, storing the resulting information in a separate RADOS pool from the file data.</p>
</div>
<div class="paragraph">
<p>The Metadata Server coordinates a distributed cache among all MDS and CephFS clients. The cache serves to improve metadata access latency and allow clients to safely (coherently) mutate metadata state (e.g. via chmod). The MDS issues capabilities and directory entry leases to indicate what state clients may cache and what manipulations clients may perform (e.g. writing to a file).</p>
</div>
<div class="paragraph">
<p>A capability grants the client the ability to cache and possibly manipulate some portion of the data or metadata associated with the inode. When another client needs access to the same information, the MDS will revoke the capability and the client will eventually return it, along with an updated version of the inode’s metadata (in the event that it made changes to it while it held the capability).</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>mds_cache_memory_limit</strong>
This sets a target maximum memory usage of the MDS cache and is the primary tunable to limit the MDS memory usage. The MDS will try to stay under a reservation of this limit (by default 95%; 1 - mds_cache_reservation) by trimming unused metadata in its cache and recalling cached items in the client caches.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph config get mds.fs_name.ceph-node01.uygvno mds_cache_memory_limit
4294967296    ----&gt; 4GB</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>mds_cache_reservation</strong>
The cache reservation (memory or inodes) for the MDS cache to maintain. Once the MDS begins dipping into its reservation, it will recall client state until its cache size shrinks to restore the reservation.</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph config get mds.fs_name.ceph-node01.uygvno mds_cache_reservation
0.050000   ----&gt; 5%</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_fs_volumes_and_subvolumes"><a class="anchor" href="#_fs_volumes_and_subvolumes"></a>9. FS Volumes and Subvolumes</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A single source of truth for CephFS exports is implemented in the volumes module of the Ceph Manager daemon (ceph-mgr). The OpenStack shared file system service (manila), Ceph Container Storage Interface (CSI), storage administrators among others can use the common CLI provided by the ceph-mgr volumes module to manage the CephFS exports.</p>
</div>
<div class="paragraph">
<p>The ceph-mgr volumes module implements the following file system export abstractions:</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">FS subvolume groups, an abstraction for a directory level higher than FS subvolumes to effect policies</div>
Volumes and Subvolumes are used by rook in turn by ODF &amp; Fusion/ODF.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs volume ls
[
    {
        "name": "fs_name"
    }
]
# ceph fs subvolume create fs_name subvol1 --namespace-isolated
# ceph fs subvolume ls fs_name
[
    {
        "name": "subvol1"
    }
]
# ceph fs subvolume authorize  fs_name subvol1 cli_sub
# ceph fs subvolume authorized_list fs_name subvol1
[
    {
        "subvol1": "rw"
    }
]
# ceph auth get client.cli_sub
[client.cli_sub]
	key = AQCvfL5jF5NLGBAATtMf58oSE8KInuRqgG4Lug==
	caps mds = "allow rw path=/volumes/_nogroup/subvol1/8ee41860-a331-4aae-b95c-1ba0540ae883"
	caps mon = "allow r"
	caps osd = "allow rw pool=cephfs.fs_name.data namespace=fsvolumens_subvol1"
# ceph fs subvolume getpath fs_name subvol1
/volumes/_nogroup/subvol1/8ee41860-a331-4aae-b95c-1ba0540ae883
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/volumes/_nogroup/subvol1/8ee41860-a331-4aae-b95c-1ba0540ae883 /mnt -o name=cli_sub,secret="AQCvfL5jF5NLGBAATtMf58oSE8KInuRqgG4Lug=="
# touch /mnt/ok
#</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_snapshots"><a class="anchor" href="#_snapshots"></a>10. Snapshots</h2>
<div class="sectionbody">
<div class="paragraph">
<p>CephFS enables asynchronous snapshots by default These snapshots are stored in a hidden directory called .snap.</p>
</div>
<div class="paragraph">
<p>You can enable/disable snapshots per FS with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs set &lt;fs_name&gt; allow_new_snaps true</code></pre>
</div>
</div>
<div class="paragraph">
<p>When snapshots are enabled, all directories in CephFS will have a special .snap directory. (You may configure a different name with the client snapdir setting if you wish.)
To create a CephFS snapshot, you need to mount the FS and create a subdirectory under .snap with a name of your choice.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/ /mnt -o name=admin,secret="AQDQBLxjayHzNRAAfyt3lqvdAK6kQLN4QgQTSg=="
# mkdir /mnt/.snap/snapshot1
# ls /mnt
clientec  dir4  test-quota  volumes
# ls /mnt/.snap/snapshot1
clientec  dir4  test-quota  volumes
# rm -Rf /mnt/.snap/snapshot1</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
We are using admin, but you need to authorize clients to make snapshots
for the CephFS file system with the <code>s</code> client capability option.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_schedule_snapshots"><a class="anchor" href="#_schedule_snapshots"></a>10.1. Schedule snapshots</h3>
<div class="paragraph">
<p>The snap_schedule module is enabled with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph mgr module enable snap_schedule</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can schedule a snapshot and check it&#8217;s status with the help of the <code>ceph fs snap-schedule</code> command</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs snap-schedule add / 1h
Schedule set for path /
# ceph fs snap-schedule list /
/ 1h
# ceph fs snap-schedule status / | jq .
{
  "fs": "fs_name",
  "subvol": null,
  "path": "/",
  "rel_path": "/",
  "schedule": "1h",
  "retention": {},
  "start": "2023-01-11T00:00:00",
  "created": "2023-01-11T09:38:07",
  "first": null,
  "last": null,
  "last_pruned": null,
  "created_count": 0,
  "pruned_count": 0,
  "active": true
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>keep 24 snapshots at least an hour apart</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs snap-schedule retention add / h 24
Retention added to path /</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The snapshot schedule data is stored in a rados object in the cephfs metadata pool. At runtime, all data lives in a sqlite database that is serialized and stored as a rados object.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_client_sessions"><a class="anchor" href="#_client_sessions"></a>11. Client Sessions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If a client is misbehaving, or has died, we may want to evict it from the system
Eviction terminates the client&#8217;s session with the MDS (the one where you ran the command), and blacklists the client: i.e. records its address in the OSDMap as an instruction to the OSDs to refuse I/O to the client.
Other MDS daemons also watch the OSDMap and kill their sessions with the client, and refuse to open new sessions with it.
Eviction is a “big hammer”: it&#8217;s for when something has gone wrong, not a convenient way of unmounting clients.</p>
</div>
<div class="paragraph">
<p>Each client has a session with each MDS daemon to which it submits MDS requests.  MDS also stores a small metadata map for each session.  We can list these:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph tell mds.0 client ls
2023-01-11T05:44:46.262-0500 7f0803fff700  0 client.144601 ms_handle_reset on v2:192.168.56.24:6800/2208661102
2023-01-11T05:44:46.284-0500 7f0803fff700  0 client.144659 ms_handle_reset on v2:192.168.56.24:6800/2208661102
[
    {
        "id": 144451,
        "entity": {
            "name": {
                "type": "client",
                "num": 144451
            },
            "addr": {
                "type": "v1",
                "addr": "192.168.56.252:0",
                "nonce": 797768304
            }
        },
        "state": "open",
        "num_leases": 0,
        "num_caps": 1,
        "request_load_avg": 0,
        "uptime": 4779.8243540920002,
        "requests_in_flight": 0,
        "num_completed_requests": 1,
        "num_completed_flushes": 0,
        "reconnecting": false,
        "recall_caps": {
            "value": 0,
            "halflife": 60
        },
        "release_caps": {
            "value": 0,
            "halflife": 60
        },
        "recall_caps_throttle": {
            "value": 0,
            "halflife": 1.5
        },
        "recall_caps_throttle2o": {
            "value": 0,
            "halflife": 0.5
        },
        "session_cache_liveness": {
            "value": 0,
            "halflife": 300
        },
        "cap_acquisition": {
            "value": 0,
            "halflife": 10
        },
        "delegated_inos": [],
        "inst": "client.144451 v1:192.168.56.252:0/797768304",
        "completed_requests": [
            {
                "tid": 9,
                "created_ino": "0x0"
            }
        ],
        "prealloc_inos": [],
        "client_metadata": {
            "client_features": {
                "feature_bits": "0x0000000000007bff"
            },
            "metric_spec": {
                "metric_flags": {
                    "feature_bits": "0x000000000000001f"
                }
            },
            "entity_id": "admin",
            "hostname": "workstation-lb1719.rhpds.opentlc.com",
            "kernel_version": "4.18.0-305.el8.x86_64",
            "root": "/"
        }
    }
]</code></pre>
</div>
</div>
<div class="paragraph">
<p>MDS daemons will sometimes evict a client automatically, if the client has not communicated for a period of time.</p>
</div>
<div class="paragraph">
<p>Can be troublesome if the system has badly behaved clients:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Clients with flaky networks</p>
</li>
<li>
<p>Overloaded clients that go into swap-hell and stop responding</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>You can configure timeout <code>mds_session_timeout</code> (default 60s) and set <code>mds_session_blacklist_on_timeout</code> to false to attempt to let flaky clients rejoin after eviction.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deep_dive_metadata_layout"><a class="anchor" href="#_deep_dive_metadata_layout"></a>12. Deep Dive Metadata layout</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s take a look at the CephFS metadata layout in RADOS.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<code>ceph daemon mds.&lt;id&gt; flush journal</code> first if your filesystem is new to flush the metadata
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># rados -p cephfs.fs_name.meta ls
601.00000000
602.00000000
600.00000000
603.00000000
60a.00000000
613.00000000
1.00000000.inode
200.00000000
610.00000000
200.00000001
606.00000000
607.00000000
60e.00000000
mds1_openfiles.0
mds0_openfiles.0
60f.00000000
608.00000000
500.00000001
501.00000000
612.00000000
401.00000000
604.00000000
101.00000000.inode
500.00000000
mds_snaptable
605.00000000
101.00000000
mds1_sessionmap
mds0_inotable
611.00000000
100.00000000
mds0_sessionmap
60c.00000000
201.00000001
mds1_inotable
609.00000000
60b.00000000
60d.00000000
201.00000000
400.00000000
100.00000000.inode
1.00000000</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>mds_snaptable</strong> Tracks snapshots (obviously), One per filesystem: owned by rank 0, Map of snapid_t to SnapInfo Stored as a normal object (not omap)
<strong>Inode table</strong> mds0_inotable/mds1_inotable. Free list of inode numbers (no actual inodes in here) One per MDS rank: each rank is granted 1/MAX_MDS share of overall space.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cephfs-table-tool 0 show inode
{
    "0": {
        "data": {
            "version": 0,
            "inotable": {
                "projected_free": [
                    {
                        "start": 1099511627776,
                        "len": 1099511627776
                    }
                ],
                "free": [
                    {
                        "start": 1099511627776,
                        "len": 1099511627776
                    }
                ]
            }
        },
        "result": 0
    }
}</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Session Map</strong> mds0_sessionmap/mds1_sessionmap.</p>
<div class="ulist">
<ul>
<li>
<p>One per MDS rank Internal class</p>
</li>
<li>
<p><code>SessionMapStore</code> Stored as OMAP,</p>
</li>
<li>
<p>keys are <code>entity_name_t</code>, values are <code>Session</code></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cephfs-table-tool all show session
{
    "0": {
        "data": {
            "sessions": []
        },
        "result": 0
    },
    "1": {
        "data": {
            "sessions": []
        },
        "result": 0
    }
}</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Journal Pointer objects</strong> 400.00000000. MDS daemons have the ability to rewrite their journals safely by writing out a fresh one and then switching the “pointer” object over before purging the old one.</p>
</li>
<li>
<p><strong>Journal objects</strong> 200.00000000,200.00000001,200.0000000X .Object names increment as the journal grows, multiple objects will exist depending on the length of journal.  200.0000000 remains as journal header (contains expire/trim/write positions)</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_objects_from_data_pool"><a class="anchor" href="#_objects_from_data_pool"></a>12.1. Objects from Data pool</h3>
<div class="paragraph">
<p>Cephfs uses striping: like RBD, RGW.</p>
</div>
<div class="paragraph">
<p>It follows the patter: &lt;inode number&gt;.&lt;stripe index&gt;</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># rados -p cephfs.fs_name.data ls
100000003f0.00000003
100000003f0.00000002
100000003f0.00000001
100000003f0.00000000</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Controlled by <code>file_layout_t</code></p>
<div class="ulist">
<ul>
<li>
<p>Layout can only be changed on empty files: once data is written it is not moved around.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mds_autoscaler"><a class="anchor" href="#_mds_autoscaler"></a>13. MDS Autoscaler</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The MDS Autoscaler Module monitors file systems to ensure sufficient MDS daemons are available. It works by adjusting the placement specification for the orchestrator backend of the MDS service. To enable, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph mgr module enable mds_autoscaler</code></pre>
</div>
</div>
<div class="paragraph">
<p>The module will monitor the following file system settings to inform placement count adjustments:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">max_mds file system setting
standby_count_wanted file system setting</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Ceph monitor daemons are still responsible for promoting or stopping MDS according to these settings. The mds_autoscaler simply adjusts the number of MDS which are spawned by the orchestrator.</p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
