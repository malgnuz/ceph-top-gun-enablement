<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Object Storage Benchmarking :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/ceph_performance_object.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_ui.html">Deploy Ceph from the UI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_dashboard_metrics.html">Ceph Dashboard Management &amp; Metrics</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_configuration.html">Ceph Configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pgs.html">Ceph Health and PGs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_export.html">RBD Import/Export</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_mirroring.html">RBD Mirroring</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge.html">Challenge RBD</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_logging.html">Troubleshooting Logs Debug Mode</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_break_and_fix.html">Troubleshooting Break &amp; Fix Hands-on</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_fio.html">Benchmarking Ceph block and File</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>Ceph Benchmarking</li>
    <li><a href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/ceph_performance_object.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Object Storage Benchmarking</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Object storage has long been seen as a solution for the long-term retention of large volumes of archive or inactive data.  Object stores represent a cost-conscious platform for data that is generally streamed or accessed sequentially.  Backup data is an excellent example of this kind of workload.</p>
</div>
<div class="paragraph">
<p>Increasingly we see object stores used for high-performance workloads.  New solutions such as AI and analytics require access to data with high throughput, a degree of randomness, and high parallelism.  This profile can also be seen with more traditional applications such as CDNs (Content Data Networks), where multiple parallel streaming is the norm.</p>
</div>
<div class="paragraph">
<p>Storage performance metrics generally cover block-based storage and file systems, with little coverage for object stores (more on this later).  It’s easy to see why this scenario has developed, as block-based storage platforms are generally used for managing traditional OLTP and transactional applications that are latency-sensitive.</p>
</div>
<div class="paragraph">
<p>In this section we will provide some guiadence of setting up and using two of
the most popular benchmarking tools:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Cosbench</strong></p>
</li>
<li>
<p><strong>Warp</strong></p>
</li>
<li>
<p><a href="https://sibench.io/index.html">Sibench</a>(New)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Using an Object performance benchmarking tool like cosbench or warp to run
different load tests that will help us get very valuable baseline data so we
can get a rough estimate on the amount of nodes needed to reach a certain IO
throughput target in the Ceph cluster.</p>
</div>
<div class="paragraph">
<p>To generate a considerable amount of load on the Ceph cluster we will need a
certain number of client nodes to concurrently execute and put load on the S3
endpoint/Ceph cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_what_is_our_objective"><a class="anchor" href="#_what_is_our_objective"></a>2. What is our Objective?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting the Benchmarking tests set your objectives. here is an example:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Baseline performance testing. Looking for our limits.</p>
<div class="ulist">
<ul>
<li>
<p>Set the Goal</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>During this initial test our <strong>goal is to understand the limits of our cluster</strong>,
Ideally we will have enough clients so that we can saturate and take to the
limit the cluster, we then need to understand what is causing the performance
bottleneck, if it’s something related to our Ceph configuration, that can be
tuned to increase the performance or if we are seeing resource saturation
spinning drives,network stack. <strong>If our limitation/bottlenect is coming from the
hardware resources we have achieved the objective of this testing</strong> .</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Configure the Benchmark workload(Ideally very similar to production)</p>
<div class="ulist">
<ul>
<li>
<p>We want to work during the tests with 3 types of object sizes:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Small objects(1/15/30MB).  Throughput Testing.</p>
</li>
<li>
<p>Medium(32/64/128MB). Throughput Testing.</p>
</li>
<li>
<p>Mini &amp; Tiny (4KB-1MB). Objects per second Testing. Understanding the performance hit when working with small objects.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Workload types:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>GET. 80%. PUT. 15%. DELETE. 5%</p>
</li>
<li>
<p>GET 75%.  PUT. 15%. DELETE. 5%. STATS 5%.</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>What do we extract from the results
Once we finish this testing we need to have a clear understanding of the maximum throughput we are able to achieve on the full cluster, Individually with different object sizes and globally with a mix of object sizes, with the maximum total throughput of the cluster we will be able to calculate the maximum throughput per cluster node, this can be used in the future to calculate the number of nodes needed to achieve a certain target throughput.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_tools"><a class="anchor" href="#_tools"></a>3. Tools</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_cosbench"><a class="anchor" href="#_cosbench"></a>3.1. Cosbench</h3>
<div class="paragraph">
<p><a href="https://github.com/intel-cloud/cosbench/blob/master/COSBenchUserGuide.pdf">Cosbench User Guide</a> has all installation and config information.</p>
</div>
<div class="sect3">
<h4 id="_dependencies_installation"><a class="anchor" href="#_dependencies_installation"></a>3.1.1. Dependencies &amp; Installation:</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># yum install nc
# yum install java-1.8.0-openjdk.x86_64
# systemctl stop iptables; systemctl disable iptables
# systemctl stop firewalld; systemctl disable firewalld
# wget https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip
# unzip 0.4.2.c4.zip
# cd 0.4.2.c4
# chmod 755 *.sh</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_architecture"><a class="anchor" href="#_architecture"></a>3.1.2. Architecture:</h4>
<div class="ulist">
<ul>
<li>
<p>There is a controller node to drive and aggregate results.</p>
</li>
<li>
<p>There are driver nodes to run test driver processes. Each driver node can host multiple driver processes.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/cos_intro.png" alt="Cos Intro">
</div>
</div>
<div class="paragraph">
<p><code>./start_driver.sh</code> – Run this on each driver node. Can do it on master node too if master node is itself the lone driver node.
Can specify "[num_drivers][IP address of all drivers][base port]" as optional args.
Each driver then runs at port [base_port + index].
Default driver base port is 18088.</p>
</div>
<div class="paragraph">
<p><code>./start_controller.sh</code> – only on master node. Its <code><em>conf/controller.conf</em></code> should have entries for each worker node
in [driver&lt;n&gt;] sections. Default controller port is 19088.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuration_files"><a class="anchor" href="#_configuration_files"></a>3.1.3. Configuration files:</h4>
<div class="ulist">
<ul>
<li>
<p>conf/controller.conf – specifies how many drivers, and URLs of all driver process endpoints.
If driver nodes run multiple driver processes, this should have endpoints for every process.</p>
</li>
<li>
<p>conf/driver.conf – these are actually overwritten by start-driver.sh. So no point modifying these directly.
Instead modify conf/driver_template.conf.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Actual testing procedure is configured using a <strong>workload configuration file</strong> in conf/.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Attached a working Ceph S3 workload configuration - cosbench-ceph-s3-workload.conf - based on the default S3 workload conf conf/s3-config-sample.xml</p>
</li>
<li>
<p>Note the &lt;storage&gt; tag attributes.</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code><em>endpoint</em></code> should be the RGW host:port. For actual AWS S3 testing, just remove the endpoint attribute and
path_style_access can be removed too.</p>
</li>
<li>
<p><code>path_style_access=true</code> is extremely important for Ceph S3; Otherwise, the Amazon S3 SDK library used by
COSBench defaults to virtual host style access, tries to contact bucket URLs like
<a href="http://s3testqwer1.cephmon1" class="bare">http://s3testqwer1.cephmon1</a> and if RGW host is not configured for virtual host addressing,
S3 client fails with “Unknown name or service s3testqwer1.cephmon1”</p>
</li>
</ol>
</div>
</li>
<li>
<p>Another example S3 workload configuration: <a href="http://www.spinics.net/lists/ceph-users/msg28805.html" class="bare">http://www.spinics.net/lists/ceph-users/msg28805.html</a> . The multiple &lt;storage&gt; elements are not necessary – COSBench code seems to inherit global elements in each work stage or work element.</p>
</li>
<li>
<p>For actual AWS S3 endpoints:</p>
<div class="ulist">
<ul>
<li>
<p>Bucket names should be globally unique across all S3 users. Since default name s3testqwer&lt;n&gt; is likely to be created by somebody else, COSBench fails with 403 access denied errors with default</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Start a test</strong>:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">./cli.sh submit conf/s3test.conf</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Monitor a test</strong>: From a browser, open <a href="http://&lt;cosbench-controller-node:19088/controller/" class="bare">http://&lt;cosbench-controller-node:19088/controller/</a>. Then open the item under active workload, and drill down into workload, work stage and missions by clicking on “view details”.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_troubleshooting"><a class="anchor" href="#_troubleshooting"></a>3.1.4. Troubleshooting:</h4>
<div class="paragraph">
<p>Log files and logging levels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>log/system.log – the controller`s log.Logging level is set by log_level in [controller] section of conf/controller.conf. Set to DEBUG|INFO
log/mission/[mission-id].log – Actual worker logs. This is where any S3 client errors are recorded.
Set “log_level” to DEBUG|INFO in [driver_n_ ] sections of conf/controller.conf
Set “log_level” to DEBUG|INFO in [driver] section of conf/driver_template.conf.</p>
</li>
<li>
<p><code>"Error 403 Access denied for actual AWS S3 endpoint"</code> - Check if the bucket has a globally unique name.
The default s3testqwer&lt;n&gt; bucket names in workload configs are likely to be owned by somebody else already.</p>
</li>
<li>
<p>Error</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">  "[INFO] [NoneStorage] - performing PUT at /s3testqwer1
   [WARN] [S3Storage] - below exception encountered when creating bucket s3testqwer1: Unable to execute HTTP request: s3testqwer1.cephmon1...
   [NoneStorage] - performing PUT at /s3testqwer2/myobjects10
   [S3Storage] - below exception encountered when creating object myobjects10 at s3testqwer2: Unable to execute HTTP request: s3testqwer2.cephmon1: Name or service not known"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The error here is that S3 client is attempting to use virtual host style URLs [<a href="http://bucket.host" class="bare">http://bucket.host</a>]() but Ceph RGW is not configured to handle it by default.
Instead, tell S3 client to use path style URLs, by suffixing bucket and object names to URL. Set &lt;storage &#8230;&#8203; config=“&#8230;&#8203;.path_style_access=true”&gt; in workload config file.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/cosbench1.png" alt="cos image1">
</div>
</div>
</div>
<div class="sect3">
<h4 id="_example_template_for_cosbench"><a class="anchor" href="#_example_template_for_cosbench"></a>3.1.5. Example template for cosbench</h4>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;workload name="s3-sample" description="sample benchmark for s3"&gt;

  &lt;storage type="s3" config="accesskey=YOURACCESSKEY;secretkey=YOURSECRETKEY;endpoint=http://cephmon1;path_style_access=true" /&gt;

  &lt;workflow&gt;

    &lt;workstage name="init"&gt;
      &lt;work type="init" workers="1" config="cprefix=s3testqwer;containers=r(1,2)" /&gt;
    &lt;/workstage&gt;

    &lt;workstage name="prepare"&gt;
      &lt;work type="prepare" workers="1" config="cprefix=s3testqwer;containers=r(1,2);objects=r(1,10);sizes=c(64)KB" /&gt;
    &lt;/workstage&gt;

    &lt;workstage name="main"&gt;
      &lt;work name="main" workers="8" runtime="30"&gt;
        &lt;operation type="read" ratio="80" config="cprefix=s3testqwer;containers=u(1,2);objects=u(1,10)" /&gt;
        &lt;operation type="write" ratio="20" config="cprefix=s3testqwer;containers=u(1,2);objects=u(11,20);sizes=c(64)KB" /&gt;
      &lt;/work&gt;
    &lt;/workstage&gt;

    &lt;workstage name="cleanup"&gt;
      &lt;work type="cleanup" workers="1" config="cprefix=s3testqwer;containers=r(1,2);objects=r(1,20)" /&gt;
    &lt;/workstage&gt;

    &lt;workstage name="dispose"&gt;
      &lt;work type="dispose" workers="1" config="cprefix=s3testqwer;containers=r(1,2)" /&gt;
    &lt;/workstage&gt;
  &lt;/workflow&gt;

&lt;/workload&gt;</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_warp"><a class="anchor" href="#_warp"></a>3.2. Warp</h3>
<div class="paragraph">
<p>The warp benchmarking tool is a cloud ready S3 benchmarking tool that can be used to simulate a variety of object workloads.</p>
</div>
<div class="paragraph">
<p>This doc is intended to provide a quick how-to when deploying warp for ODF or RGW deployments.</p>
</div>
<div class="sect3">
<h4 id="_installing_warp"><a class="anchor" href="#_installing_warp"></a>3.2.1. Installing warp</h4>
<div class="paragraph">
<p>Warp has two main modes of execution, as a client and a server. The client is effectively the workload generator, and is told the attributes of the workload to execute by the server. The server component may also run in multiple modes;
workload orchestration with the client
results analysis
Results comparisons
You need to install warp on a server that has the ‘oc’ binary and access to the target k8s/OCP cluster.</p>
</div>
<div class="paragraph">
<p>Download warp using git or wget/unzip.</p>
</div>
<div class="paragraph">
<p>Using ‘git’ (your resulting directory will be called ‘warp’)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># git clone https://github.com/minio/warp.git</code></pre>
</div>
</div>
<div class="paragraph">
<p>Using wget and unzip (your resulting directory will be called warp-master)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># wget https://github.com/minio/warp/archive/refs/heads/master.zip &amp;&amp; unzip master.zip</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_using_warp"><a class="anchor" href="#_using_warp"></a>3.2.2. Using Warp</h4>
<div class="paragraph">
<p>Unlike tools like the benchmark operator, the warp clients are designed to persist. By default, each worker removes the objects it used from the target S3 instance as the final stage of the benchmark job (this can be changed with a --keep-data flag). Therefore when testing different workloads, you only need to deploy the clients once, and then submit different jobs to exercise different workload profiles.</p>
</div>
<div class="paragraph">
<p>Another default behavior is the generation of the analysis file. The server will attempt to create this on the root filesystem of the pod, which will typically fail with permissions. There are several ways to ensure the analysis data is persisted, shown here.</p>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_warp_for_odfnoobaa"><a class="anchor" href="#_configuring_warp_for_odfnoobaa"></a>3.2.3. Configuring warp for ODF/Noobaa</h4>
<div class="paragraph">
<p>The project folder provides sample yaml files for the client and server in the projects k8s directory.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To enable warp to run against an ODF noobaa environment</p>
<div class="ulist">
<ul>
<li>
<p>Create an OBC and extract the ACCESS/SECRET keys</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>[OPTIONAL] Create a separate namespace for the warp clients and jobs</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploy the warp clients</p>
</li>
<li>
<p>Modify the StatefulSet definition in warp.yaml file as follows
[Optional] If the target environment is using self-signed certs, you will need to tell the clients to skip SSL verification. You do this by updating the spec.template.spec.containers.args to include ‘--insecure’ as an args option</p>
</li>
<li>
<p>Create the clients</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># oc -n &lt;namespace&gt; create -f warp.yaml</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Deploy the server (batch job)</p>
</li>
<li>
<p>Modify warp-job.yaml</p>
<div class="paragraph">
<p>[Optional] give the job a specific name that reflects the type of run</p>
</div>
</li>
<li>
<p>Update the environment variables for WARP_ACCESS_KEY and WARP_SECRET_KEY with the credentials from step 1.</p>
</li>
<li>
<p>Update the args passed to the container</p>
</li>
<li>
<p>The first argument is the client action. Use ‘get’ for an initial smoke test</p>
<div class="paragraph">
<p>[Optional] ‘--objects’ defines the object count each client will act against (default is 2,500)</p>
</div>
</li>
<li>
<p>‘--bucket’ updated to reflect the bucket created by the OBC</p>
</li>
<li>
<p>‘--warp-client’ to reflect the internal dns names of the clients</p>
</li>
<li>
<p>‘--host’ should be used the internal S3 endpoint</p>
</li>
<li>
<p>Add a ‘--tls’ parameter</p>
</li>
<li>
<p>‘--concurrent’ is an int that governs the level of concurrency the client attempts with the server</p>
</li>
<li>
<p>‘--obj.size’ should be set to the object size for the workload e.g. 16MiB</p>
</li>
<li>
<p>Submit the job</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># oc -n &lt;namespace&gt; create -f warp-job.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>High level results are in the output of the job (output example)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># oc logs job.batch/warp-job</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_warp_for_odfrgw"><a class="anchor" href="#_configuring_warp_for_odfrgw"></a>3.2.4. Configuring warp for ODF/RGW</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create an object store user using the following yaml file.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat rgw_user.yaml
apiVersion: ceph.rook.io/v1
kind: CephObjectStoreUser
metadata:
  name: ceph-rgw-user
  namespace: openshift-storage
spec:
  store: ocs-storagecluster-cephobjectstore
  displayName: ceph-rgw-user

# oc create -f rgw_user.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Discover the secret name from the rgw user</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># oc get cephobjectstoreuser/&lt;user name&gt; -o jsonpath='{.status.info.secretName}'</code></pre>
</div>
</div>
</li>
<li>
<p>Retrieve the Access key, Secret Key and Endpoint from the newly created user’s secret.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># oc -n openshift-storage get secrets rook-ceph-object-user-ocs-storagecluster-cephobjectstore-ceph-rgw-user -o json | jq -r .data.AccessKey | base64 -d
# oc -n openshift-storage get secrets rook-ceph-object-user-ocs-storagecluster-cephobjectstore-ceph-rgw-user -o json | jq -r .data.SecretKey | base64 -d
# oc -n openshift-storage get secrets rook-ceph-object-user-ocs-storagecluster-cephobjectstore-ceph-rgw-user -o json | jq -r .data.Endpoint | base64 -d</code></pre>
</div>
</div>
</li>
<li>
<p>Or as a one-liner</p>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># oc get secret &lt;secret-name&gt; -o go-template='AccessKey={{.data.AccessKey | base64decode}}{{"\n"}}SecretKey={{.data.SecretKey | base64decode}}{{"\n"}}Endpoint={{.data.Endpoint | base64decode }}{{"\n"}}'</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_analysis_file_handling"><a class="anchor" href="#_analysis_file_handling"></a>3.2.5. Analysis File Handling</h4>
<div class="paragraph">
<p>The most important component of any test run is the results file, and warp is no different. There are several ways to expose the detailed results.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Simple</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To save the status file (zst) you can create a PVC that binds to an external NFS server. Now when you run the job with the ‘--benchdata’ parameter defined, the output stats will be written to the mountpoint, which can later be used as input for the analyze feature.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Programmatic
The data from a run can be exposed over http in JSON format (--serve), avoiding the requirement for PVC’s and NFS servers (as long as you’re happy with test results remaining local to your environment!). In fact the http endpoint supports the following endpoints (ref)</p>
</li>
<li>
<p><code>v1/stop</code>
Stops an active job, pod stays active due to the serve loop</p>
</li>
<li>
<p><code>v1/status</code>
JSON response, last_status and data_ready are two key fields</p>
</li>
<li>
<p><code>v1/aggregated</code>
Aggregated summary of the run in JSON format (This is probably the most useful - but it is very verbose!)</p>
</li>
<li>
<p><code>v1/operations/json</code>
Provides a JSON representation of the zst file</p>
</li>
<li>
<p><code>v1/operations</code>
Downloads the zst file
e.g. curl localhost:7762/v1/operations -o my-testrun.zst</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_analysis"><a class="anchor" href="#_analysis"></a>3.2.6. Analysis</h4>
<div class="paragraph">
<p>The warp binary has an analyze mode, which takes as input the zst file from a test run and produces a summary of the run’s performance, and may optionally be used to generate a CSV file that covers each op request.</p>
</div>
<div class="paragraph">
<p>You can run the analysis job as a pod in OCP, or execute it locally - all you really need is the zst file.</p>
</div>
<div class="paragraph">
<p><strong>Example</strong></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># podman run --rm -v /var/lib/nfs/data:/mnt minio/warp:latest analyze --analyze.v /mnt/warp-get-32mb.csv.zst


Operation: PUT (15). Ran 15s. Concurrency: 3. Warp Instances: 3.


Requests considered: 4:
 * Avg: 2.815s, 50%: 2.997s, 90%: 3.439s, 99%: 3.439s, Fastest: 2.24s, Slowest: 3.439s
Throughput:
* Average: 34.56 MiB/s, 1.08 obj/s


Throughput, split into 6 x 1s:
 * Fastest: 40.0MiB/s, 1.25 obj/s (1s, starting 00:33:45 UTC)
 * 50% Median: 34.3MiB/s, 1.07 obj/s (1s, starting 00:33:47 UTC)
 * Slowest: 31.0MiB/s, 0.97 obj/s (1s, starting 00:33:49 UTC)

Operation: GET (768). Ran 1m1s. Concurrency: 3. Warp Instances: 3.

Requests considered: 751:

 * Avg: 225ms, 50%: 35ms, 90%: 622ms, 99%: 1.953s, Fastest: 26ms, Slowest: 5.297s
 * TTFB: Avg: 118ms, Best: 8ms, 25th: 9ms, Median: 11ms, 75th: 21ms, 90th: 185ms, 99th: 1.316s, Worst: 5.277s
 * First Access: Avg: 1.947s, 50%: 1.946s, 90%: 5.297s, 99%: 5.297s, Fastest: 631ms, Slowest: 5.297s
 * First Access TTFB: Avg: 1.395s, Best: 611ms, 25th: 905ms, Median: 1.275s, 75th: 1.371s, 90th: 5.277s, 99th: 5.277s, Worst: 5.277s
 * Last Access: Avg: 744ms, 50%: 638ms, 90%: 1.891s, 99%: 1.891s, Fastest: 34ms, Slowest: 1.891s
 * Last Access TTFB: Avg: 319ms, Best: 9ms, 25th: 26ms, Median: 32ms, 75th: 946ms, 90th: 1.288s, 99th: 1.288s, Worst: 1.288s

Throughput:
* Average: 424.93 MiB/s, 13.28 obj/s

Throughput, split into 56 x 1s:
 * Fastest: 1092.6MiB/s, 34.14 obj/s (1s, starting 00:34:38 UTC)
 * 50% Median: 385.5MiB/s, 12.05 obj/s (1s, starting 00:34:30 UTC)
 * Slowest: 39.7MiB/s, 1.24 obj/s (1s, starting 00:34:04 UTC)</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_evaluation_of_warp"><a class="anchor" href="#_evaluation_of_warp"></a>3.2.7. Evaluation of Warp</h4>
<div class="paragraph">
<p>The warp project should not be considered feature complete, and has gaps relating to our current workflows and expectations.</p>
</div>
<div class="paragraph">
<p><strong>The Good:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>It’s simple to use and you can be running a benchmark in minutes within kubernetes</p>
</li>
<li>
<p>It supports bare-metal and k8s based deployments</p>
</li>
<li>
<p>It can provide extremely verbose output (per op), that could benefit problem determination</p>
</li>
<li>
<p>It’s written in golang, so dependencies aren’t a problem!</p>
</li>
<li>
<p>It supports GET/PUT/LIST/STAT/MIXED/MULTIPART</p>
</li>
<li>
<p>It outputs high level and detailed stats</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>The Bad:</strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>By default the output format is txt or at best csv - you need the http handler to access summary analysis in JSON format</p>
</li>
<li>
<p>Logs don’t have timestamps!</p>
</li>
<li>
<p>There isn’t a UI, so organizing jobs and managing a suit of tests is an exercise for the user</p>
</li>
<li>
<p>Documentation is incomplete - another read-the-code™ project?</p>
</li>
<li>
<p>Owned by a competitor, with some feature specific to the minio platform (e.g. server profiling)</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_links"><a class="anchor" href="#_links"></a>4. Links</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://old.ceph.com/planet/tooling-for-large-scale-red-hat-ceph-storage-performance-testing/" class="bare">https://old.ceph.com/planet/tooling-for-large-scale-red-hat-ceph-storage-performance-testing/</a></p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
