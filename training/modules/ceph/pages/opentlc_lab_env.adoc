== Red Hat Ceph Storage Lab Introduction

.Goals
* Provision the lab environment
* Become familiar with the lab environment
* Install Ceph Storage using the `cephadm` utility

:numbered:

== Provision Lab Environment

=== Provision Environment

The lab environment consists of 13 virtual servers, one of which is a workstation/bastion host. In this section, you provision the lab environment, which sets up the components required for the lab.

The lab environment is a playground with instructions on setting up some of the scenarios. The lab is a guide and does not provide step-by-step instructions, nor does it provide details on implementing best practices. For topics or scenarios that are not covered in this lab, or commands that do not provide the expected output, refer to Red Hat's documentation.

. Navigate to the link:https://labs.opentlc.com[OPENTLC lab portal^] and log in with your username and password.

[NOTE]
If you do not remember your username or password, go to the link:https://www.opentlc.com/pwm[OPENTLC Account Management page^] to obtain a username reminder or reset your password.

. Navigate to *Services -> Catalogs -> All Services -> OPENTLC Cloud Infrastructure Labs*.

. On the left side of the screen, select *Ceph 5 Playground*.

. On the right, click *Order*.

. On the next page, check the box to confirm that you understand the runtime and expiration dates, then click *Submit* on the bottom right to order your environment.

. After a few minutes, check your email account for a message from Red Hat confirming that provisioning started.

[WARNING]
====
Do not select *App Control -> Start* after ordering the lab. The lab environment is already building, and selecting *Start* may corrupt it or cause other complications.
====

[TIP]
====
You can select *Status* to view the status of your environment.
====

=== Set Up Registry Prerequisites

In this section, you verify that you can access `registry.redhat.io` and retrieve your username and password token.

. Navigate to link:https://access.redhat.com/terms-based-registry/\#/accounts[https://access.redhat.com/terms-based-registry/#/accounts^].

. Select your account name, and note your username and password from the *Token Information* tab:

== Explore Lab Environment and Server List

The Ceph lab environment provisioned for each student provides all of the requirements to fulfil the exercises.

[NOTE]
The lab environment is cloud-based, so that you can access it over the Internet from anywhere. However, do not expect the performance to match a dedicated environment.

[NOTE]
====
You can access any system in your lab environment using the virtual console, the link provided in the email message you received with information about accessing your lab environment.
====

[WARNING]
====
While you have root access to all of the systems, including the workstation node, and you can make any configuration changes necessary, do not enable root login from SSH to the workstation node. The default password is not secure, and your machine may be compromised and subsequently deleted without warning.
====

. Verify that your environment is ready for use by connecting to your administration host through SSH using the details provided in the external provisioning email, which includes your username and password:

.Example Command
[source,sh]
-----
# ssh username@workstation-67b8.dynamic.opentlc.com
-----
* It may take up to 30 minutes for your lab environment to be ready. If you cannot connect to your administration host, continue to try periodically until your connection succeeds.

[NOTE]
====
* If you are using Windows either use the built-in SSH client present on recent builds of Windows 10 and Windows 11 or use a terminal program such as PuTTY to connect instead of the `ssh` command shown.

* To learn more about SSH and keys, read link:https://www.opentlc.com/ssh.html[Setting Up an SSH Key Pair^].
====

== Prepare for Red Hat Ceph Storage Installation

=== Prepare the workstation

Once you have logged in via SSH to your opentlc workstation, and used sudo to
escalate into
root,  we are going to
run a playbook to get the prereqs for the labs in place.

Clone the cephadm automation repo. You will need to authenticate with your key to get access,
in this example, we use a private key in the root of the workstation called
`key.rsa` to authenticate against IBMs GitHub.

----
GIT_SSH_COMMAND="ssh -i key.rsa" git clone git@github.ibm.com:dparkes/cephadmdeploy.git
----

[IMPORTANT]
====
You will need to authenticate against github.ibm.com to be able to download the
git repo, you could for example re-create a ssh keypair for the root user in the workstation node
and add it to your github account for example:

----
ssh-keygen -t rsa -f /root/key.rsa -N ''
Generating public/private rsa key pair.
Your identification has been saved in gitkey.
Your public key has been saved in key.rsa
----

Add the public `/root/key.rsa.pub` to your github account -> settings -> ssh and GPG keys
====


We then run the client/workstation playbook.

----
# ansible-galaxy collection install community.crypto
# ansible-playbook cephadmdeploy/ansible-cephadm-deploy/prepare-client.yaml -i workstation,
----

=== Prepare Ceph nodes for deployment

Once the client playbook finishes, we can start preparing our ceph clusters. We
have eight servers available, four per ceph cluster.

*Ceph Cluster1:* ceph-node01,ceph-node02,ceph-node03,proxy01

*Ceph Cluster2:* ceph-mon01,ceph-mon02,ceph-mon03,proxy02

image::opentlc_lab.jpg[Opentcl Diagram]


The previous client playbook created two inventory files for us.

----
# cat /root/cluster1
ceph-node01.example.com
ceph-node02.example.com
ceph-node03.example.com
proxy01.example.com

[admin]
ceph-node01.example.com

[osds]
ceph-node01.example.com
ceph-node02.example.com
ceph-node03.example.com

[mgmt]
proxy01.example.com

[client]
workstation.example.com
----

----
# cat /root/cluster2
ceph-mon01.example.com
ceph-mon02.example.com
ceph-mon03.example.com
proxy02.example.com

[admin]
ceph-mon01.example.com

[osds]
ceph-mon01.example.com
ceph-mon02.example.com
ceph-mon03.example.com

[mgmt]
proxy02.example.com

[client]
workstation.example.com
----

To get all the pre-requisites ready to deploy our ceph clusters, we can run the
`cephadmdeploy/ansible-cephadm-deploy/deploy-cephadm.yaml` for each of our
clusters.

[NOTE]
====
This playbook doesn't deploy ceph it just prepares the pre-reqs needed, repos,
DNS name resolution, etc
====

For that first, we need to configure a group_vars file

----
# vi cephadmdeploy/ansible-cephadm-deploy/group_vars/all.yaml
update_cluster_os: true
dedicated_observability: true
reg_password: 'REG_PASS'    <--------- MODIFY
reg_username: email@email.com    <--------- MODIFY 
rhcs_subscription_username: email@redhat.com  <--------- MODIFY
rhcs_subscription_password: 'SUB_PASS'        <--------- MODIFY
hosts_add_ansible_managed_hosts: false
dnsmasq_upstream_servers_ip: 150.239.16.12
----

[NOTE]
====
If `update_cluster_os: true` is set to true a full OS upgrade will take place
to the latest RHEL 8 minor version.
====

For each cluster, we run the following:

----
# ansible-playbook -i /root/cluster1 cephadmdeploy/ansible-cephadm-deploy/deploy-cephadm.yaml
----

[IMPORTANT]
====

Once the playbook for cluster 1 finishes, you need to run the same command but the inventory of cluster 2

----
# ansible-playbook -i /root/cluster2 cephadmdeploy/ansible-cephadm-deploy/deploy-cephadm.yaml
----

====



[IMPORTANT]
====
If you are following the training, don't use these automated scripts, follow
the manual steps of a full ceph deployment in this link for the ceph_deploy_basic.adoc[CLI] or this
one for the ceph_deploy_ui.adoc[UI]
====

Once the playbook finishes, you are ready to start the ceph deployment. If
needed several scripts have been created in the `/root` filesystem of our admin host
`ceph-node01.example.com`

----
# ssh ceph-node01.example.com ls *.sh
cephadm-ansible-run.sh   <--- runs Cephadm-ansible preflight playbook
cluster-install.sh       <--- Deploys the cephcluster with cephadm
cluster-postinstall.sh   <--- Configures Ceph post deploy, ceph rbd pool and cephfs deployment
cluster-wait-until-installed.sh  <--- waits until ceph is healthy
one_step_cluster_deploy.sh <--- Deploy full cluster in one go
purge_ceph_cluster.sh  <--- Purge/Delete current cluster
zap-disks.sh <--- Zaps Disks
----

The cluster-install.sh script is used for the cephadm bootstrap a spec file `ceph-node01.example.com`. You can edit and modify
this file to your needs:

----
# ls -l /root/cluster-spec.yaml
-rw-r--r-- 1 root root 1240 Dec 27 16:13 /root/cluster-spec.yaml
----

