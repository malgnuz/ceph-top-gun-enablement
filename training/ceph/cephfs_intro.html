<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Cephfs Introduction :: Ceph Top Gun Enablement</title>
    <link rel="canonical" href="https://likid0.github.io/ceph-top-gun-enablement/training/ceph/cephfs_intro.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../../_/css/site.css">
    <script>var uiRootPath = '../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">
          <img src="../../_/img/header_logo_reverse.svg" height="48px" alt="Ceph">
      </a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Get Help</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/5" target="_blank">Ceph Documentation</a>
            <a class="navbar-item" href="https://bugzilla.redhat.com/describecomponents.cgi?product=Red%20Hat%20OpenShift%20Container%20Storage" target="_blank">Browse Bugs</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Improve Guides</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/likid0/ceph-top-gun-enablement/issues/new/choose" target="_blank">Open Issue</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Infos</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://www.redhat.com/en/blog/channel/red-hat-storage" target="_blank">Our Blog</a>
            <a class="navbar-item" href="https://www.youtube.com/channel/UCoyG8VyvB-XUxQl1mD3T3Gw" target="_blank">Youtube</a>
            <a class="navbar-item" href="https://docs.ceph.com/en/latest/" target="_blank">Ceph Storage Technology</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="training" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Ceph Top-Gun Enablement</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Lab Setup</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="opentlc_lab_env.html">Opentlc Lab Env</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Core Ceph</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_introduction.html">Ceph Introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_architecture.html">Ceph Architecture</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cluster_partitioning.html">Ceph Cluster Partitioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_hardware.html">Ceph Hardware Recommendations</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_intro.html">Ceph Install Methods </a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephadm_intro.html">Cephadm Orchestrator</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_basic.html">Deploy Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deploy_ui.html">Deploy Ceph from the UI</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_dashboard_metrics.html">Ceph Dashboard Management &amp; Metrics</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cli_intro.html">Ceph CLI basic commands</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_configuration.html">Ceph Configuration</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pools.html">Ceph storage pools config</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_pgs.html">Ceph Health and PGs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_bluestore.html">Ceph OSD Bluestore</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_recovery.html">Ceph OSD Failure/Recovery</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephx.html">Rados CephX Auth/AuthZ</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_version.html">What version of Ceph am I running?</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph-upgrades_cephadm.html">Upgrade Ceph with Cephadm</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge.html">Challenge Ceph Deployment</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RADOS Block Device</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_intro.html">RADOS Block Device introduction</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_export.html">RBD Import/Export</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_mirroring.html">RBD Mirroring</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge.html">Challenge RBD</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">CephFS Shared FileSystem</span>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="cephfs_intro.html">CephFS introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephfs_advanced.html">CephFS Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge.html">Challenge Cephfs</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph RadosGW</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_intro.html">RGW Introduction &amp; Deployment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_arch_deep_dive.html">RGW Deep Dive</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ha.html">RGW High Availability</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_ssl.html">RGW &amp; Ingress with SSL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_users_quotas.html">RGW Users &amp; Quotas</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_auth.html">RGW Auth &amp; Authz</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_object_versioning.html">RGW S3 Object Versioning</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_placement_and_storage_classes.html">RGW Placement &amp; Storage Classes</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_life_cycle_management.html">RGW Life Cycle Management</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_policy.html">RGW S3 Bucket Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_introduction.html">RGW Secure Token Service</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_sts_bucket_role_policy.html">RGW Bucket vs Role Policy</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_multisite.html">RGW Multisite Replication</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_cloudsync.html">RGW Object Cloud Transition</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_archive.html">RGW Archive Zone</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_presignedurl.html">RGW presigned URL</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_opslog.html">RGW Opslog</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="radosgw_bucket_notification.html">RGW bucket Notification</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge.html">Challenge RGW</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Troubleshooting</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_logging.html">Troubleshooting Logs Debug Mode</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-nearfull-osds.html">Troubleshooting nearfull OSDs</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_bluestore.html">Troubleshooting Bluestore issues</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="trouble-shooting-large-omap-objects.html">Troubleshooting Large Omap Objects</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="troubleshooting_break_and_fix.html">Troubleshooting Break &amp; Fix Hands-on</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Benchmarking</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_example.html">Setting the Inital Baseline</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_fio.html">Benchmarking Ceph block and File</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_performance_object.html">Benchmarking Ceph Object(RGW)</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Stretched</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rhcs-stretched-deploy.html">Ceph Stretch Mode</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Ceph Challenge Solutions</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_deployment_challenge_solution.html">Ceph Deployment Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="cephrbd_challenge_solution.html">Ceph RBD Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_cephfs_challenge_solution.html">Ceph CephFS Solution</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="ceph_rgw_challenge_solution.html">Ceph RGW Solution</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Ceph Top-Gun Enablement</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Ceph Top-Gun Enablement</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Ceph Top-Gun Enablement</a></li>
    <li>CephFS Shared FileSystem</li>
    <li><a href="cephfs_intro.html">CephFS introduction &amp; Deployment</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///antora/training/modules/ceph/pages/cephfs_intro.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Cephfs Introduction</h1>
<div class="sect1">
<h2 id="_introduction"><a class="anchor" href="#_introduction"></a>1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Ceph File System, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, RADOS. CephFS endeavours to provide a state-of-the-art, multi-use, highly available, and performant file store for various applications, including traditional use cases like shared home directories, HPC scratch space, and distributed workflow shared storage.</p>
</div>
<div class="paragraph">
<p>CephFS achieves these goals through the use of some novel architectural choices. Notably, file metadata is stored in a separate RADOS pool from file data and served via a resizable cluster of Metadata Servers, or MDS, which may scale to support higher throughput metadata workloads. File system clients have direct access to RADOS for reading and writing file data blocks. For this reason, workloads may linearly scale with the size of the underlying RADOS object store; that is, there is no gateway or broker mediating data I/O for clients.</p>
</div>
<div class="paragraph">
<p>Access to data is coordinated through the cluster of MDS, which serve as authorities for the state of the distributed metadata cache cooperatively maintained by clients and MDS. Mutations to metadata are aggregated by each MDS into a series of efficient writes to a journal on RADOS; no metadata state is stored locally by the MDS. This model allows for coherent and rapid collaboration between clients within a POSIX file system.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/cephfs-architecture.svg" alt="cephfs" width="740" height="580">
</div>
</div>
<div class="paragraph">
<p>Upstream documentation on MDS Architecture and Concepts <a href="https://docs.ceph.com/en/latest/cephfs/#cephfs-concepts">link</a></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_mds_the_metadata_server"><a class="anchor" href="#_mds_the_metadata_server"></a>2. MDS the metadata server</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Metadata Server (MDS) manages metadata for CephFS clients.</p>
</div>
<div class="paragraph">
<p>While the data for inodes in a Ceph file system is stored in RADOS and accessed by the clients directly, inode metadata and directory information is managed by the Ceph metadata server (MDS). The MDSs act as mediators for all metadata-related activity, storing the resulting information in a separate RADOS pool from the file data.</p>
</div>
<div class="paragraph">
<p>CephFS clients can request that the MDS fetch or change inode metadata on its
behalf, but an MDS can also grant the client capabilities (aka caps) for each
inode</p>
</div>
<div class="paragraph">
<p>Since the cache is distributed, the MDS must take great care to ensure that no client holds capabilities that may conflict with other clients’ capabilities or operations that it does itself. This allows cephfs clients to rely on much greater cache coherence than a filesystem like NFS, where the client may cache data and metadata beyond the point where it has changed on the server.</p>
</div>
<div class="paragraph">
<p>When a client needs to query/change inode metadata or perform an operation on a directory, it has two options. It can request the MDS directly or serve the information out of its cache. With CephFS, the latter is only possible if the client has the necessary caps.</p>
</div>
<div class="paragraph">
<p>Clients can send simple requests to the MDS to query or request changes to certain metadata. The replies to these requests may also grant the client a certain set of caps for the inode, allowing it to perform subsequent requests without consulting the MDS.</p>
</div>
<div class="paragraph">
<p>Clients can also request caps directly from the MDS, which is necessary to read or write file data.</p>
</div>
<div class="paragraph">
<p>MDS daemons operate in two modes: active and standby.  You need a minimum of 2 mds services to have HA at the MDS service level.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploy_cephfs"><a class="anchor" href="#_deploy_cephfs"></a>3. Deploy CephFS</h2>
<div class="sectionbody">
<div class="paragraph">
<p>One or more MDS daemons are required to use the CephFS file system. These are created automatically if the newer <code>ceph fs volume</code> interface is used to create a new file system.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For more control over the deploying process, you can do a 3 step process:
- Manually create the pools that are associated with the CephFS
- Start the MDS service on the hosts
- Create the CephFS file system.
In our example, we are going to do the single command process to get everything ready
with the help of the <code>ceph fs volume</code> interface</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a CephFS volume, specifying <code>fs_name</code> as the name and a comma-separated list of host names as the placement:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh"># ceph fs volume create fs_name --placement=ceph-node01,proxy01
Volume created successfully (no MDS daemons created)</code></pre>
</div>
</div>
</li>
<li>
<p>Confirm that the CephFS volume was created:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh"># ceph fs volume ls
[
    {
        "name": "fs_name"
    }
]

# ceph osd lspools | grep cephfs
6 cephfs.fs_name.meta
7 cephfs.fs_name.data</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If we use the ceph fs volume create fs_name --placement=ceph-node01,proxy01 ,
cephadm the orchestrator takes care of deploying the MDS service for us, so it
takes care of creating the ceph pools, the ceph daemons, and the ceph FS</p>
</div>
<div class="paragraph">
<p>If you need to manually create the MDS services you can use this example:</p>
</div>
<div class="paragraph">
<p>Create a YAML file called <code>mds.spec.yaml</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">[source,yaml]
cat &lt;&lt;EOF &gt; mds.spec.yaml
service_type: mds
service_id: fs_name
placement:
  count: 2
  hosts:
  - ceph-node01
  - proxy01
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>As the root user on the <code>ceph-node01</code> server, apply the specification to manually deploy the MDS daemons using the YAML file that you created:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh"># ceph orch apply -i mds.spec.yaml
Scheduled mds.fs_name update...</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>List the services that are running on the new installation and verify that the <code>mds.fs_name</code> service is created:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh"># ceph orch ls | grep mds
NAME                       PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
mds.fs_name                           2/2  3m ago     3m   count:2</code></pre>
</div>
</div>
</li>
<li>
<p>View the <code>mds</code> daemon processes that are running:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh"># ceph orch ps | grep mds
NAME                               HOST                    PORTS        STATUS         REFRESHED  AGE  VERSION  IMAGE ID      CONTAINER ID
mds.fs_name.ceph-node01.vnuima      ceph-node01               running (19s)  13s ago    19s  16.2.4   8d91d370c2b8  c91ca8508916
mds.fs_name.proxy01.txydml         proxy01                running (17s)  15s ago    17s  16.2.4   8d91d370c2b8  d4c2cd362001</code></pre>
</div>
</div>
</li>
<li>
<p>Verify that <code>mds</code> is available and up:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-sh hljs" data-lang="sh"># ceph -s
cluster:
	id:     7d4ee168-d9b9-11eb-bc7e-2cc260754989
	health: HEALTH_WARN
					nodeep-scrub flag(s) set

services:
	mon: 3 daemons, quorum ceph-node01.example.com,ceph-node02,ceph-node03 (age 22m)
	mgr: ceph-node01.example.com.cntwzr(active, since 22m), standbys: ceph-node02.pxyuuu
	mds: 1/1 daemons up, 1 standby
	osd: 3 osds: 3 up (since 22m), 3 in (since 13h)
			 flags nodeep-scrub

data:
	volumes: 1/1 healthy
	pools:   4 pools, 129 pgs
	objects: 34 objects, 4.1 MiB
	usage:   25 MiB used, 30 GiB / 30 GiB avail
	pgs:     129 active+clean

# ceph fs status
cephfs - 0 clients
======
RANK  STATE              MDS                ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  cephfs.ceph-node03.ifnlti  Reqs:    0 /s    10     13     12      0
       POOL           TYPE     USED  AVAIL
cephfs.cephfs.meta  metadata  96.0k  9609M
cephfs.cephfs.data    data       0   9609M
     STANDBY MDS
cephfs.proxy01.udpgpo
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)

# ceph fs ls
name: cephfs, metadata pool: cephfs.cephfs.meta, data pools: [cephfs.cephfs.data ]

# ceph mds stat
fs_name:1 {0=fs_name.ceph-node01.gojgii=up:active} 1 up:standby</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cephfs_clients_how_to_mount_cephfs"><a class="anchor" href="#_cephfs_clients_how_to_mount_cephfs"></a>4. Cephfs Clients, how to mount Cephfs?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can mount CephFS file systems with:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The kernel client</p>
</li>
<li>
<p>The FUSE client</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The kernel driver is developed separately from the core ceph code, and as such,
it sometimes differs from the FUSE driver in feature implementation. The
following details the implementation status of various CephFS features in the
kernel driver. For more information on the current kernel client, supported
features check this upstream <a href="https://docs.ceph.com/en/quincy/cephfs/kernel-features/">link</a></p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The kernel client requires Linux, which is available starting with RHEL 8.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pre_reqs_for_mounting_a_cephfs_filesystem"><a class="anchor" href="#_pre_reqs_for_mounting_a_cephfs_filesystem"></a>5. Pre-Reqs for Mounting a Cephfs Filesystem</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this Lab, we are going to mount the Cephfs Filesystems from the
<code>workstation.example.com</code> node, so we need to run the following pre-req
commands for this server.</p>
</div>
<div class="paragraph">
<p>Install the ceph-common package. For the FUSE client, also install the ceph-fuse package.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">workstation# dnf install ceph-common ceph-fuse -y</code></pre>
</div>
</div>
<div class="paragraph">
<p>A minimal Ceph configuration file must be stored in <code>/etc/ceph/ceph.conf</code> by
default. We are going to copy the conf file from our admin ceph node, in this
case, <code>ceph-node01</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">workstation# scp -p ceph-node01:/etc/ceph/ceph.conf /etc/ceph/ceph.conf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Authorise the client to access the CephFS file system. In this example, we give
the user <code>0</code> Read/Write in the CephFS filesystem we created.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph-node01# ceph fs authorize fs_name client.0 / rw
[client.0]
	key = AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ==</code></pre>
</div>
</div>
<div class="paragraph">
<p>Get the new authorisation key with the ceph auth to get command and copy it to the
<code>/etc/ceph</code> folder</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph-node01# ceph auth get client.0 -o /etc/ceph/ceph.client.0.keyring
ceph-node01# cat /etc/ceph/ceph.client.0.keyring
[client.0]
	key = AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ==
	caps mds = "allow rw fsname=fs_name"
	caps mon = "allow r fsname=fs_name"
	caps osd = "allow rw tag cephfs data=fs_name"</code></pre>
</div>
</div>
<div class="paragraph">
<p>From <code>workstation</code> run scp to copy the keyring in <code>ceph-node01</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">workstation#  scp -p ceph-node01:/etc/ceph/ceph.client.0.keyring /etc/ceph/ceph.client.0.keyring
Warning: Permanently added 'ceph-node01,172.16.7.61' (ECDSA) to the list of known hosts.
ceph.client.0.keyring                                                                                        100%  181   303.2KB/s   00:00</code></pre>
</div>
</div>
<div class="paragraph">
<p>Check that we can now query our ceph cluster from node <code>workstation</code>, we
specify the id of the user we created called <code>0</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph --id 0 -s
  cluster:
    id:     a8292be8-8c21-11ed-b76b-2cc26078e4ef
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we have the fulfilled the pre-reqs we are ready to mount the CephFS Filesystem.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you have more than one FS configured in your ceph cluster you will need to
add an extra parameter with the FS you want to mount to all your mount commands, examples:</p>
</div>
<div class="paragraph">
<p>Fuse client</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph-fuse --id 0 /mnt/cephfs-fuse --client_fs fs_name</code></pre>
</div>
</div>
<div class="paragraph">
<p>Kernel client</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/dir0 /mnt/cephfs-kernel -o name=0,secret="AQDTCOVjxKGiEBAA2EW9BpnpEz/5UQ+keKVfVw==",fs=fs_name</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_fuse_client"><a class="anchor" href="#_fuse_client"></a>5.1. FUSE client</h3>
<div class="paragraph">
<p>With the Fuse Client installed, we can simply run the command <code>ceph-fuse</code> and
the id of our user</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph-fuse --id 0 /mnt
ceph-fuse[31811]: starting ceph client
2023-01-04T07:08:45.962-0500 7fb89f204380 -1 init, newargv = 0x55ae882073d0 newargc=15
ceph-fuse[31811]: starting fuse
# df -h | grep mnt
ceph-fuse       9.5G     0  9.5G   0% /mnt</code></pre>
</div>
</div>
<div class="paragraph">
<p>ceph-fuse mounts by default the / root of the filesystem, if we want to mount
at a specific tree level, we can use the -r parameter, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># mkdir /mnt/dir0
# umount /mnt
# ceph-fuse --id 0 -r /dir0 /mnt
fuse[31883]: starting ceph client
2023-01-04T07:11:29.032-0500 7fca021b4380 -1 init, newargv = 0x55f5dd7908d0 newargc=15
ceph-fuse[31883]: starting fuse
# ls -l /mnt
total 0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s add a file and check the pool df status, to verify that files are being
stored in the fs_name filesystem, still from our workstation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">dd if=/dev/zero of=/mnt/100MB bs=4k iflag=fullblock,count_bytes count=100M</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can write to the FS ok, but we can&#8217;t query the FS status with user 0 that we
created it doesn&#8217;t have permisions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph --id 0 fs status
Error EACCES: access denied: does your client key have mgr caps? See http://docs.ceph.com/en/latest/mgr/administrator/#client-authentication</code></pre>
</div>
</div>
<div class="paragraph">
<p>Ah ok, so we are getting an error when trying to get information about the
filesystem status with the permissions we have assigned to the user with id 0, we
don&#8217;t have the needed permissions on our user, let&#8217;s copy the admin keyring and
use it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># scp -p ceph-node01:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring
Warning: Permanently added 'ceph-node01,172.16.7.61' (ECDSA) to the list of known hosts.
ceph.client.admin.keyring                                                                                                 100%  151   232.2KB/s   00:00
ceph fs status
fs_name - 1 clients
=======
RANK  STATE              MDS                 ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  fs_name.ceph-node01.gojgii  Reqs:    0 /s    13     16     13      4
        POOL           TYPE     USED  AVAIL
cephfs.fs_name.meta  metadata   200k  9606M
cephfs.fs_name.data    data     256M  9606M
       STANDBY MDS
fs_name.ceph-node02.mvtdpg
MDS version: ceph version 16.2.8-85.el8cp (0bdc6db9a80af40dd496b05674a938d406a9f6f5) pacific (stable)</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we want to auto-mount the FS on boot, we need to add it to /etc/fstab with the following format:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
# echo 'ceph-node01:6789,ceph-node02:6789,ceph-node03:6789 /mnt fuse.ceph ceph.id=0,ceph.client_mountpoint=/dir0,_netdev 0 0' &gt;&gt; /etc/fstab
# mount /mnt</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s leave the FS umounted to try out the kernel client in the next section.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When using the FUSE client as a non-root user, add user_allow_other in the /etc/
fuse.conf configuration file.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>A host that runs OSDs must not mount Ceph RBD images or CephFS file systems
using the kernel-based client. Mounted resources can become unresponsive due
to memory deadlocks or blocked I/O pending on stale sessions.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_kernel_client"><a class="anchor" href="#_kernel_client"></a>5.2. Kernel Client</h3>
<div class="paragraph">
<p>Using the same user with id 0, we can go straight ahead and mount our Cephfs FS
with the kernel client:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat /etc/ceph/ceph.client.0.keyring  | grep key
	key = AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ==
# mount -t ceph ceph-node01.example.com,ceph-node02.example.com:/dir0 /mnt -o name=0,secret="AQBAaLVjpZu3NhAAlu30WoNaBn08obWB6T5IEQ=="
# ls -l /mnt
total 204800
-rw-r--r--. 1 root root 104857600 Jan  4 07:26 100MB</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we want to auto-mount the FS on boot, we need to add it to /etc/fstab with the following format:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># cat /etc/ceph/ceph.client.0.keyring | grep key | awk '{print $NF}' &gt;&gt; /etc/ceph/ceph.client.0.kernel.keyring
# echo 'ceph-node01.example.com,ceph-node02.example.com:/dir0 /mnt ceph # name=0,secretfile=/etc/ceph/ceph.client.0.kernel.keyring,_netdev 0 0' &gt;&gt; /etc/fstab
# mount /mnt</code></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>With the kernel client to be able to use the mount <code>secretfile</code> option, we need to
have the ceph-common packages installed</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_some_basic_client_capabilities"><a class="anchor" href="#_some_basic_client_capabilities"></a>6. Some Basic Client Capabilities</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Use Ceph authentication capabilities to restrict your file system clients to the lowest possible level of authority needed.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Path Restriction</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Use path-based MDS authentication capabilities to restrict clients to only mount and work within a certain directory.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">ceph fs authorize &lt;fs_name&gt; client.&lt;client_id&gt; &lt;path-in-cephfs&gt; rw</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Free Space Reporting</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>By default, when a client is mounting a sub-directory, the used space (df) will be calculated from the quota on that sub-directory rather than reporting the overall amount of space used on the cluster.</p>
</div>
<div class="paragraph">
<p>If you would like the client to report the overall usage of the file system and not just the quota usage on the sub-directory mounted, then set the following config option on the client:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">client quota df = false</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Layout &amp; Quota Restriction</strong></p>
</li>
</ul>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For more information on Cephfs Quotas follow this <a href="https://docs.ceph.com/en/latest/cephfs/quota/#quotas">link</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To set layouts or quotas, clients require the ‘p’ flag in addition to ‘rw’. This restricts all the attributes that are set by special extended attributes with a “ceph.” prefix, as well as restricting other means of setting these fields (such as openc operations with layouts).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">client.0
    key: AQAz7EVWygILFRAAdIcuJ12opU/JKyfFmxhuaw==
    caps: [mds] allow rwp
    caps: [mon] allow r
    caps: [osd] allow rw tag cephfs data=cephfs_a</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Snapshot Restrictions</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To create or delete snapshots, clients require the ‘s’ flag in addition to ‘rw’. Note that when the capability string contains the ‘p’ flag, the ‘s’ flag must appear after it (all flags except ‘rw’ must be specified in alphabetical order).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">client.0
    key: AQAz7EVWygILFRAAdIcuJ12opU/JKyfFmxhuaw==
    caps: [mds] allow rw, allow rws path=/bar
    caps: [mon] allow r
    caps: [osd] allow rw tag cephfs data=cephfs_a</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_read_on_root_fs_readwrite_only_on_dir1"><a class="anchor" href="#_read_on_root_fs_readwrite_only_on_dir1"></a>6.1. Read on / root FS, Read/Write only on /dir1</h3>
<div class="paragraph">
<p>We are going to create a new user with id 1</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs authorize fs_name client.1 / r /dir1 rw
[client.1]
	key = AQBNerVj77J2IBAAnmvdDyaMv20nxT0NyFd2cA==
# ceph auth get client.1 -o /etc/ceph/ceph.client.1.keyring
exported keyring for client.1
# cat /etc/ceph/ceph.client.1.keyring
[client.1]
	key = AQBNerVj77J2IBAAnmvdDyaMv20nxT0NyFd2cA==
	caps mds = "allow r fsname=fs_name, allow rw fsname=fs_name path=/dir1"
	caps mon = "allow r fsname=fs_name"
	caps osd = "allow rw tag cephfs data=fs_name"</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can now check by mounting the filesystem with user id <code>1</code>, that we can only
read on <code>/</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># umount /mnt
[root@workstation-lbedc2 ~]# ceph-fuse --id 1 /mnt
ceph-fuse[32460]: starting ceph client
2023-01-04T08:12:26.261-0500 7f311702f380 -1 init, newargv = 0x55a884998430 newargc=15
ceph-fuse[32460]: starting fuse
# ls /mnt
dir0  dir1
# touch /mnt/dir2
touch: cannot touch '/mnt/dir2': Permission denied
# touch /mnt/dir1/file1
# ls /mnt/dir1/file1
/mnt/dir1/file1</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_more_examples"><a class="anchor" href="#_more_examples"></a>6.2. More examples</h3>
<div class="paragraph">
<p>By yourself check out what these other example client caps allow the user to do:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell"># ceph fs authorize cephfs client.2 /client2 rw
# ceph fs authorize cephfs client.4 /client4 rwp</code></pre>
</div>
</div>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <a class="navbar-item" href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage" target="_blank">
      <img src="../../_/img/header_logo.svg" alt="Ceph">
  </a>
</footer>
<script id="site-script" src="../../_/js/site.js" data-ui-root-path="../../_"></script>
<script async src="../../_/js/vendor/highlight.js"></script>
  </body>
</html>
